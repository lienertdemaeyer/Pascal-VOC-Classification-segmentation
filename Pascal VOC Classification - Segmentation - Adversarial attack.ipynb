{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"_cell_guid":"35358cfb-b13d-4277-8dd5-4e663c8cd775","_uuid":"3b40b846-d7da-46d8-b354-c6d5c5ded56e","papermill":{"duration":0.014397,"end_time":"2022-04-12T14:48:23.501501","exception":false,"start_time":"2022-04-12T14:48:23.487104","status":"completed"},"tags":[]},"source":["# 1. Overview\n","This assignment consists of *three main parts* for which we expect you to provide code and extensive documentation in the notebook:\n","* Image classification (Sect. 2)\n","* Semantic segmentation (Sect. 3)\n","* Adversarial attacks (Sect. 4)\n","\n","In the first part, you will train an end-to-end neural network for image classification. In the second part, you will do the same for semantic segmentation. For these two tasks we expect you to put a significant effort into optimizing performance and as such competing with fellow students via the Kaggle competition. In the third part, you will try to find and exploit the weaknesses of your classification and/or segmentation network. For the latter there is no competition format, but we do expect you to put significant effort in achieving good performance on the self-posed goal for that part. Finally, we ask you to reflect and produce an overall discussion with links to the lectures and \"real world\" computer vision (Sect. 5). It is important to note that only a small part of the grade will reflect the actual performance of your networks. However, we do expect all things to work! In general, we will evaluate the correctness of your approach and your understanding of what you have done that you demonstrate in the descriptions and discussions in the final notebook."]},{"attachments":{},"cell_type":"markdown","metadata":{"papermill":{"duration":0.014263,"end_time":"2022-04-12T14:48:23.530341","exception":false,"start_time":"2022-04-12T14:48:23.516078","status":"completed"},"tags":[]},"source":["## 1.1 Deep learning resources\n","If you did not yet explore this in *Group assignment 1 (Sect. 2)*, we recommend using the TensorFlow and/or Keras library for building deep learning models. You can find a nice crash course [here](https://colab.research.google.com/drive/1UCJt8EYjlzCs1H1d1X0iDGYJsHKwu-NO)."]},{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"7ddf657a-b938-4a49-87dc-b0db9af9156d","_uuid":"c65ea4f1-cc90-408f-b8e0-7c7399ec7e21","execution":{"iopub.execute_input":"2023-05-24T19:17:58.510181Z","iopub.status.busy":"2023-05-24T19:17:58.508958Z","iopub.status.idle":"2023-05-24T19:18:04.063368Z","shell.execute_reply":"2023-05-24T19:18:04.061843Z","shell.execute_reply.started":"2023-05-24T19:17:58.510136Z"},"papermill":{"duration":5.416492,"end_time":"2022-04-12T14:48:28.96151","exception":false,"start_time":"2022-04-12T14:48:23.545018","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\liene\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]},{"ename":"ModuleNotFoundError","evalue":"No module named 'seaborn'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[1;32mIn[1], line 24\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mimblearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39munder_sampling\u001b[39;00m \u001b[39mimport\u001b[39;00m RandomUnderSampler\n\u001b[0;32m     23\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mimblearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpipeline\u001b[39;00m \u001b[39mimport\u001b[39;00m Pipeline\n\u001b[1;32m---> 24\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mseaborn\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39msns\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mrandom\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtqdm\u001b[39;00m\n","\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'seaborn'"]}],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","from matplotlib import pyplot as plt\n","import torchvision\n","import torch\n","from torchvision.transforms import ToTensor\n","from torchvision import models, transforms\n","from torch.utils.data import DataLoader, Dataset,random_split,ConcatDataset,WeightedRandomSampler\n","from torch.optim.lr_scheduler import CosineAnnealingLR, StepLR, ReduceLROnPlateau, OneCycleLR\n","import pandas as pd\n","import numpy as np\n","from PIL import Image\n","import torch.nn as nn\n","import torch.optim as optim\n","from sklearn.metrics import average_precision_score, precision_score, recall_score, f1_score, accuracy_score, confusion_matrix, classification_report\n","from sklearn.utils.class_weight import compute_class_weight\n","from sklearn.preprocessing import label_binarize, MultiLabelBinarizer\n","import numpy as np\n","from torch.utils.data import WeightedRandomSampler\n","from sklearn.model_selection import train_test_split\n","from imblearn.over_sampling import SMOTE\n","from imblearn.under_sampling import RandomUnderSampler\n","from imblearn.pipeline import Pipeline\n","import seaborn as sns\n","import random\n","import tqdme\n","\n","\n","device = (\"cuda\"  if torch.cuda.is_available()  else \"mps\"  if torch.backends.mps.is_available() else \"cpu\")"]},{"attachments":{},"cell_type":"markdown","metadata":{"papermill":{"duration":0.014416,"end_time":"2022-04-12T14:48:28.990998","exception":false,"start_time":"2022-04-12T14:48:28.976582","status":"completed"},"tags":[]},"source":["## 1.2 PASCAL VOC 2009\n","For this project you will be using the [PASCAL VOC 2009](http://host.robots.ox.ac.uk/pascal/VOC/voc2009/index.html) dataset. This dataset consists of colour images of various scenes with different object classes (e.g. animal: *bird, cat, ...*; vehicle: *aeroplane, bicycle, ...*), totalling 20 classes."]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1ce67f49-6bf6-4e5c-b5e4-576e893616a9","_uuid":"3b1c5fbb-757f-4349-b224-e281c540e1ad","execution":{"iopub.execute_input":"2023-05-24T19:18:04.066945Z","iopub.status.busy":"2023-05-24T19:18:04.066239Z","iopub.status.idle":"2023-05-24T19:18:32.289847Z","shell.execute_reply":"2023-05-24T19:18:32.288493Z","shell.execute_reply.started":"2023-05-24T19:18:04.066895Z"},"papermill":{"duration":21.336481,"end_time":"2022-04-12T14:48:50.342062","exception":false,"start_time":"2022-04-12T14:48:29.005581","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# Loading the training data\n","train_df = pd.read_csv('/kaggle/input/kul-h02a5a-computer-vision-ga2-2023/train/train_set.csv', index_col=\"Id\")\n","labels = train_df.columns\n","train_df[\"img\"] = [np.load('/kaggle/input/kul-h02a5a-computer-vision-ga2-2023/train/img/train_{}.npy'.format(idx)) for idx, _ in train_df.iterrows()]\n","train_df[\"seg\"] = [np.load('/kaggle/input/kul-h02a5a-computer-vision-ga2-2023/train/seg/train_{}.npy'.format(idx)) for idx, _ in train_df.iterrows()]\n","print(\"The training set contains {} examples.\".format(len(train_df)))\n","\n","# Show some examples\n","fig, axs = plt.subplots(2, 20, figsize=(10 * 20, 10 * 2))\n","for i, label in enumerate(labels):\n","    df = train_df.loc[train_df[label] == 1]\n","    axs[0, i].imshow(df.iloc[0][\"img\"], vmin=0, vmax=255)\n","    axs[0, i].set_title(\"\\n\".join(label for label in labels if df.iloc[0][label] == 1), fontsize=40)\n","    axs[0, i].axis(\"off\")\n","    axs[1, i].imshow(df.iloc[0][\"seg\"], vmin=0, vmax=20)  # with the absolute color scale it will be clear that the arrays in the \"seg\" column are label maps (labels in [0, 20])\n","    axs[1, i].axis(\"off\")\n","    \n","plt.show()\n","\n","# The training dataframe contains for each image 20 columns with the ground truth classification labels and 20 column with the ground truth segmentation maps for each class\n","train_df.head(1)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-24T19:18:32.292493Z","iopub.status.busy":"2023-05-24T19:18:32.292044Z","iopub.status.idle":"2023-05-24T19:18:45.186377Z","shell.execute_reply":"2023-05-24T19:18:45.184936Z","shell.execute_reply.started":"2023-05-24T19:18:32.292447Z"},"papermill":{"duration":11.507733,"end_time":"2022-04-12T14:49:02.044233","exception":false,"start_time":"2022-04-12T14:48:50.5365","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# Loading the test data\n","test_df = pd.read_csv('/kaggle/input/kul-h02a5a-computer-vision-ga2-2023/test/test_set.csv', index_col=\"Id\")\n","test_df[\"img\"] = [np.load('/kaggle/input/kul-h02a5a-computer-vision-ga2-2023/test/img/test_{}.npy'.format(idx)) for idx, _ in test_df.iterrows()]\n","test_df[\"seg\"] = [-1 * np.ones(img.shape[:2], dtype=np.int8) for img in test_df[\"img\"]]\n","print(\"The test set contains {} examples.\".format(len(test_df)))\n","\n","print(test_df[\"seg\"][0].shape)\n","print(test_df[\"seg\"][0])\n","\n","# The test dataframe is similar to the training dataframe, but here the values are -1 --> your task is to fill in these as good as possible in Sect. 2 and Sect. 3; in Sect. 6 this dataframe is automatically transformed in the submission CSV!\n","test_df.head(1)\n","\n","\n","\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## 1.4 Showing the dataset in Pytorch"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-24T19:18:45.22002Z","iopub.status.busy":"2023-05-24T19:18:45.219337Z","iopub.status.idle":"2023-05-24T19:20:08.34503Z","shell.execute_reply":"2023-05-24T19:20:08.343378Z","shell.execute_reply.started":"2023-05-24T19:18:45.219969Z"},"trusted":true},"outputs":[],"source":["# Original dataset from torchvision (I exposed all the parameters )\n","\n","download = True # please set to true if an error occurs\n","\n","dowload_folder = './kaggle/temp/'\n","\n","# Segmentation dataset\n","pvocS_train = torchvision.datasets.VOCSegmentation(root = dowload_folder, \n","                                             year = '2009', \n","                                             image_set  = 'train', \n","                                             download = download, \n","                                             transform = None, \n","                                             target_transform = None, \n","                                             transforms = None)\n","pvocS_val = torchvision.datasets.VOCSegmentation(root = dowload_folder, \n","                                             year = '2009', \n","                                             image_set  = 'val', \n","                                             download = download, \n","                                             transform = None, \n","                                             target_transform = None, \n","                                             transforms = None)\n","\n","# Detection dataset\n","pvocD_train = torchvision.datasets.VOCDetection(root = dowload_folder, \n","                                             year = '2009', \n","                                             image_set  = 'train', \n","                                             download = download, \n","                                             transform = None, \n","                                             target_transform = None, \n","                                             transforms = None)\n","\n","pvocD_val = torchvision.datasets.VOCDetection(root = dowload_folder, \n","                                             year = '2009', \n","                                             image_set  = 'val', \n","                                             download = download, \n","                                             transform = None, \n","                                             target_transform = None, \n","                                             transforms = None)\n","\n","itemtoget = 0 # index of the example item you want to show\n","\n","# Plot the first item on each dataset to see if it works \n","# Note: __getitem__() returns the image and the label in a tuple (image, label)\n","\n","fig, axs = plt.subplots(1, 6, figsize=(10 * 20, 10 * 2))\n","\n","axs[0].imshow(pvocS_train.__getitem__(itemtoget)[0])\n","axs[1].imshow(pvocS_val.__getitem__(itemtoget)[0])\n","axs[2].imshow(pvocS_train.__getitem__(itemtoget)[1])\n","axs[3].imshow(pvocS_val.__getitem__(itemtoget)[1])\n","\n","axs[4].imshow(pvocD_train.__getitem__(itemtoget)[0])\n","axs[5].imshow(pvocD_val.__getitem__(itemtoget)[0])\n","\n","for i in range(4):\n","    axs[i].axis(\"off\")\n","\n","plt.show()\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"papermill":{"duration":0.196641,"end_time":"2022-04-12T14:49:03.240824","exception":false,"start_time":"2022-04-12T14:49:03.044183","status":"completed"},"tags":[]},"source":["# 2. Image classification\n","The goal here is simple: implement a classification CNN and train it to recognise all 20 classes (and/or background) using the training set and compete on the test set (by filling in the classification columns in the test dataframe)."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-24T19:20:08.347588Z","iopub.status.busy":"2023-05-24T19:20:08.346678Z","iopub.status.idle":"2023-05-24T19:20:08.359295Z","shell.execute_reply":"2023-05-24T19:20:08.358108Z","shell.execute_reply.started":"2023-05-24T19:20:08.347543Z"},"papermill":{"duration":3.702597,"end_time":"2022-04-12T14:49:07.13902","exception":false,"start_time":"2022-04-12T14:49:03.436423","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["class RandomClassificationModel:\n","    \"\"\"\n","    Random classification model: \n","        - generates random labels for the inputs based on the class distribution observed during training\n","        - assumes an input can have multiple labels\n","    \"\"\"\n","    def fit(self, X, y):\n","        \"\"\"\n","        Adjusts the class ratio variable to the one observed in y. \n","\n","        Parameters\n","        ----------\n","        X: list of arrays - n x (height x width x 3)\n","        y: list of arrays - n x (nb_classes)\n","\n","        Returns\n","        -------\n","        self\n","        \"\"\"\n","        self.distribution = np.mean(y, axis=0)\n","        print(\"Setting class distribution to:\\n{}\".format(\"\\n\".join(f\"{label}: {p}\" for label, p in zip(labels, self.distribution))))\n","        return self\n","        \n","    def predict(self, X):\n","        \"\"\"\n","        Predicts for each input a label.\n","        \n","        Parameters\n","        ----------\n","        X: list of arrays - n x (height x width x 3)\n","            \n","        Returns\n","        -------\n","        y_pred: list of arrays - n x (nb_classes)\n","        \"\"\"\n","        np.random.seed(0)\n","        return [np.array([int(np.random.rand() < p) for p in self.distribution]) for _ in X]\n","    \n","    def __call__(self, X):\n","        return self.predict(X)\n","    \n","#model = RandomClassificationModel()\n","#model.fit(train_df[\"img\"], train_df[labels])\n","#test_df.loc[:, labels] = model.predict(test_df[\"img\"])\n","#test_df.head(1)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## 2.2 CNN Classification"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### 2.2.1 Loading and Preprocessing the data"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### 2.2.1.1 Load the data\n","\n","- Read the train dataset CSV file into a pandas DataFrame.\n","- Extract the labels and image paths from the DataFrame.\n","- Define the `ImageDataset` class for custom dataset handling.\n","- Define the data transformations using torchvision transforms.\n","- Create an instance of the `ImageDataset` for the train dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-24T19:20:08.362534Z","iopub.status.busy":"2023-05-24T19:20:08.361298Z","iopub.status.idle":"2023-05-24T19:20:08.37699Z","shell.execute_reply":"2023-05-24T19:20:08.375636Z","shell.execute_reply.started":"2023-05-24T19:20:08.362478Z"},"trusted":true},"outputs":[],"source":["# ImageDataset class\n","class ImageDataset(Dataset):\n","    def __init__(self, images, labels=None, transform=None, target_transform=None):\n","        self.images = images\n","        self.labels = labels\n","        self.transform = transform\n","        self.target_transform = target_transform\n","\n","    def __len__(self):\n","        return len(self.images)\n","\n","    def __getitem__(self, idx):\n","        image = self.images[idx]\n","        if self.transform is not None:\n","            image = Image.fromarray(image)\n","            image = self.transform(image)\n","        \n","        if self.labels is not None:\n","            label = self.labels[idx]\n","            if self.target_transform is not None:\n","                label = self.transform(label)\n","            return image, label\n","        else:\n","            return image\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-24T19:20:08.379681Z","iopub.status.busy":"2023-05-24T19:20:08.378638Z","iopub.status.idle":"2023-05-24T19:20:11.550615Z","shell.execute_reply":"2023-05-24T19:20:11.549147Z","shell.execute_reply.started":"2023-05-24T19:20:08.379637Z"},"trusted":true},"outputs":[],"source":["# Calculate mean and std\n","train_dataset_for_stats = ImageDataset(train_df['img'].to_list(), train_df[labels].to_numpy(), transform=ToTensor())\n","meanRGB = [np.mean(x[0].numpy(), axis=(1, 2)) for x in train_dataset_for_stats]\n","stdRGB = [np.std(x[0].numpy(), axis=(1, 2)) for x in train_dataset_for_stats]\n","\n","\n","meanR=np.mean([m[0] for m in meanRGB])\n","meanG=np.mean([m[1] for m in meanRGB])\n","meanB=np.mean([m[2] for m in meanRGB])\n","\n","stdR=np.mean([s[0] for s in stdRGB])\n","stdG=np.mean([s[1] for s in stdRGB])\n","stdB=np.mean([s[2] for s in stdRGB])\n","\n","print(meanR,meanG,meanB)\n","print(stdR,stdG,stdB)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-24T19:20:11.555431Z","iopub.status.busy":"2023-05-24T19:20:11.555081Z","iopub.status.idle":"2023-05-24T19:20:11.561755Z","shell.execute_reply":"2023-05-24T19:20:11.560197Z","shell.execute_reply.started":"2023-05-24T19:20:11.555396Z"},"trusted":true},"outputs":[],"source":["# Data transforms (normalization & data augmentation)\n","stats = ((meanR, meanG, meanB), (stdR, stdG, stdB))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-24T19:20:11.569896Z","iopub.status.busy":"2023-05-24T19:20:11.569472Z","iopub.status.idle":"2023-05-24T19:20:11.577305Z","shell.execute_reply":"2023-05-24T19:20:11.575906Z","shell.execute_reply.started":"2023-05-24T19:20:11.569854Z"},"trusted":true},"outputs":[],"source":["# Train set transforms\n","train_transforms = transforms.Compose([\n","    transforms.Resize((300, 300)),\n","    transforms.RandomChoice([\n","        transforms.ColorJitter(brightness=(0.80, 1.20)),\n","        transforms.RandomGrayscale(p=0.25)\n","    ]),\n","    transforms.RandomHorizontalFlip(p=0.25),\n","    transforms.RandomRotation(25),\n","    transforms.ToTensor(),\n","    transforms.Normalize(*stats, inplace=True)\n","])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-24T19:20:11.579958Z","iopub.status.busy":"2023-05-24T19:20:11.578872Z","iopub.status.idle":"2023-05-24T19:20:11.587888Z","shell.execute_reply":"2023-05-24T19:20:11.586595Z","shell.execute_reply.started":"2023-05-24T19:20:11.579914Z"},"trusted":true},"outputs":[],"source":["# Validation set transforms\n","val_transforms = transforms.Compose([\n","    transforms.Resize((300, 300)),\n","    transforms.RandomCrop((300, 300)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(*stats, inplace=True)\n","])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-24T19:20:11.590389Z","iopub.status.busy":"2023-05-24T19:20:11.589451Z","iopub.status.idle":"2023-05-24T19:20:11.601381Z","shell.execute_reply":"2023-05-24T19:20:11.600484Z","shell.execute_reply.started":"2023-05-24T19:20:11.590347Z"},"trusted":true},"outputs":[],"source":["test_transforms = transforms.Compose([\n","    transforms.Resize((350, 350)),\n","    transforms.CenterCrop((300, 300)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(*stats, inplace=True)\n","])"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### 2.2.1.2 Class Distribution Histogram\n","\n","The class distribution histogram indicates that the labels are imbalanced, with the \"person\" class being on average 4x more frequent than the other classes. As people are often in images with other classes like bicycles and motorbikes. However for the classifier to learn from a fair propotion of each class and to prevent the model from focusing too much on a specific class, the imbalance needs to be adressed.\n","\n","**Upsampling & Undersampling:**\n","\n","One solution is to upsample, but since there are only a limited number of instances, this would result in resampling the same instances multiple times, hindering the model's generalization. Alternatively, we could undersample the majority class, however the could lead to removal of images from already underrepresented classes.\n","\n","**Weighting**\n","\n","Instead we will attempt to address this imbalance during the learning process. This weighting scheme gives importance to the minority class, while it reduced the dominance of the majority class. The weights are calculated based on the class counts, where the weight of a class is inversely proportional to its occurrence in the dataset. During model training, it was observed that this weighting approach imporved generalization and led to faster performance on the validation set."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-24T19:20:11.603974Z","iopub.status.busy":"2023-05-24T19:20:11.602899Z","iopub.status.idle":"2023-05-24T19:20:11.983725Z","shell.execute_reply":"2023-05-24T19:20:11.982467Z","shell.execute_reply.started":"2023-05-24T19:20:11.60393Z"},"trusted":true},"outputs":[],"source":["# Initialize the dictionary to store the count of each class\n","class_count = {label: 0 for label in labels}\n","\n","# Iterate through the ground truth labels in the dataset\n","for label in labels:\n","    class_count[label] = train_df[label].sum()\n","\n","# Create a dictionary of classes and their numeric index (ordered alphabetically)\n","class_dictionary = {i: label for i, label in enumerate(sorted(labels))}\n","class_dictionary_inv = {label: i for i, label in class_dictionary.items()}\n","\n","# Plot the histogram of the class distribution\n","plt.figure(figsize=(12, 6))\n","plt.bar(class_dictionary_inv.keys(), class_count.values())\n","plt.xlabel('Class')\n","plt.xticks(list(class_dictionary.keys()), list(class_dictionary.values()), rotation=45)\n","plt.ylabel('Count')\n","plt.title('Histogram of Class Distribution')\n","plt.show()\n","\n","# Calculate the number of labels for each class by summing the values in each column\n","class_counts = train_df[labels].sum(axis=0)\n","\n","# Display the vector with the number of labels for each class\n","print(class_counts)\n","\n","# Calculate the total number of labels\n","total_labels = class_counts.sum()\n","\n","# Display the total number of labels\n","print(f\"Total: {total_labels}\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### 2.2.1.3 Calculating and Normalizing Class Weights for Imbalanced Labels"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-24T19:20:11.985867Z","iopub.status.busy":"2023-05-24T19:20:11.985236Z","iopub.status.idle":"2023-05-24T19:20:14.933377Z","shell.execute_reply":"2023-05-24T19:20:14.932052Z","shell.execute_reply.started":"2023-05-24T19:20:11.985802Z"},"trusted":true},"outputs":[],"source":["def calculate_weights(class_counts):\n","    weights = np.ones_like(class_counts)\n","    total_samples = class_counts.sum()\n","    neg_counts = [total_samples - pos_count for pos_count in class_counts]\n","    for cdx, (pos_count, neg_count) in enumerate(zip(class_counts, neg_counts)):\n","        weights[cdx] = neg_count / (pos_count + 1e-5)\n","\n","    return torch.as_tensor(weights, dtype=torch.float)\n","\n","weights = calculate_weights(class_counts)\n","weights_tensor = torch.tensor(weights, dtype=torch.float, device=device)\n","\n","print(weights_tensor)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### 2.2.2 Split the data into train and validation sets and create Dataloader"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-24T19:20:14.935685Z","iopub.status.busy":"2023-05-24T19:20:14.935272Z","iopub.status.idle":"2023-05-24T19:20:14.948242Z","shell.execute_reply":"2023-05-24T19:20:14.947004Z","shell.execute_reply.started":"2023-05-24T19:20:14.935643Z"},"trusted":true},"outputs":[],"source":["# Split the data into train and validation sets\n","train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42)\n","\n","# Create instances of ImageDataset for the train and validation datasets\n","train_dataset = ImageDataset(train_df['img'].to_list(), train_df[labels].to_numpy(), transform=train_transforms)\n","val_dataset = ImageDataset(val_df['img'].to_list(), val_df[labels].to_numpy(), transform=val_transforms)\n","\n","# Create instances of ImageDataset for the test set\n","test_dataset = ImageDataset(test_df['img'].to_list(), test_df[labels].to_numpy(), transform=test_transforms)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### 2.2.2.1 Perform data augmentation by equally augmenting the samples of each class"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-24T19:20:14.950939Z","iopub.status.busy":"2023-05-24T19:20:14.949949Z","iopub.status.idle":"2023-05-24T19:20:14.963664Z","shell.execute_reply":"2023-05-24T19:20:14.962402Z","shell.execute_reply.started":"2023-05-24T19:20:14.950893Z"},"trusted":true},"outputs":[],"source":["def augment_dataset(dataset, num_augmentations):\n","    transform_list = [\n","        transforms.RandomHorizontalFlip(p=0.5),\n","        transforms.RandomVerticalFlip(p=0.5),\n","        transforms.RandomRotation(degrees=30),\n","        transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n","        transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), shear=(10,10,10,10)),\n","        transforms.RandomPerspective(),\n","    ]\n","\n","    augmented_datasets = []\n","    for i in range(num_augmentations):\n","        transform = transforms.Compose(random.sample(transform_list, k=3))\n","        augmented_dataset = []\n","        for data in dataset:\n","            augmented_data = list(data)\n","            augmented_data[0] = transform(augmented_data[0])\n","            augmented_dataset.append(tuple(augmented_data))\n","        augmented_datasets.append(augmented_dataset)\n","\n","    # Concatenate the augmented datasets\n","    concatenated_dataset = []\n","    for augmented_dataset in augmented_datasets:\n","        concatenated_dataset += augmented_dataset\n","\n","    return concatenated_dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-24T19:20:14.965967Z","iopub.status.busy":"2023-05-24T19:20:14.965442Z","iopub.status.idle":"2023-05-24T19:20:54.299553Z","shell.execute_reply":"2023-05-24T19:20:54.298135Z","shell.execute_reply.started":"2023-05-24T19:20:14.965922Z"},"trusted":true},"outputs":[],"source":["augmented_train_set = augment_dataset(train_dataset, num_augmentations=4)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-24T19:20:54.30236Z","iopub.status.busy":"2023-05-24T19:20:54.301194Z","iopub.status.idle":"2023-05-24T19:20:54.308162Z","shell.execute_reply":"2023-05-24T19:20:54.306909Z","shell.execute_reply.started":"2023-05-24T19:20:54.302314Z"},"trusted":true},"outputs":[],"source":["augmented_train_set = ConcatDataset([train_dataset, augmented_train_set])"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### 2.2.2.2 Perform data augmentation by augmenting the samples of all classes except the predominant 'person' class in order to balance the dataset"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Upsampling  approach - Drawbacks\n","\n","In the context of an imbalanced dataset, there are a few strategies we could consider to address the issue. One approach is upsampling, where we generate synthetic data to increase the representation of the minority class. However, in our case, since the dataset is not large, upsampling could result in duplicating the same instances multiple times. This can be problematic because it may lead to overfitting and hinder the generalization capability of our classifier.\n","\n","Another approach is undersampling, which involves removing a portion of the majority class to balance the class distribution. However, if we were to apply undersampling to the images containing people, for example, we run the risk of removing images from classes that are already in the minority. This is not an ideal solution for our dataset, as we cannot afford to lose valuable data from these minority classes.\n","\n","To mitigate the potential issues mentioned earlier and explore the effectiveness of the upsampling approach, we decided to augment our dataset. However, we did that by increasing the number of instances for every class except the \"person\" class, maintaining a relatively proportional balance.\n","\n","Despite implementing the dataset augmentation approach, we did not observe an improvement in performance. Therefore, we have decided to keep the former strategy of assigning weights to each class."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-24T19:20:54.310921Z","iopub.status.busy":"2023-05-24T19:20:54.310096Z","iopub.status.idle":"2023-05-24T19:20:54.324379Z","shell.execute_reply":"2023-05-24T19:20:54.323131Z","shell.execute_reply.started":"2023-05-24T19:20:54.310877Z"},"trusted":true},"outputs":[],"source":["def augment_dataset_except_class(dataset, num_augmentations):\n","    transform_list = [\n","        transforms.RandomHorizontalFlip(p=0.5),\n","        transforms.RandomVerticalFlip(p=0.5),\n","        transforms.RandomRotation(degrees=30),\n","        transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n","        transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), shear=(10, 10, 10, 10)),\n","        transforms.RandomPerspective(),\n","    ]\n","\n","    augmented_datasets = []\n","    for i in range(num_augmentations):\n","        transform = transforms.Compose(random.sample(transform_list, k=3))\n","        augmented_dataset = []\n","        for data in dataset:\n","            if data[1][14] != 1:\n","                augmented_data = list(data)\n","                augmented_data[0] = transform(augmented_data[0])\n","                augmented_dataset.append(tuple(augmented_data))\n","            else:\n","                augmented_dataset.append(data)\n","        augmented_datasets.append(augmented_dataset)\n","\n","    # Concatenate the augmented datasets\n","    concatenated_dataset = []\n","    for augmented_dataset in augmented_datasets:\n","        concatenated_dataset += augmented_dataset\n","\n","    return concatenated_dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-24T19:20:54.326748Z","iopub.status.busy":"2023-05-24T19:20:54.326198Z","iopub.status.idle":"2023-05-24T19:20:54.34098Z","shell.execute_reply":"2023-05-24T19:20:54.339806Z","shell.execute_reply.started":"2023-05-24T19:20:54.3267Z"},"trusted":true},"outputs":[],"source":["# augmented_train_set = augment_dataset_except_class(train_dataset, num_augmentations=4)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### 2.2.2.3 Create Dataloader"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-24T19:20:54.344097Z","iopub.status.busy":"2023-05-24T19:20:54.342962Z","iopub.status.idle":"2023-05-24T19:20:54.353887Z","shell.execute_reply":"2023-05-24T19:20:54.352887Z","shell.execute_reply.started":"2023-05-24T19:20:54.344049Z"},"trusted":true},"outputs":[],"source":["# Create DataLoaders for train_dataset and val_dataset\n","# For the 'trained from scratch' case data augmentation is needed to train the model, due to the lack of sufficient training data\n","train_loader = DataLoader(augmented_train_set, batch_size=32, shuffle=True, num_workers=2, pin_memory=True)\n","\n","# train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=0)\n","val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=2, pin_memory=True)\n","\n","# Visualize train loader size\n","len(train_loader)\n","\n","# Create DataLoader for test_dataset\n","test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2, pin_memory=True)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### 2.2.2.4 Visualize some of the generated training images"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-24T19:20:54.356379Z","iopub.status.busy":"2023-05-24T19:20:54.355643Z","iopub.status.idle":"2023-05-24T19:20:54.368289Z","shell.execute_reply":"2023-05-24T19:20:54.367189Z","shell.execute_reply.started":"2023-05-24T19:20:54.356334Z"},"trusted":true},"outputs":[],"source":["def visualize_batch(train_loader, class_names):\n","    # Get a batch of images and labels from train_loader\n","    images, batch_labels = next(iter(train_loader))\n","    \n","    # Convert images from tensors to numpy arrays\n","    images = images.numpy()\n","\n","    # Create a figure \n","    fig, axes = plt.subplots(nrows=4, ncols=4, figsize=(10, 10))\n","    for i, ax in enumerate(axes.flat):\n","        if i < len(images):\n","            ax.imshow(np.transpose(images[i], (1, 2, 0)))\n","\n","            # Get the class indices where the label is 1\n","            label_indices = [j for j, val in enumerate(batch_labels[i]) if val == 1]\n","\n","            # Map class indices to class names\n","            label_names = [class_names[idx] for idx in label_indices]\n","\n","            ax.set_title(label_names)\n","\n","        ax.set_xticks([])\n","        ax.set_yticks([])\n","\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-24T19:20:54.37055Z","iopub.status.busy":"2023-05-24T19:20:54.370138Z","iopub.status.idle":"2023-05-24T19:20:56.145458Z","shell.execute_reply":"2023-05-24T19:20:56.144351Z","shell.execute_reply.started":"2023-05-24T19:20:54.37051Z"},"trusted":true},"outputs":[],"source":["visualize_batch(train_loader, labels)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## 2.2.3.1 Creating CNN Models"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-24T19:20:56.148407Z","iopub.status.busy":"2023-05-24T19:20:56.147572Z","iopub.status.idle":"2023-05-24T19:20:56.162588Z","shell.execute_reply":"2023-05-24T19:20:56.16132Z","shell.execute_reply.started":"2023-05-24T19:20:56.148335Z"},"trusted":true},"outputs":[],"source":["# Define the network architecture\n","class SimpleNetwork(nn.Module):\n","    def __init__(self):\n","        super(SimpleNetwork, self).__init__()\n","        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n","        self.relu1 = nn.ReLU()\n","        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n","        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n","        self.relu2 = nn.ReLU()\n","        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n","        self.fc1 = nn.Linear(180000, 20)  # Update the input size of the fully connected layer\n","\n","    def forward(self, x):\n","        x = self.conv1(x)\n","        x = self.relu1(x)\n","        x = self.pool1(x)\n","        x = self.conv2(x)\n","        x = self.relu2(x)\n","        x = self.pool2(x)\n","        x = x.view(x.size(0), -1)\n","        x = self.fc1(x)\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-24T19:20:56.166141Z","iopub.status.busy":"2023-05-24T19:20:56.164618Z","iopub.status.idle":"2023-05-24T19:20:56.188508Z","shell.execute_reply":"2023-05-24T19:20:56.187267Z","shell.execute_reply.started":"2023-05-24T19:20:56.16608Z"},"trusted":true},"outputs":[],"source":["class ComplexNetwork(nn.Module):\n","    def __init__(self):\n","        super(ComplexNetwork, self).__init__()\n","\n","        # Convolutional layers\n","        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n","        self.bn1 = nn.BatchNorm2d(32)\n","        self.relu1 = nn.ReLU(inplace=True)\n","        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n","\n","        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n","        self.bn2 = nn.BatchNorm2d(64)\n","        self.relu2 = nn.ReLU(inplace=True)\n","        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n","\n","        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n","        self.bn3 = nn.BatchNorm2d(128)\n","        self.relu3 = nn.ReLU(inplace=True)\n","        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n","\n","        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n","        self.bn4 = nn.BatchNorm2d(256)\n","        self.relu4 = nn.ReLU(inplace=True)\n","        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n","\n","        # Fully connected layers\n","        self.fc1 = nn.Linear(82944, 512)\n","        self.dropout1 = nn.Dropout(p=0.5)\n","        self.relu5 = nn.ReLU(inplace=True)\n","        self.fc2 = nn.Linear(512, 20)\n","        self.relu6 = nn.ReLU(inplace=True)\n","#         self.dropout2 = nn.Dropout(p=0.5)\n","        \n","\n","    def forward(self, x):\n","        # Convolutional layers\n","        x = self.conv1(x)\n","        x = self.bn1(x)\n","        x = self.relu1(x)\n","        x = self.pool1(x)\n","\n","        x = self.conv2(x)\n","        x = self.bn2(x)\n","        x = self.relu2(x)\n","        x = self.pool2(x)\n","\n","        x = self.conv3(x)\n","        x = self.bn3(x)\n","        x = self.relu3(x)\n","        x = self.pool3(x)\n","\n","        x = self.conv4(x)\n","        x = self.bn4(x)\n","        x = self.relu4(x)\n","        x = self.pool4(x)\n","\n","        # Flatten the output from convolutional layers before passing to fully connected layers\n","        x = x.view(x.size(0), -1)\n","        \n","        # Fully connected layers\n","        x = self.fc1(x)\n","        x = self.relu5(x)\n","#         x = self.dropout1(x)\n","        x = self.fc2(x)\n","#         x = self.relu6(x)\n","#         x = self.dropout2(x)\n","        \n","\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-24T19:20:56.191204Z","iopub.status.busy":"2023-05-24T19:20:56.190742Z","iopub.status.idle":"2023-05-24T19:20:56.214151Z","shell.execute_reply":"2023-05-24T19:20:56.212708Z","shell.execute_reply.started":"2023-05-24T19:20:56.19116Z"},"trusted":true},"outputs":[],"source":["class TinyVGG(nn.Module):\n","    def __init__(self):\n","        super(TinyVGG, self).__init__()\n","\n","        self.channel_dim = 1 if torch.cuda.is_available() else -1\n","\n","        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n","        self.relu1 = nn.ReLU()\n","        self.bn1 = nn.BatchNorm2d(32)\n","        self.pool1 = nn.MaxPool2d(kernel_size=3, stride=3)\n","        self.dropout1 = nn.Dropout2d(p=0.25)\n","\n","        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n","        self.relu2 = nn.ReLU()\n","        self.bn2 = nn.BatchNorm2d(64)\n","        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n","        self.relu3 = nn.ReLU()\n","        self.bn3 = nn.BatchNorm2d(64)\n","        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n","        self.dropout2 = nn.Dropout2d(p=0.25)\n","\n","        self.conv4 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n","        self.relu4 = nn.ReLU()\n","        self.bn4 = nn.BatchNorm2d(128)\n","        self.conv5 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n","        self.relu5 = nn.ReLU()\n","        self.bn5 = nn.BatchNorm2d(128)\n","        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n","        self.dropout3 = nn.Dropout2d(p=0.25)\n","\n","        self.flatten = nn.Flatten()\n","        self.fc1 = nn.Linear(80000, 1024)\n","        self.relu6 = nn.ReLU()\n","        self.bn6 = nn.BatchNorm1d(1024)\n","        self.dropout4 = nn.Dropout(p=0.5)\n","        self.fc2 = nn.Linear(1024, 20)\n","        \n","    def forward(self, x):\n","        x = self.conv1(x)\n","        x = self.relu1(x)\n","        x = self.bn1(x)\n","        x = self.pool1(x)\n","        x = self.dropout1(x)\n","\n","        x = self.conv2(x)\n","        x = self.relu2(x)\n","        x = self.bn2(x)\n","        x = self.conv3(x)\n","        x = self.relu3(x)\n","        x = self.bn3(x)\n","        x = self.pool2(x)\n","        x = self.dropout2(x)\n","\n","        x = self.conv4(x)\n","        x = self.relu4(x)\n","        x = self.bn4(x)\n","        x = self.conv5(x)\n","        x = self.relu5(x)\n","        x = self.bn5(x)\n","        x = self.pool3(x)\n","        x = self.dropout3(x)\n","\n","        x = self.flatten(x)\n","        x = self.fc1(x)\n","        x = self.relu6(x)\n","        x = self.bn6(x)\n","        x = self.dropout4(x)\n","        x = self.fc2(x)\n","\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-24T19:20:56.216917Z","iopub.status.busy":"2023-05-24T19:20:56.216128Z","iopub.status.idle":"2023-05-24T19:20:56.230902Z","shell.execute_reply":"2023-05-24T19:20:56.229682Z","shell.execute_reply.started":"2023-05-24T19:20:56.216829Z"},"trusted":true},"outputs":[],"source":["def get_alexnet_model(input_shape, nb_classes):\n","    model = models.alexnet(weights=None)\n","    # Replace the last fully connected layer with a new one\n","    model.classifier[6] = nn.Linear(4096, nb_classes)\n","    # Modify the input layer to accept the desired input shape\n","    model.features[0] = nn.Conv2d(input_shape[0], 64, kernel_size=11, stride=4, padding=2)\n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-24T19:20:56.249Z","iopub.status.busy":"2023-05-24T19:20:56.248168Z","iopub.status.idle":"2023-05-24T19:20:56.255724Z","shell.execute_reply":"2023-05-24T19:20:56.254349Z","shell.execute_reply.started":"2023-05-24T19:20:56.248953Z"},"trusted":true},"outputs":[],"source":["def build_resnet():\n","    model = models.resnet34(weights=None, num_classes=20)\n","    model = model.to(device)\n","    return model"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Initialize the model, define the loss function, the optimizer, the scheduler, and train the model."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-24T19:20:56.258556Z","iopub.status.busy":"2023-05-24T19:20:56.257971Z","iopub.status.idle":"2023-05-24T19:20:56.770753Z","shell.execute_reply":"2023-05-24T19:20:56.769544Z","shell.execute_reply.started":"2023-05-24T19:20:56.258512Z"},"trusted":true},"outputs":[],"source":["# # Initialize the model, loss function, and optimizer\n","# model = SimpleNetwork().to(device)\n","model = ComplexNetwork().to(device)\n","# model = TinyVGG().to(device)\n","\n","# input_shape = (3, 300, 300)\n","# nb_classes = 20\n","# model = get_alexnet_model(input_shape, nb_classes).to(device)\n","# model = build_resnet()\n","\n","criterion = nn.BCEWithLogitsLoss(weight=weights_tensor)\n","\n","optimizer = optim.SGD(model.parameters(), lr=0.00015, momentum=0.9)\n","\n","# Define the scheduler\n","max_lr = 0.01  # Maximum learning rate\n","num_epochs = 2\n","steps_per_epoch = len(train_loader)\n","scheduler = OneCycleLR(optimizer, max_lr, steps_per_epoch, epochs = num_epochs)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### 1cycle learning\n","The 1cycle learning rate policy changes the learning rate after every batch. It was chosen as the preferable scheduler since it offers faster convergence, improved generalization, reduced overfitting, robustness to hyperparameter tuning, and flexibility in training neural networks. "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-24T19:20:56.773423Z","iopub.status.busy":"2023-05-24T19:20:56.772398Z","iopub.status.idle":"2023-05-24T19:20:56.780677Z","shell.execute_reply":"2023-05-24T19:20:56.779425Z","shell.execute_reply.started":"2023-05-24T19:20:56.773368Z"},"trusted":true},"outputs":[],"source":["print(model)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Description of ComplexNetwork architecture (best performing custom architecture):\n","\n","* The **architecture** consists of four sets of convolutional layers followed by batch normalization, ReLU activation, and max pooling operations.\n","* The **convolutional layers** have kernel sizes of 3x3, stride 1, and padding 1 to maintain the spatial dimensions.\n","* The number of output channels progressively increases from 32 to 256, indicating increasing complexity and abstraction of features.\n","* After each convolutional layer, **batch normalization** is applied to normalize the activations and improve network stability.\n","* **ReLU activation** functions introduce non-linearity to the network, allowing it to learn complex representations.\n","* **Max pooling** with a kernel size of 2x2 and stride 2 is used to downsample the spatial dimensions, reducing computational complexity and capturing more prominent features.\n","* After the convolutional layers, the output is flattened to a 1D vector before being fed into the fully connected layers.\n","* The architecture has two **fully connected layers** with 512 and 20 units, respectively.\n","* ReLU activation functions are used after each fully connected layer to introduce non-linearity.\n","* The final output is obtained from the second fully connected layer.\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Things we tried to improve performance that did not work out:\n","* A dropout layer with a dropout probability of 0.5 was applied after the first fully connected layer to mitigate overfitting.\n","\n","1. Dropout is primarily beneficial for regularizing and preventing overfitting in neural networks. However, in the case of imbalanced datasets, like ours, where certain classes have significantly fewer samples than others, dropout may further reduce the already limited representation of minority classes.\n","1. Dropout randomly sets a fraction of the input units to zero during training, which helps in reducing interdependent learning among neurons. However, when the dataset is imbalanced, dropping out units may result in the loss of important information specific to minority classes.\n","1. Dropout acts as a regularization technique by introducing noise and forcing the network to learn redundant representations. This may reduce the model's capacity to learn complex patterns present in the minority classes."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### ResNet-34 Vs ComplexNetwork\n","\n","ResNet-34 is a powerful and deep architecture known for its skip connections and excellent performance on image classification tasks. It is suitable for complex datasets and situations where high accuracy is desired. On the other hand, ComplexNetwork is a simpler and computationally lighter alternative that still performed reasonably well on our image classification tasks while using limited computational resources. The choice between the two models was arbitrary, since both the ResNet-34 and the CompleNetwork were evaluated for the image classification task on our small dataset, and their performance was found to be similar on average. Therefore, we will demonstrate the performance of **ComplexNetwork**. In addition the ResNet-34 was specifically trained on the small dataset at hand, with all its weights determined through the training process. Finally, the improved performance of ResNet-34 when utilizing pre-trained weights, will be demonstrated later."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-24T19:20:56.783813Z","iopub.status.busy":"2023-05-24T19:20:56.782505Z","iopub.status.idle":"2023-05-24T19:20:56.800237Z","shell.execute_reply":"2023-05-24T19:20:56.798905Z","shell.execute_reply.started":"2023-05-24T19:20:56.78375Z"},"trusted":true},"outputs":[],"source":["def train(model, dataloader, criterion, optimizer, device):\n","    model.train()\n","    train_loss = 0.0\n","    train_correct = 0\n","    train_total = 0\n","    epoch_train_preds = []\n","    epoch_train_true = []\n","\n","    for images, labels in dataloader:\n","        images, labels = images.to(device), labels.to(device)\n","\n","        optimizer.zero_grad()\n","        outputs = model(images)\n","        loss = criterion(outputs, labels.float())\n","        loss.backward()\n","        optimizer.step()\n","\n","        train_loss += loss.item() * images.size(0)\n","\n","        predicted_labels = (torch.sigmoid(outputs) >= 0.5).float()\n","        train_correct += (predicted_labels == labels).sum().item()\n","        train_total += labels.size(0) * len(labels)\n","\n","        \n","        epoch_train_preds.extend(predicted_labels.detach().cpu().numpy())\n","        epoch_train_true.extend(labels.detach().cpu().numpy())\n","\n","    train_loss /= len(dataloader.dataset)\n","#     train_acc = train_correct / train_total\n","    train_acc = accuracy_score(labels.cpu().detach().numpy().reshape(-1), predicted_labels.cpu().detach().numpy().reshape(-1))\n","    train_precision = precision_score(epoch_train_true, epoch_train_preds, average='weighted', zero_division=1)\n","#     train_precision = precision_score(labels.cpu().detach().numpy().reshape(-1), predicted_labels.cpu().detach().numpy().reshape(-1), zero_division = 1)\n","    train_recall = recall_score(epoch_train_true, epoch_train_preds, average='weighted', zero_division=1)\n","#     train_recall = recall_score(labels.cpu().detach().numpy().reshape(-1), predicted_labels.cpu().detach().numpy().reshape(-1))\n","    train_f1 = f1_score(epoch_train_true, epoch_train_preds, average='weighted', zero_division=1)\n","#     train_f1 = f1_score(labels.cpu().detach().numpy().reshape(-1), predicted_labels.cpu().detach().numpy().reshape(-1))\n","\n","    return train_loss, train_acc, train_precision, train_recall, train_f1"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-24T19:20:56.803489Z","iopub.status.busy":"2023-05-24T19:20:56.802197Z","iopub.status.idle":"2023-05-24T19:20:56.818741Z","shell.execute_reply":"2023-05-24T19:20:56.817788Z","shell.execute_reply.started":"2023-05-24T19:20:56.803454Z"},"trusted":true},"outputs":[],"source":["def validate(model, dataloader, criterion, device):\n","    model.eval()\n","    val_loss = 0.0\n","    val_correct = 0\n","    val_total = 0\n","    epoch_val_preds = []\n","    epoch_val_true = []\n","\n","    with torch.no_grad():\n","        for images, labels in dataloader:\n","            images, labels = images.to(device), labels.to(device)\n","\n","            outputs = model(images)\n","            loss = criterion(outputs, labels.float())\n","\n","            val_loss += loss.item() * images.size(0)\n","\n","            predicted_labels = (torch.sigmoid(outputs) >= 0.5).float()\n","            val_correct += (predicted_labels == labels).sum().item()\n","            val_total += labels.size(0) * len(labels)\n","\n","            epoch_val_preds.extend(predicted_labels.detach().cpu().numpy())\n","            epoch_val_true.extend(labels.detach().cpu().numpy())\n","\n","    val_loss /= len(dataloader.dataset)\n","#     val_acc = val_correct / val_total\n","    val_acc = accuracy_score(labels.cpu().detach().numpy().reshape(-1), predicted_labels.cpu().detach().numpy().reshape(-1))\n","    val_precision = precision_score(epoch_val_true, epoch_val_preds, average='weighted', zero_division=1)\n","#     val_precision = precision_score(labels.cpu().detach().numpy().reshape(-1), predicted_labels.cpu().detach().numpy().reshape(-1), zero_division=1)\n","    val_recall = recall_score(epoch_val_true, epoch_val_preds, average='weighted', zero_division=1)\n","#     val_recall = recall_score(labels.cpu().detach().numpy().reshape(-1), predicted_labels.cpu().detach().numpy().reshape(-1))\n","    val_f1 = f1_score(epoch_val_true, epoch_val_preds, average='weighted', zero_division=1)\n","#     val_f1 = f1_score(labels.cpu().detach().numpy().reshape(-1), predicted_labels.cpu().detach().numpy().reshape(-1))\n","\n","    return val_loss, val_acc, val_precision, val_recall, val_f1\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-24T19:20:56.821524Z","iopub.status.busy":"2023-05-24T19:20:56.82068Z","iopub.status.idle":"2023-05-24T19:21:38.006729Z","shell.execute_reply":"2023-05-24T19:21:38.004954Z","shell.execute_reply.started":"2023-05-24T19:20:56.821449Z"},"trusted":true},"outputs":[],"source":["# Add lists to store training and validation losses and metrics\n","train_losses, val_losses = [], []\n","train_accuracies, train_precisions, train_recalls, train_f1s = [], [], [], []\n","val_accuracies, val_precisions, val_recalls, val_f1s = [], [], [], []\n","patience = 5\n","\n","for epoch in range(num_epochs):\n","    # Training\n","    train_loss, train_acc, train_precision, train_recall, train_f1 = train(model, train_loader, criterion, optimizer, device)\n","    train_losses.append(train_loss)\n","    train_accuracies.append(train_acc)\n","    train_precisions.append(train_precision)\n","    train_recalls.append(train_recall)\n","    train_f1s.append(train_f1)\n","    \n","    # Update the learning rate using the scheduler\n","    scheduler.step()\n","    \n","    # Validation\n","    val_loss, val_acc, val_precision, val_recall, val_f1 = validate(model, val_loader, criterion, device)\n","    val_losses.append(val_loss)\n","    val_accuracies.append(val_acc)\n","    val_precisions.append(val_precision)\n","    val_recalls.append(val_recall)\n","    val_f1s.append(val_f1)\n","    \n","    # Early stopping\n","    if len(val_losses) > patience:\n","        last_losses = val_losses[-patience:]\n","        if all(last_losses[i] <= last_losses[i+1] for i in range(patience-1)):\n","            print(\"Validation loss did not improve for {} consecutive epochs. Early stopping...\".format(patience))\n","            break\n","    \n","    print(f\"Epoch {epoch+1}, train_loss: {train_loss:.4f}, val_loss: {val_loss:.4f}\")\n","    print(f\"train_acc {train_acc: .4f} | val_acc: {val_acc: .4f}\")\n","    print(f\"train_precision: {train_precision:.4f} | val_precision: {val_precision:.4f}\")\n","    print(f\"train_recall: {train_recall:.4f} | val_recall: {val_recall:.4f}\")\n","    print(f\"train_f1_score: {train_f1:.4f} | val_f1_score: {val_f1:.4f}\")\n","    print('------------------------------------')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-24T19:21:38.010103Z","iopub.status.busy":"2023-05-24T19:21:38.009589Z","iopub.status.idle":"2023-05-24T19:21:40.812921Z","shell.execute_reply":"2023-05-24T19:21:40.811643Z","shell.execute_reply.started":"2023-05-24T19:21:38.010049Z"},"trusted":true},"outputs":[],"source":["# Evaluate performance of a random batch\n","batch_images, batch_labels = next(iter(val_loader))\n","\n","batch_preds = model(batch_images.to(device))\n","batch_preds = (torch.sigmoid(batch_preds.to(device)) >= 0.5).float()\n","\n","# Update the 'labels' list to include the correct subset of classes\n","evaluated_classes = [labels[i] for i in range(len(labels)) if i in torch.unique(torch.argmax(batch_labels, dim=1))]\n","true_labels_subset = [labels[i] for i in torch.argmax(batch_labels, dim=1).cpu().numpy()]  # Convert class indices to class names\n","predicted_labels_subset = [labels[i] for i in torch.argmax(batch_preds, dim=1).cpu().numpy()]  # Convert class indices to class names\n","# print(true_labels_subset)\n","print(predicted_labels_subset)\n","# Compute confusion matrix\n","cm = confusion_matrix(true_labels_subset, predicted_labels_subset, labels=labels)\n","\n","# Plot confusion matrix\n","plt.figure(figsize=(8, 8))\n","sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=labels, yticklabels=labels)\n","plt.xlabel(\"Predicted Labels\")\n","plt.ylabel(\"True Labels\")\n","plt.title(\"Confusion Matrix\")\n","plt.show()\n","\n","# Compute and print classification report\n","report = classification_report(true_labels_subset, predicted_labels_subset, zero_division=1)#, target_names=evaluated_classes, )\n","print(report)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Early stopping\n","In the provided implementation, the early stopping mechanism is implemented using a patience value, which is set to 5. After each epoch, the validation loss is computed. If the number of validation losses recorded is greater than the patience value, the last patience number of losses are extracted and then we check whether the last patience losses are less than or equal to the subsequent loss. This way we examine if the validation loss did not improve for patience consecutive epochs. If this condition is satisfied, it means that the model's performance is not improving, and the training process is stopped by breaking out of the training loop. This implementation of early stopping ensures that the training process is terminated when there is no significant improvement in the validation loss over a specified number of epochs. By stopping the training early, it helps prevent overfitting and saves computational resources."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-24T19:21:40.815663Z","iopub.status.busy":"2023-05-24T19:21:40.814854Z","iopub.status.idle":"2023-05-24T19:21:40.827874Z","shell.execute_reply":"2023-05-24T19:21:40.826359Z","shell.execute_reply.started":"2023-05-24T19:21:40.815613Z"},"trusted":true},"outputs":[],"source":["# Plot images alongside corresponding model predictions\n","def plot_batch(batch_images, batch_labels, labels, model):\n","    batch_preds = model(batch_images)\n","    batch_preds = (torch.sigmoid(batch_preds) >= 0.5).float()\n","\n","    fig, axes = plt.subplots(nrows=8, ncols=4, figsize=(20, 40))\n","    fig.tight_layout()\n","\n","    for idx, (image, label, pred) in enumerate(zip(batch_images.cpu(), batch_labels, batch_preds)):\n","        row = idx // 4\n","        col = idx % 4\n","\n","        image = image.permute(1, 2, 0).clamp(0, 1)\n","        axes[row, col].imshow(image)\n","\n","        true_labels = [labels[i] for i, val in enumerate(label) if val == 1.0]\n","        pred_labels = [labels[i] for i, val in enumerate(pred) if val == 1.0]\n","\n","        axes[row, col].set_title(f\"True Labels: {true_labels}\\nPredicted Labels: {pred_labels}\")\n","        axes[row, col].axis('off')\n","\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-24T19:21:40.830227Z","iopub.status.busy":"2023-05-24T19:21:40.829716Z","iopub.status.idle":"2023-05-24T19:21:47.738179Z","shell.execute_reply":"2023-05-24T19:21:47.736433Z","shell.execute_reply.started":"2023-05-24T19:21:40.830186Z"},"trusted":true},"outputs":[],"source":["batch_images, batch_labels = next(iter(val_loader))\n","plot_batch(batch_images.to(device), batch_labels.to(device), labels, model)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Evaluate - Interpret visualisations\n","\n","* Good Cases:\n","\n","**Distinctive objects**: Objects such as 'aeroplane', 'bicycle', 'bird', 'boat', 'bus', 'motorbike', 'person', 'train', and 'tvmonitor' have distinct visual features that make them relatively easier to classify correctly. These objects are likely to have well-defined shapes and unique characteristics that can be captured by the model, leading to accurate predictions.\n","\n","**Sufficient representation**: Having a sufficient number of examples for each class ensures that the model can learn the specific patterns and features associated with each object, reducing the chances of misclassification. It is obvious that in our case only the predominant **'person'** class is most of the times correctly classified. \n","\n","* Bad Cases:\n","\n","**Imbalanced dataset**: The dataset is highly imbalanced, meaning some classes have significantly more samples than others, the model may struggle to perform well on the minority classes. Objects such as **'bottle'**, **'cow'**, **'sheep'**, **'bicycle'**, **'train'**, and **'bus'** have fewer examples, leading to reduced exposure and lower accuracy for these classes.\n","\n","* Potential Mistakes:\n","\n","**Confusion between similar classes**: Objects that share visual similarities, such as **'car'** and **'bus'**, **'sheep'** and **'cow'**, or **'chair'** and **'sofa'**, can be a source of confusion for the model. The model may struggle to differentiate between these objects, leading to misclassifications, especially when the dataset does not provide sufficient discriminative features or examples for these classes."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Plotting the metrics"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-24T19:21:47.747316Z","iopub.status.busy":"2023-05-24T19:21:47.741465Z","iopub.status.idle":"2023-05-24T19:21:47.756283Z","shell.execute_reply":"2023-05-24T19:21:47.755166Z","shell.execute_reply.started":"2023-05-24T19:21:47.747272Z"},"trusted":true},"outputs":[],"source":["def plot_loss(train_losses, val_losses):\n","    epochs = range(1, len(train_losses) + 1)\n","    plt.plot(epochs, train_losses, label='Training Loss')\n","    plt.plot(epochs, val_losses, label='Validation Loss')\n","    plt.xlabel('Epochs')\n","    plt.ylabel('Loss')\n","    plt.title('Training and Validation Loss')\n","    plt.legend()\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-24T19:21:47.758866Z","iopub.status.busy":"2023-05-24T19:21:47.758063Z","iopub.status.idle":"2023-05-24T19:21:47.769815Z","shell.execute_reply":"2023-05-24T19:21:47.768792Z","shell.execute_reply.started":"2023-05-24T19:21:47.758785Z"},"trusted":true},"outputs":[],"source":["def plot_accuracy(train_accuracies, val_accuracies):\n","    epochs = range(1, len(train_accuracies) + 1)\n","    plt.plot(epochs, train_accuracies, label='Training Accuracy')\n","    plt.plot(epochs, val_accuracies, label='Validation Accuracy')\n","    plt.xlabel('Epochs')\n","    plt.ylabel('Accuracy')\n","    plt.title('Training and Validation Accuracy')\n","    plt.legend()\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-24T19:21:47.772264Z","iopub.status.busy":"2023-05-24T19:21:47.77153Z","iopub.status.idle":"2023-05-24T19:21:47.781952Z","shell.execute_reply":"2023-05-24T19:21:47.780935Z","shell.execute_reply.started":"2023-05-24T19:21:47.772223Z"},"trusted":true},"outputs":[],"source":["def plot_precision(train_precisions, val_precisions):\n","    epochs = range(1, len(train_precisions) + 1)\n","    plt.plot(epochs, train_precisions, label='Training Precision')\n","    plt.plot(epochs, val_precisions, label='Validation Precision')\n","    plt.xlabel('Epochs')\n","    plt.ylabel('Precision')\n","    plt.title('Training and Validation Precision')\n","    plt.legend()\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-24T19:21:47.784579Z","iopub.status.busy":"2023-05-24T19:21:47.783722Z","iopub.status.idle":"2023-05-24T19:21:47.797307Z","shell.execute_reply":"2023-05-24T19:21:47.795888Z","shell.execute_reply.started":"2023-05-24T19:21:47.784518Z"},"trusted":true},"outputs":[],"source":["def plot_recall(train_recalls, val_recalls):\n","    epochs = range(1, len(train_recalls) + 1)\n","    plt.plot(epochs, train_recalls, label='Training Recall')\n","    plt.plot(epochs, val_recalls, label='Validation Recall')\n","    plt.xlabel('Epochs')\n","    plt.ylabel('Recall')\n","    plt.title('Training and Validation Recall')\n","    plt.legend()\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-24T19:21:47.799694Z","iopub.status.busy":"2023-05-24T19:21:47.798966Z","iopub.status.idle":"2023-05-24T19:21:47.807844Z","shell.execute_reply":"2023-05-24T19:21:47.806541Z","shell.execute_reply.started":"2023-05-24T19:21:47.799634Z"},"trusted":true},"outputs":[],"source":["def plot_f1_score(train_f1_scores, val_f1_scores):\n","    epochs = range(1, len(train_f1_scores) + 1)\n","    plt.plot(epochs, train_f1_scores, label='Training F1 Score')\n","    plt.plot(epochs, val_f1_scores, label='Validation F1 Score')\n","    plt.xlabel('Epochs')\n","    plt.ylabel('F1 Score')\n","    plt.title('Training and Validation F1 Score')\n","    plt.legend()\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-24T19:21:47.8111Z","iopub.status.busy":"2023-05-24T19:21:47.809933Z","iopub.status.idle":"2023-05-24T19:21:49.230192Z","shell.execute_reply":"2023-05-24T19:21:49.228874Z","shell.execute_reply.started":"2023-05-24T19:21:47.811046Z"},"trusted":true},"outputs":[],"source":["# Plot the metrics\n","plot_loss(train_losses, val_losses)\n","plot_accuracy(train_accuracies, val_accuracies)\n","plot_precision(train_precisions, val_precisions)\n","plot_recall(train_recalls, val_recalls)\n","plot_f1_score(train_f1s, val_f1s)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### ComplexNetwork - Discussion\n","\n","**Loss**: The training loss decreases consistently from epoch to epoch, indicating that the model is learning and improving its predictions. However, the validation loss fluctuates and does not show a clear decreasing trend, which suggests that the model might be overfitting the training data.\n","\n","**Accuracy**: The training accuracy starts at 0.9167 in the first epoch and gradually increases to 0.9296 in the final epoch. The validation accuracy also improves but fluctuates around 0.9187, indicating that the model generalizes moderately well to unseen data.\n","\n","**Precision**: The training precision starts at 0.1164 and increases to 0.7508 in the last epoch. The validation precision also improves but fluctuates between 0.6519 and 0.9655, indicating some inconsistency in the model's ability to correctly predict positive samples.\n","\n","**Recall**: The training recall starts at 0.0335 and increases to 0.1048 in the final epoch, while the validation recall shows a similar trend but with lower values, ranging from 0.0069 to 0.1172. This suggests that the model struggles to capture all positive instances, especially in the validation set.\n","\n","**F1-Score**: The training F1-score starts at 0.0501 and increases to 0.1669 in the last epoch. The validation F1-score also improves but with values ranging from 0.0110 to 0.1568, indicating that the model has room for improvement in terms of finding the right balance between precision and recall."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### TinyVGG Implementation - Discussion\n","\n","**Loss**: The training loss decreases consistently from epoch to epoch, indicating that the model is learning and improving its predictions. However, the validation loss fluctuates and does not show a clear decreasing trend, which suggests that the model might be overfitting the training data.\n","\n","**Accuracy**: The training accuracy starts at 0.3822 in the first epoch and gradually increases to 0.6005 in the final epoch. The validation accuracy also improves but fluctuates around 0.6, indicating that the model generalizes moderately well to unseen data.\n","\n","**Precision**: The training precision starts at 0.1129 and increases to 0.7892 in the last epoch. The validation precision also improves but fluctuates between 0.3037 and 1.0, indicating some inconsistency in the model's ability to correctly predict positive samples.\n","\n","**Recall**: The training recall starts at 0.4242 and increases to 0.5499 in the final epoch, while the validation recall shows a similar trend but with lower values, ranging from 0.0 to 0.0183. This suggests that the model struggles to capture all positive instances, especially in the validation set.\n","\n","**F1-Score**: The training F1-score starts at 0.1660 and increases to 0.6461 in the last epoch. The validation F1-score also improves but with values ranging from 0.0 to 0.0337, indicating that the model has room for improvement in terms of finding the right balance between precision and recall."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### SimpleNetwork - Discussion\n","\n","**Loss**: The training loss gradually decreases over the epochs, indicating that the model is learning and improving its ability to minimize the difference between predicted and actual values. However, the validation loss initially decreases but then starts to increase again after a few epochs, suggesting that the model may be overfitting to the training data.\n","\n","**Accuracy**: Both the training and validation accuracies hover around 0.60, indicating that the model performs similarly on both the training and validation sets. However, the accuracy does not improve significantly over the epochs, suggesting that the model may have reached a performance limit.\n","\n","**Precision**: The precision values for both training and validation sets are relatively high, especially for the validation set. This indicates that when the model predicts a positive result, it is often correct, particularly for the validation data. The precision values improve over the epochs, suggesting that the model becomes better at minimizing false positives.\n","\n","**Recall**: The recall values are relatively low for both training and validation sets, indicating that the model has a tendency to miss true positive results. The recall values remain relatively stable and do not show significant improvement over the epochs.\n","\n","**F1 Score**: The F1 scores follow a similar pattern to the recall values. They are relatively low and do not show substantial improvement over the epochs. This suggests that the model struggles to balance precision and recall effectively."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Overall discussion"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Training a model from scratch on a small, imbalanced dataset with 20 classes is challenging for several reasons, which can lead to poor performance despite taking various measures to improve it. Here are some reasons why such a scenario is generally expected to result in suboptimal performance:\n","\n","* **Limited Data**: With a small dataset, the model has limited examples to learn from, making it difficult to capture the complexity and variability of the 20 classes adequately. Deep learning models typically require a large amount of diverse data to generalize well.\n","\n","* **Class Imbalance**: Imbalanced datasets pose a significant challenge as the model may become biased towards the majority classes, often at the expense of the minority classes. Despite assigning weights to each class, which can help address the imbalance to some extent, the scarcity of samples from minority classes can still block the model's ability to learn and generalize effectively.\n","\n","* **Complexity of the Problem**: Handling a classification problem with 20 classes is inherently more complex than binary or few-class classification. Each additional class introduces more variations, making it harder for the model to discriminate between them accurately.\n","\n","* **Model Capacity and Architecture**: Both shallow and deep CNN architectures have their limitations. Shallow architectures (SimpleNetwork, ComplexNetwork, TinyVGG) may lack the capacity to capture complex patterns and relationships in the data, while deep architectures (AlexNet, ResNet-34) might struggle with overfitting due to limited training data. Augmentation techniques can help increase the effective size of the dataset, but they cannot fully compensate for the inherent limitations of a small dataset.\n","\n","* **Adaptive Learning Rate and Scheduler**: Using adaptive learning rate techniques and schedulers can help in stabilizing and optimizing the learning process. However, while they can improve training dynamics and convergence, they might not be sufficient to overcome the challenges posed by the small, imbalanced dataset alone, **as it is evident in our case**."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## 2.2.3 Loading Pre-trained ResNet50 Model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-24T19:21:49.234424Z","iopub.status.busy":"2023-05-24T19:21:49.234083Z","iopub.status.idle":"2023-05-24T19:21:50.554807Z","shell.execute_reply":"2023-05-24T19:21:50.553527Z","shell.execute_reply.started":"2023-05-24T19:21:49.234391Z"},"trusted":true},"outputs":[],"source":["# Load the pre-trained ResNet50 model\n","model = models.resnet50(pretrained=True)\n","\n","# Modify the classifier\n","num_features = model.fc.in_features\n","num_classes = 20\n","model.fc = torch.nn.Linear(num_features, num_classes)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### 2.2.4 Set up the model and Optimizer\n","- Load the pre-trained VGG model and replace the last layer for classification.\n","- Move the model to the device.\n","- Set up the loss function and optimizer.\n","\n","### Motivation for Loss function and optimizer\n","\n","**Loss function:**\n","\n","The `BCEWithLogitsLoss` (Binary Cross-Entropy with Logits Loss) is often used in multi-label classification problems, where each input can belong to multiple classes and each label is independent of each other. The loss function combines the binary cross-entropy loss and the sigmoid activation function in a single class. This provides high numerical stability. The equation for Binary Cross-Entropy is given as:\n","\n","\\begin{equation}\n","L(y, t)=-\\left(t^* \\log (y)+(1-t) * \\log (1-y)\\right)\n","\\end{equation}\n","\n","with L(y,t) the loss, y the predicted probability and t the true label (0 or 1). y is obtained by applying the sigmoid function to the output logit from the model.\n","\n","The `weight` parameter allows to specify a weight tensor. This weight tensor assigns different weights to each class individually in multilabel classification problems. It allows to handle class imbalance by assigning higher weights to underrepresented classes and lower weights to overrepresented classes."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-24T19:21:50.557519Z","iopub.status.busy":"2023-05-24T19:21:50.556852Z","iopub.status.idle":"2023-05-24T19:21:50.606412Z","shell.execute_reply":"2023-05-24T19:21:50.605244Z","shell.execute_reply.started":"2023-05-24T19:21:50.557465Z"},"trusted":true},"outputs":[],"source":["# Move the model to the device (GPU if available)\n","model = model.to(device)\n","model.eval()\n","\n","# Set up the loss function and the optimizer\n","criterion = nn.BCEWithLogitsLoss(weight=weights_tensor)\n","optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### 2.2.5 Training loop\n","- Define lists to store training and validation metrics.\n","- Iterate over epochs.\n","- Set the model in training mode.\n","- Iterate over batches in the train DataLoader.\n","- Move inputs and targets to the device.\n","- Zero the gradients and perform forward and backward passes.\n","- Calculate and store training metrics.\n","- Set the model in evaluation mode.\n","- Iterate over batches in the validation DataLoader.\n","- Calculate and store validation metrics.\n","- Print and store the metrics for each epoch.\n","\n","### Motivation for Model Evaluation Metrics and Implementation:\n","\n","\n","Since we are dealing with a multi-label classification problem. The sigmoid function is used. This function converts the model's output logits to probabilties. This allow each class's probability to be independent of each other, allowing for multiple classes to have a high probability.\n","\n","**Accuracy:**\n","\n","In determining the appropriateness of accuracy as a metric for our problem, it is essential to understand the definition of accuracy within the context of multi-label classification. Accuracy can be formally defined as:\n","\n","\\begin{equation}\n","\\text { Accuracy }=\\frac{\\text { True Positives }+ \\text { True Negatives }}{\\text { True Positives }+ \\text { True Negatives }+ \\text { False Positives }+ \\text { False Negatives }}\n","\\end{equation}\n","\n","Imagine the following truth and predicted labels:\n","\n","$y_{true} = [1, 0, 1, 0, 0, 0]$\n","\n","$y_{pred} = [0, 1, 1, 0, 1, 0]$\n","\n","In this case we have:\n","- 1 false negative (1st element)\n","- 2 false positives (2nd and 5th elements)\n","- 2 true negatives (4th and 6 th elements)\n","- 1 true positive (3rd element)\n","\n","The accuracy is:\n","\n","\\begin{equation}\n","\\text { Accuracy }=\\frac{1+2}{1+2+2+1}=\\frac{3}{6}=50 \\%\n","\\end{equation}\n","\n","The obtained accuracy is unexpectedly high, given that only 1 out of 6 labels are correctly predicted. In practice, we are dealing with 20 classes, which implies that the predicted array would be significantly larger and likely contain a greater proportion of zeros (as each image is expected to belong to a limited number of classes). \n","\n","**F1 Score:**\n","\n","The F1-score was chosen, because it provides a balanced model's performance assessment, combining both precision and recall. Precision defined as: \n","\n","\\begin{equation}\n","\\text { Precision }=\\frac{\\mathrm{TP}}{\\mathrm{TP}+\\mathrm{FP}}\n","\\end{equation}\n","\n","focuses on measuring the model's ability to correctly identify positive instances. Whereas recall, defined as:\n","\n","\\begin{equation}\n","\\text { Recall }=\\frac{\\mathrm{TP}}{\\mathrm{TP}+\\mathrm{FN}}\n","\\end{equation}\n","\n","measures the ability of the model to identify all positive instances within the dataset. The F1-score defined as: \n","\n","\\begin{equation}\n","F 1-\\frac{2 * \\text { Precision* Recall }}{\\text { Precision }+ \\text { Recall }}\n","\\end{equation}\n","\n","provides a mean of precision and recall, allowing for equal contribution of both metrics in evaluating the model's performance.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-24T19:21:50.608979Z","iopub.status.busy":"2023-05-24T19:21:50.608502Z","iopub.status.idle":"2023-05-24T19:22:52.723393Z","shell.execute_reply":"2023-05-24T19:22:52.721662Z","shell.execute_reply.started":"2023-05-24T19:21:50.608934Z"},"trusted":true},"outputs":[],"source":["# Set the number of epochs\n","num_epochs = 2\n","\n","# Initialize lists to store the training and validation loss values\n","train_f1_scores = []\n","val_f1_scores = []\n","train_avg_precisions = []\n","val_avg_precisions = []\n","train_losses = []\n","val_losses = []\n","\n","# Training loop\n","for epoch in range(num_epochs):\n","    running_loss = 0.0\n","    running_val_loss = 0.0\n","    epoch_train_true = []\n","    epoch_train_pred = []\n","    epoch_val_true = []\n","    epoch_val_pred = []\n","    model.train()\n","\n","    # Print the current epoch number\n","    print(f'Epoch {epoch + 1}/{num_epochs}')\n","\n","    # Train the model\n","    for batch_idx, (inputs, targets) in enumerate(train_loader):\n","        # Move inputs and targets to the device (GPU if available)\n","        inputs, targets = inputs.to(device), targets.to(device)\n","\n","        # Zero the parameter gradients\n","        optimizer.zero_grad()\n","\n","        # Forward pass\n","        outputs = model(inputs)\n","        loss = criterion(outputs, targets.float().to(device))\n","\n","        # Store true and predicted labels for F1 score and average precision calculation\n","        epoch_train_true.extend(targets.cpu().numpy().tolist())\n","        epoch_train_pred.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n","\n","        # Backward pass\n","        loss.backward()\n","        optimizer.step()\n","\n","        # Print statistics\n","        running_loss += loss.item()\n","\n","    train_losses.append(running_loss / len(train_loader))\n","\n","    # Validate the model\n","    model.eval()\n","    with torch.no_grad():\n","        for inputs, targets in val_loader:\n","            inputs, targets = inputs.to(device), targets.to(device)\n","            outputs = model(inputs)\n","\n","            # Multiply targets by the class_weights_tensor to get weighted_targets\n","            loss = criterion(outputs, targets.float().to(device))\n","            running_val_loss += loss.item()\n","\n","            # Store true and predicted labels for F1 score and average precision calculation\n","            epoch_val_true.extend(targets.cpu().numpy().tolist())\n","            epoch_val_pred.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n","\n","        val_losses.append(running_val_loss / len(val_loader))\n","\n","    \n","    # Convert the lists to NumPy arrays\n","    epoch_train_true = np.array(epoch_train_true)\n","    epoch_train_pred = np.array(epoch_train_pred)\n","    epoch_val_true = np.array(epoch_val_true)\n","    epoch_val_pred = np.array(epoch_val_pred)\n","\n","    # Apply a threshold to the predicted labels\n","    threshold = 0.5\n","    epoch_train_pred = (epoch_train_pred >= threshold).astype(int)\n","    epoch_val_pred = (epoch_val_pred >= threshold).astype(int)\n","\n","    n_classes = 20\n","    train_binarized_true = epoch_train_true\n","    train_binarized_pred = epoch_train_pred\n","    val_binarized_true = epoch_val_true\n","    val_binarized_pred = epoch_val_pred\n","\n","    train_avg_precision = average_precision_score(train_binarized_true, train_binarized_pred, average='weighted')\n","    val_avg_precision = average_precision_score(val_binarized_true, val_binarized_pred, average='weighted')\n","\n","    # Calculate F1 scores and average precision values\n","    train_f1 = f1_score(epoch_train_true, epoch_train_pred, average='weighted')\n","    val_f1 = f1_score(epoch_val_true, epoch_val_pred, average='weighted')\n","\n","    train_f1_scores.append(train_f1)\n","    val_f1_scores.append(val_f1)\n","    train_avg_precisions.append(train_avg_precision)\n","    val_avg_precisions.append(val_avg_precision)\n","\n","    print(f'Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_losses[-1]:.4f}, Val Loss: {val_losses[-1]:.4f}, '\n","          f'Train F1: {train_f1:.4f}, Val F1: {val_f1:.4f}, Train Avg Precision: {train_avg_precision:.4f}, '\n","          f'Val Avg Precision: {val_avg_precision:.4f}')\n","    \n","print('Finished Training')"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### 2.2.6 Verification code"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-24T19:22:52.727277Z","iopub.status.busy":"2023-05-24T19:22:52.725686Z","iopub.status.idle":"2023-05-24T19:22:52.736312Z","shell.execute_reply":"2023-05-24T19:22:52.734857Z","shell.execute_reply.started":"2023-05-24T19:22:52.727219Z"},"trusted":true},"outputs":[],"source":["labels = train_df.columns[0:20]\n","\n","for i, label in enumerate(labels):\n","    print(f\"{i+1}: {label}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-24T19:22:52.739632Z","iopub.status.busy":"2023-05-24T19:22:52.738675Z","iopub.status.idle":"2023-05-24T19:22:55.001635Z","shell.execute_reply":"2023-05-24T19:22:55.00036Z","shell.execute_reply.started":"2023-05-24T19:22:52.739582Z"},"trusted":true},"outputs":[],"source":["# Get class names from labels\n","class_names = list(train_df.columns[0:20])\n","\n","# Number of samples you want to print\n","num_samples_to_print = 2\n","\n","for i in range(len(val_dataset)):\n","    sample, true_labels = val_dataset[i]\n","    inputs = sample.unsqueeze(0).to(device)\n","    true_labels = [class_names[j] for j, val in enumerate(true_labels) if val == 1]\n","\n","    model.eval()\n","    with torch.no_grad():\n","        outputs = model(inputs)\n","        probabilities = torch.sigmoid(outputs).cpu().numpy().tolist()[0]\n","        predicted_labels = [class_names[j] for j, val in enumerate(probabilities) if val >= 0.5]\n","\n","    # Print only the first num_samples_to_print samples\n","    if i < num_samples_to_print:\n","        print(f\"Sample {i+1}:\")\n","        print(f\"True Labels: {true_labels}\")\n","        print(f\"Predicted Labels: {predicted_labels}\")\n","        \n","        for j, prob in enumerate(probabilities):\n","            class_label = class_names[j]\n","            print(f\"{j+1}: {class_label}: {prob}\")\n","\n","        print()\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### 2.2.7 Visualizing Training and Validation Metrics: Loss, F1 Score and Average Precision"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-24T19:22:55.005134Z","iopub.status.busy":"2023-05-24T19:22:55.00348Z","iopub.status.idle":"2023-05-24T19:22:56.136472Z","shell.execute_reply":"2023-05-24T19:22:56.13522Z","shell.execute_reply.started":"2023-05-24T19:22:55.005087Z"},"trusted":true},"outputs":[],"source":["# Create a 1x3 grid of subplots\n","fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15, 5))\n","\n","# Plot the training and validation loss\n","axes[0].plot(train_losses, label='Train Loss')\n","axes[0].plot(val_losses, label='Validation Loss')\n","axes[0].set_xlabel('Epoch')\n","axes[0].set_ylabel('Loss')\n","axes[0].legend()\n","\n","# Plot the training and validation F1 scores\n","axes[1].plot(train_f1_scores, label='Train F1 Score')\n","axes[1].plot(val_f1_scores, label='Validation F1 Score')\n","axes[1].set_xlabel('Epoch')\n","axes[1].set_ylabel('F1 Score')\n","axes[1].legend()\n","\n","# Plot the training and validation average precision\n","axes[2].plot(train_avg_precisions, label='Train Avg Precision')\n","axes[2].plot(val_avg_precisions, label='Validation Avg Precision')\n","axes[2].set_xlabel('Epoch')\n","axes[2].set_ylabel('Average Precision')\n","axes[2].legend()\n","\n","# Adjust the spacing between subplots\n","plt.subplots_adjust(wspace=0.3)\n","\n","# Display the plots\n","plt.show()\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### 2.2.8 Adding the results to the dataframe"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-24T19:22:56.144946Z","iopub.status.busy":"2023-05-24T19:22:56.142053Z","iopub.status.idle":"2023-05-24T19:23:10.008503Z","shell.execute_reply":"2023-05-24T19:23:10.007231Z","shell.execute_reply.started":"2023-05-24T19:22:56.144895Z"},"trusted":true},"outputs":[],"source":["# Get class names from labels\n","class_names = list(test_df.columns[0:20])\n","class_vector = np.empty((750,20))\n","print(class_names)\n","\n","for i in range(750):\n","\n","    if i % 100 == 0:\n","        print(i)\n","    im = test_dataset.__getitem__(i)[0]\n","    im = np.expand_dims(im,axis=0)\n","    im = torch.tensor(im).to(device)\n","\n","    model.eval()\n","    with torch.no_grad():\n","        outputs = model(im)\n","        probabilities = torch.sigmoid(outputs).cpu().numpy().tolist()[0]\n","\n","    for c in range(len(class_names)):\n","        class_vector[i,c] = probabilities[c]\n","    \n","\n","for c in range(len(class_names)):\n","    test_df[class_names[c]][:] = class_vector[:,c]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-24T19:23:10.011324Z","iopub.status.busy":"2023-05-24T19:23:10.010456Z","iopub.status.idle":"2023-05-24T19:23:30.875139Z","shell.execute_reply":"2023-05-24T19:23:30.873866Z","shell.execute_reply.started":"2023-05-24T19:23:10.011275Z"},"trusted":true},"outputs":[],"source":["test_df.head(5)"]},{"attachments":{},"cell_type":"markdown","metadata":{"papermill":{"duration":0.19763,"end_time":"2022-04-12T14:49:07.53601","exception":false,"start_time":"2022-04-12T14:49:07.33838","status":"completed"},"tags":[]},"source":["# 3. Semantic segmentation\n","The goal here is to implement a segmentation CNN that labels every pixel in the image as belonging to one of the 20 classes (and/or background). Use the training set to train your CNN and compete on the test set (by filling in the segmentation column in the test dataframe).\n","\n","# 3.0 Segmentation oriented CNNs\n","\n","Given the assignment and the tasks we were called to solve, we reviewed the related literature [1,2] on the issue. Two of the most fundamental deep learning architectures for the task of semantic image segmentation in the last decade are ResNet[3] and U-Net[4]. The latter constitutes one of the earliest forms of DL-based techniques to tackle the subject of image segmentation in the context of biomedicine, while the former constitutes the backbone of the majority of the latest implementations of complex neural networks to tackle the task. \n","Given that the resources provided (part of PASCAL VOC 2009) were limited, as well as our training resources, we decided to work with the aforementioned architectures, as they are proven to be efficient, while also providing us the opportunity to explore the basis of the DL-based approach theory on the task. Due to ResNet being more computationally demanding, we chose to use its variant - ResNet50 - in its pretrained form, while implementing from scratch and training U-Net, which was easier to implement from scratch and less computationally demanding to train.\n","\n","Convolutional Neural Networks for sematic segmentation use an Encoder-Decoder architecture where the encoder is tasked with discriminating pixels while the decoder is tasked with projecting the results into regions. "]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["References:\n","- 1: https://doi.org/10.1016/j.asoc.2018.05.018\n","- 2: https://doi.org/10.1109/TPAMI.2021.3059968\n","- 3: https://arxiv.org/abs/1512.03385\n","- 4: https://arxiv.org/abs/1505.04597"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## 3.1 Segmentation using UNet\n","\n","Some notable examples of models used in semantic segmentation are ResNet and UNet. The following section shows a prediction example of the Pre-trained ResNet50 model, which we took as a guide on how the input and output tensors should look like as well as the overall appearance of the segmentation result."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-24T19:23:30.877631Z","iopub.status.busy":"2023-05-24T19:23:30.877207Z","iopub.status.idle":"2023-05-24T19:23:43.973791Z","shell.execute_reply":"2023-05-24T19:23:43.972795Z","shell.execute_reply.started":"2023-05-24T19:23:30.877588Z"},"trusted":true},"outputs":[],"source":["from torchvision.models.segmentation.fcn import FCN_ResNet50_Weights\n","from torchvision.models.segmentation.fcn import fcn_resnet50\n","from torchvision.transforms.functional import *\n","\n","# Here we try the pre-processed model ResNet50 for Image Segmentation\n","\n","indextoget = 5\n","\n","img = pvocS_val.__getitem__(indextoget)[0]\n","\n","print(img)\n","\n","# Initialize model with the best available weights\n","weights = FCN_ResNet50_Weights.DEFAULT\n","model = fcn_resnet50(weights=weights)\n","model.eval() # set the model to evaluation to avoid saving gradients\n","\n","# Initialize the inference transforms (resize + transforms such as normalization)\n","preprocess = weights.transforms()\n","\n","# Apply inference preprocessing transforms\n","batch = preprocess(img).unsqueeze(0)\n","\n","# Use the model\n","prediction = model(batch)[\"out\"]\n","normalized_masks = prediction.softmax(dim=1)\n","\n","# See the result\n","class_to_idx = {cls: idx for (idx, cls) in enumerate(weights.meta[\"categories\"])}\n","\n","fig, axs = plt.subplots(1, 21, figsize=(5 * 20, 5 * 2))\n","for cla in class_to_idx:\n","    mask = normalized_masks[0, class_to_idx[cla]]\n","    axs[class_to_idx[cla]].imshow(mask.to('cpu').detach().numpy())\n","    axs[class_to_idx[cla]].set_title(cla,fontsize=14)\n","    \n","# doubleclick the plots to zoom in\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["We implemented UNet, a fully convolutional network that includes both encoder and decoder and skip connections.\n","\n","Skip connections essencial in the reconstruction of the images's original features during the decoding phase. In Unet, skip connections are implemented by concatenating the results of selected layers during the encoder phase to selected layers of the decoder phase.\n","\n","UNet's encoder consists of four \"double convolution\" steps that halve the image size while doubleing the image features where the following are applied twice in a sequence:\n","- 2D convolution (the first time it doubles the features)\n","- Normalization\n","- ReLu\n","\n","Then a MaxPooling Layer reducing the image area to a quarter after each \"double convolution\".\n","\n","There is one middle \"bottleneck\" phase using the same \"double convolution\".\n","\n","The decoder phase follows a similar architecture to the encoder.\n","First, a Transposed convolution layer that increases the image area four times is applied.\n","Then, the following are applied twice:\n","- 2D convolution (the first time it halves the features)\n","- Normalization\n","- ReLu\n","    \n","Finally, the last step is another \"double convolution\" followed by a sigmoid layer. Of note, this layer is also included in the loss fucntion that we implemented. To avoid applying it twice, we added it manually when needed in several places.\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-24T19:23:43.976711Z","iopub.status.busy":"2023-05-24T19:23:43.975731Z","iopub.status.idle":"2023-05-24T19:23:44.004316Z","shell.execute_reply":"2023-05-24T19:23:44.002922Z","shell.execute_reply.started":"2023-05-24T19:23:43.976654Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torchvision.transforms.functional as TF\n","\n","class doubleConvolution(nn.Module):\n","\n","    def __init__(self, in_channels, out_channels):\n","        super(doubleConvolution,self).__init__()\n","\n","        # create double convolution step\n","        self.conv = nn.Sequential(\n","            nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1, bias=False),\n","            nn.BatchNorm2d(out_channels),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1, bias=False),\n","            nn.BatchNorm2d(out_channels),\n","            nn.ReLU(inplace=True),\n","        )\n","\n","    # how it has to process it:\n","    def forward(self,x):\n","        return self.conv(x)\n","    \n","\n","# Net class containing all the processes. 3 channels turn into one label\n","class UNET(nn.Module):\n","\n","    def __init__(self, in_channels = 3, out_channels = 20, features = [64, 128, 256, 512]):\n","        super(UNET,self).__init__()\n","\n","        self.debug = False\n","\n","        # create list of downwards and upwards processes\n","        self.ups = nn.ModuleList()\n","        self.downs = nn.ModuleList()\n","        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n","\n","        # down\n","        for feature in features:\n","            self.downs.append(doubleConvolution(in_channels, feature))\n","            in_channels = feature\n","        \n","        # up\n","        for feature in reversed(features):\n","            self.ups.append(nn.ConvTranspose2d(feature*2, feature, kernel_size=2, stride=2))\n","            self.ups.append(doubleConvolution(feature*2, feature))\n","\n","        # middle part    \n","        self.bottleneck = doubleConvolution(features[-1], features[-1]*2)\n","\n","        # final part \n","        self.finalconv = nn.Conv2d(features[0], out_channels, kernel_size=1)\n","\n","    def forward(self,x):\n","        skip_connections = []\n","\n","        if self.debug:\n","            print(\"Received \", x.shape)\n","            print(\"Downwards path: \")\n","\n","        for down in self.downs:\n","            x = down(x)\n","            skip_connections.append(x)\n","            x = self.pool(x)\n","           \n","        x = self.bottleneck(x)\n","\n","        skip_connections = skip_connections[::-1] # REVERSE that list\n","\n","        for idx in range(0, len(self.ups), 2):\n","            x = self.ups[idx](x)\n","           \n","            skip_cons = skip_connections[idx//2]\n","\n","            if x.shape != skip_cons.shape:\n","                x = TF.resize(x,size=skip_cons.shape[2:])\n","\n","            conc_skip_cat = torch.cat((skip_cons,x),1)\n","            x = self.ups[idx+1](conc_skip_cat)\n","\n","        x = self.finalconv(x)\n","       \n","        return x"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## 3.2 Training the NN\n","\n","The following are some useful functions for saving and loading checkpoints as well as plotting the predictions. A function was also included to introduce the data in test_df for sumbission purposes."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-24T19:23:44.008768Z","iopub.status.busy":"2023-05-24T19:23:44.008087Z","iopub.status.idle":"2023-05-24T19:23:50.106853Z","shell.execute_reply":"2023-05-24T19:23:50.105317Z","shell.execute_reply.started":"2023-05-24T19:23:44.008727Z"},"trusted":true},"outputs":[],"source":["import numpy as np  # Math efficiently\n","import torch\n","import torchvision\n","import torch.nn as nn\n","import torch.optim as optim\n","from torchmetrics import JaccardIndex\n","import albumentations as A\n","from albumentations.pytorch import ToTensorV2\n","from matplotlib import pyplot as plt\n","from matplotlib import image as img\n","from tqdm import tqdm\n","import os\n","\n","DEVICE = (\"cuda\"  if torch.cuda.is_available()  else \"mps\"  if torch.backends.mps.is_available() else \"cpu\")\n","print(f\"Using {DEVICE} device\")\n","\n","def save_checkpoint(state, filename=\"new_checkcpoint.pth.tar\"):\n","    print(\"-saving\")\n","    torch.save(state, filename)\n","\n","def load_checkpoint(checkpoint, model):\n","    print(\"-loading\")\n","    model.load_state_dict(checkpoint[\"state_dict\"])\n","\n","def save_predictions_as_images(loader, model, folder=\"Results\", device=\"cuda\",epoch=None, maxnum=20):\n","\n","    model.eval()\n","\n","    if os.path.exists(folder) ==  False:\n","        os.mkdir(folder)\n","        \n","    for idx, (x, y) in enumerate(loader):\n","\n","        y = torch.round(y*255) #values were provided divided by 255\n","        arrays = [y == c for c in range(20)] \n","        y = torch.tensor(np.stack(arrays, axis=1)) \n","        y = y.squeeze(2)\n","        x = x.float().to(device)\n","        y = y.float().to(device)\n","\n","        with torch.no_grad():\n","            preds = torch.sigmoid(model(x))\n","            # preds = preds * 255\n","            preds = (preds > 0.5).float()\n","\n","        X = x.cpu()\n","        Y = y.cpu()\n","        PREDS = preds.cpu()\n","\n","        for c in range(20):\n","            torchvision.utils.save_image(PREDS[:,c,:,:],f\"{folder}/index_{idx}_class_{c}_preds.png\")\n","            torchvision.utils.save_image(Y[:,c,:,:], f\"{folder}/index_{idx}_class_{c}_targets.png\")\n","\n","        if idx>=maxnum:\n","            break\n","\n","    model.train()\n","    \n","def add_predictions_to_test_df(test_dataframe,loader,model,device,batch_size,epoch=None):\n","\n","    model.eval()\n","    \n","    loop = tqdm(loader)\n","\n","    for idx, (x, y) in enumerate(loop):\n","\n","        y = torch.round(y*255) #target values were provided divided by 255 as indices of classes\n","        arrays = [y == c for c in range(20)] \n","        y = torch.tensor(np.stack(arrays, axis=1)) \n","        y = y.squeeze(2)\n","        x = x.float().to(device) \n","        y = y.float().to(device)\n","\n","        with torch.no_grad():\n","            preds = torch.sigmoid(model(x))\n","            # preds = preds * 255\n","            preds = (preds > 0.5).float()\n","\n","        PREDS = preds.cpu().numpy()\n","        \n","        this_batch = PREDS.shape[0]\n","        classes = PREDS.shape[1]\n","\n","        for i in range(this_batch):\n","            for c in range(classes):\n","                   test_df[\"seg\"][batch_size*i+c] = PREDS[i,c,:,:]\n","                \n","    model.train()\n","\n","    return test_dataframe"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Image segmentation being an extension of image classification, the motivations that lead to the design of the image classification model in prior section are also valid. Namely:\n","- The training loop follows the same fundamental steps aside from some tensor transformations to account for compatibility of the models.\n","- The loss function was BCEWithLogitsLoss parametrized with a 10 factor for the positive class.\n","\n","Some notable changes are the following:\n","- The optimizer that we used was Adam as did the authors of UNet.\n","- We used CUDA's GradScaler to increase precision and avoid underflow during backpropagation.\n","\n","Some quality of life functions included:\n","- tqdm to generate progress bars easily.\n","- The ability to save and load the model.\n","- The ability to save the model results as images per class.\n","\n","Transformation of the images was applied to resize them to a common format to enable training in batches of 8 to 16 images at once. \n","The interpolation mode used to resize the ground truth images was \"exact neighbour\" to avoid undesired anti-aliasing"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-24T19:23:50.110253Z","iopub.status.busy":"2023-05-24T19:23:50.109406Z","iopub.status.idle":"2023-05-24T19:25:13.690914Z","shell.execute_reply":"2023-05-24T19:25:13.68927Z","shell.execute_reply.started":"2023-05-24T19:23:50.110205Z"},"trusted":true},"outputs":[],"source":["# Makes the model load a checkpoint and plot the images in the results folder\n","PLOT_MODE = False\n","PLOT_EPOCH = 0\n","\n","# Parameters to load (overrride by plot mode)\n","LOAD_MODEL = False\n","LOAD_EPOCH = 0\n","\n","# Parameters\n","LEARNING_RATE = 0.0001\n","BATCH_SIZE = 8\n","NUM_EPOCHS = 2 # 301\n","IN_CHANNELS = 3\n","OUT_CHANNELS = 20\n","PIN_MEMORY = True\n","SAVE_FREQUENCY = 1\n","\n","if LOAD_MODEL == False:\n","    LOAD_EPOCH = 0\n","\n","# some preprocess\n","if PLOT_MODE:\n","    NUM_EPOCHS = 1\n","    LOAD_MODEL = True\n","    LOAD_EPOCH = PLOT_EPOCH\n","    BATCH_SIZE = 1\n","\n","\n","model = UNET(in_channels=IN_CHANNELS, out_channels=OUT_CHANNELS).to(DEVICE)  # change the out channels for multi-class classification\n","\n","if LOAD_MODEL:\n","    checkpoint = torch.load(f\"Checkpoints/checkpoint_{LOAD_EPOCH}.pth.tar\")\n","    load_checkpoint(checkpoint,model)\n","\n","# print(weights_tensor.shape)\n","# weights_tensor.to(DEVICE)\n","\n","loss_fn = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([100]).to(DEVICE))\n","# loss_fn = nn.BCEWithLogitsLoss()  # Binary cross entropy with sigmoid added in the end. Don't forget if not using loss function\n","\n","# optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=0.9)\n","optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n","\n","# Split the data into train and validation sets\n","train_df2, val_df2 = train_test_split(train_df, test_size=0.2, random_state=42)\n","\n","# Data transforms\n","transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Resize((224, 224)),\n","])\n","\n","Ttransform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Resize((224, 224),interpolation=InterpolationMode.NEAREST),\n","])\n","\n","# Create instances of ImageDataset for the train and validation datasets\n","train_dataset = ImageDataset(train_df2['img'].to_list(), train_df2['seg'].to_numpy(), transform=transform, target_transform=Ttransform)\n","val_dataset = ImageDataset(val_df2['img'].to_list(), val_df2['seg'].to_numpy(), transform=transform, target_transform=Ttransform)\n","\n","# Create DataLoaders for train_dataset and val_dataset\n","train_dataloader_ = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n","validation_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n","\n","scaler = torch.cuda.amp.grad_scaler.GradScaler()\n","model.debug = False\n","TPR, TNR, ACC = (0,0,0)\n","loss = np.Inf\n","\n","IoU = JaccardIndex(task='multiclass',num_classes=20).to(DEVICE)\n","\n","training_loss_matrix = []\n","validation_loss_matrix = []\n","training_iou_matrix = []\n","validation_iou_matrix = []\n","\n","training_loss = []\n","validation_loss = []\n","training_iou = []\n","validation_iou = []\n","\n","for epoch in range(NUM_EPOCHS):\n","\n","    if PLOT_MODE == False:\n","\n","        # loss = train_fn(train_dataloader,model,optimizer,loss_fn,scaler)\n","\n","        # TRAINING FUNCTION ##################\n","        loop = tqdm(train_dataloader_)\n","\n","        for batch_idx_, (data_, targets_) in enumerate(loop):\n","\n","            targets_ = torch.round(targets_*255) # target values were provided divided by 255\n","            arrays = [targets_ == c for c in range(20)] # create an array for each segmentation index (for each class)\n","            arrays = np.concatenate(arrays,axis=1)\n","            targets_ = torch.tensor(arrays, dtype=float)\n","            data_ = data_.float().to(DEVICE) # add the missing dimension 0 (number of images)\n","            targets_ = targets_.float().to(DEVICE)  # float is important for binary cross entropy \n","\n","            # forward\n","            with torch.cuda.amp.autocast():\n","                predictions_ = model(data_)\n","                loss = loss_fn(predictions_, targets_)\n","                pp = torch.sigmoid(predictions_)\n","                iou = IoU(pp,targets_)\n","                \n","            # backward\n","            optimizer.zero_grad() # Clears the gradient from previous run\n","            scaler.scale(loss).backward() # use the scaler to prevent gradient loss over several layers\n","            scaler.step(optimizer)\n","            scaler.update()\n","\n","            # update tqdm\n","            loop.set_postfix(loss=loss.item())\n","\n","            training_loss_matrix.append(loss.cpu().detach().numpy())\n","            training_iou_matrix.append(iou.cpu().detach().numpy())\n","\n","        # END TRAINING FUNCTION ##################\n","\n","        training_loss.append(np.mean(training_loss_matrix))\n","        training_loss_matrix = []\n","        training_iou.append(np.mean(training_iou_matrix))\n","        training_iou_matrix = []\n","\n","        # EVALUATE THE MODEL\n","        model.eval()\n","\n","        loop = tqdm(validation_dataloader)\n","\n","        for bb, (dd, tt) in enumerate(loop):\n","\n","            tt = torch.round(tt*255) # target values were provided divided by 255\n","            arrays = [tt == c for c in range(20)] # create an array for each segmentation index (for each class)\n","            arrays = np.concatenate(arrays,axis=1)\n","            tt = torch.tensor(arrays, dtype=float)\n","            dd = dd.float().to(DEVICE) # add the missing dimension 0 (number of images)\n","            tt = tt.float().to(DEVICE)  # float is important for binary cross entropy \n","\n","            # forward\n","            with torch.cuda.amp.autocast():\n","                pp = model(dd)\n","                loss = loss_fn(predictions_, targets_)\n","                pp = torch.sigmoid(pp)\n","                iou = IoU(pp,tt)\n","        \n","            validation_loss_matrix.append(loss.cpu().detach().numpy())\n","            validation_iou_matrix.append(iou.cpu().detach().numpy())\n","\n","        validation_loss.append(np.mean(validation_loss_matrix))\n","        validation_loss_matrix = []\n","        validation_iou.append(np.mean(validation_iou_matrix))\n","        validation_iou_matrix = []\n","\n","        model.train()\n","\n","        #save model\n","        if (epoch == NUM_EPOCHS-1 or (SAVE_FREQUENCY > 0 and epoch % SAVE_FREQUENCY == 0)):\n","            if os.path.exists(\"Checkpoints\") ==  False:\n","                os.mkdir(\"Checkpoints\")\n","            checkpoint = {\"state_dict\": model.state_dict(), \"optimizer\": optimizer.state_dict()}\n","            save_ep = epoch+LOAD_EPOCH\n","            save_checkpoint(checkpoint,filename=f\"Checkpoints\\\\checkpoint_{save_ep}.pth.tar\")\n","\n","if PLOT_MODE:\n","    epoch = PLOT_EPOCH\n","    # print examples\n","    save_predictions_as_images(validation_dataloader,model,\"Results\",DEVICE,epoch=epoch)\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# 3.3 Evaluation metrics\n","\n","As for the classification problem, we kept track of the loss in both the tranining and validation sets every epoch.\n","Precision and recall would offer some information about the performance of the model but, due to the ovewhelming number of background pixels, their usefulness would be limited. A better approach is to use the Intersection over Union, known in Pytorch as the Jaccard Index.\n","\n","In multilabel tasks, it is defined as the ratio of the intersection of predictions and grounth truths over their union. With a value between 0 and 1, this index get closer to 1 as similarity between predictions and ground truths grows, 1 being a perfect match. \n","One perk of this index is the ability to ignore one class completely.\n","\n","The following plots show the evolution of loss and the Intersection over Union index over the training epoch."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Plot the results\n","\n","# Create a 1x2 grid of subplots\n","fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15, 5))\n","\n","# Plot the training and validation loss\n","axes[0].plot(training_loss, label='Train Loss')\n","axes[0].plot(validation_loss, label='Validation Loss')\n","axes[0].set_xlabel('Epoch')\n","axes[0].set_ylabel('Loss')\n","axes[0].legend()\n","\n","# Plot the training and validation IoU scores\n","axes[1].plot(training_iou, label='Train IoU Score')\n","axes[1].plot(validation_iou, label='Validation IoU Score')\n","axes[1].set_xlabel('Epoch')\n","axes[1].set_ylabel('IoU Score')\n","axes[1].legend()\n","\n","# Adjust the spacing between subplots\n","plt.subplots_adjust(wspace=0.3)\n","\n","# Display the plots\n","# plt.show()"]},{"attachments":{"Training.png":{"image/png":"iVBORw0KGgoAAAANSUhEUgAABNEAAAHACAYAAACMHv7nAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy88F64QAAAACXBIWXMAAA9hAAAPYQGoP6dpAAEAAElEQVR4nOzdd3iT9foG8Ds73S0tXdCWvZEpCIigIghu9IDoEf0JKOICnICK4sCJHA+CCxTHUdzneOTIcKACKksEQWZpGS2lLd1t9u+Pd+TN6kyapL0/18XV9M2b5Ju2BHL3eZ6vyuFwOEBEREREREREREQ+qYO9ACIiIiIiIiIiolDHEI2IiIiIiIiIiKgODNGIiIiIiIiIiIjqwBCNiIiIiIiIiIioDgzRiIiIiIiIiIiI6sAQjYiIiIiIiIiIqA4M0YiIiIiIiIiIiOrAEI2IiIiIiIiIiKgO2mAvoLnZ7XacOnUKMTExUKlUwV4OERER+ZHD4UB5eTnS09OhVvN3heQ//D8kERFRy1Xf/0O2uhDt1KlTyMjICPYyiIiIKICOHz+O9u3bB3sZ1ILw/5BEREQtX13/h2x1IVpMTAwA4QsTGxsb5NUQERGRP5WVlSEjI0P+957IX/h/SCIioparvv+HbHUhmlR+Hxsby/8AERERtVBstyN/4/8hiYiIWr66/g/JYSFERERERERERER1YIhGRERERERERERUB4ZoREREREREREREdWh1M9GIiCjwHA4HrFYrbDZbsJdCLYxGo4FWq+XMMwpJfO2j1oKvxUTUWjFEIyIivzKbzcjLy0NVVVWwl0ItVGRkJNLS0qDX64O9FCIZX/uoteFrMRG1RgzRiIjIb+x2O7Kzs6HRaJCeng69Xs/fUpPfOBwOmM1mnDlzBtnZ2ejatSvUak6moODjax+1JnwtJqLWjCEaERH5jdlsht1uR0ZGBiIjI4O9HGqBIiIioNPpkJOTA7PZDKPRGOwlEfG1j1odvhYTUWvFXxkQEZHf8TfSFEj8+aJQxZ9Nak34805ErRFf+YiIiIiIiIiIiOrAEI2IiIiIiIiIiKgODNGIiIgCZPTo0Zg9e3awl0FE1Kz42kdERC0VQzQiImr1VCpVrX9uueWWRt3v559/jieffLJJa7vllltw9dVXN+k+iIi8aUmvfT/88ANUKhVKSko8ruvfvz8ef/xxn7etrKzEQw89hE6dOsFoNKJt27YYPXo0/vvf/zZ84URE1KJxd04iImr18vLy5Mtr1qzBY489hgMHDsjHIiIiXM63WCzQ6XR13m+bNm38t0giIj/ja59g5syZ+O2337Bs2TL06tULRUVF2LJlC4qKigL2mGazGXq9PmD3T0REgcFKND96+ut9GPvyJny1+1Swl0JEFDIcDgeqzNag/HE4HPVaY2pqqvwnLi4OKpVK/rympgbx8fH4+OOPMXr0aBiNRrz//vsoKirClClT0L59e0RGRqJv37748MMPXe7XvaWpQ4cOeOaZZ3DrrbciJiYGmZmZeOONN5r09d20aROGDBkCg8GAtLQ0PPzww7BarfL1n376Kfr27YuIiAgkJiZizJgxqKysBCBUbgwZMgRRUVGIj4/HiBEjkJOT06T1EJGAr32z5c/98dpnMplwzz33IDk5GUajEeeffz62bdvWoPvw5auvvsL8+fMxYcIEdOjQAYMGDcLdd9+Nm2++2eXxH3zwQWRkZMBgMKBr165YuXKlfH1dr8WjR4/GXXfdhblz5yIpKQmXXHIJAGDfvn2YMGECoqOjkZKSgptuugmFhYV+eV5ERM1lR85ZzPt8D/722ha8uO4Acouqgr2kgGElmh/lldbg4OkKFFaYgr0UIqKQUW2xoddj64Ly2PsWjUOk3j//1D300EN46aWX8Pbbb8NgMKCmpgaDBg3CQw89hNjYWHz99de46aab0KlTJwwdOtTn/bz00kt48sknMX/+fHz66ae44447cMEFF6BHjx4NXtPJkycxYcIE3HLLLXj33Xfx119/YcaMGTAajXj88ceRl5eHKVOm4Pnnn8c111yD8vJy/PTTT3A4HLBarbj66qsxY8YMfPjhhzCbzfjtt9+gUqma8mUiIhFf+1w19bXvwQcfxGeffYbVq1cjKysLzz//PMaNG4fDhw83ufItNTUVa9euxcSJExETE+P1nKlTp2Lr1q145ZVX0K9fP2RnZ8thV12vxZLVq1fjjjvuwObNm+FwOJCXl4dRo0ZhxowZWLJkCaqrq/HQQw9h0qRJ+O6775r0nIiImsvek6WY9PpW2OzCL3C2HTuLj7blYtuCMU3+f6Xd7sCnO04gxqjFJb1SoNUEvw6MIZofGXUaAECNxR7klRARkb/Nnj0bEydOdDl2//33y5fvvvtufPPNN/jkk09qfSM5YcIEzJo1C4Dw5vTll1/GDz/80KgQbfny5cjIyMCyZcugUqnQo0cPnDp1Cg899BAee+wx5OXlwWq1YuLEicjKygIA9O3bFwBQXFyM0tJSXH755ejcuTMAoGfPng1eAxG1bKHw2ldZWYkVK1bgnXfewfjx4wEAb775JjZs2ICVK1figQceaMxTk73xxhu48cYbkZiYiH79+uH888/HddddhxEjRgAADh48iI8//hgbNmzAmDFjAACdOnWSb1/Xa7FaLbzp69KlC55//nn5do899hgGDhyIZ555Rj62atUqZGRk4ODBg+jWrVuTnhcRkb/tzD2L06U1AIS5mgMy47F040HY7A4M7dgGV/Vvh/lf7EFhhRlnqyxoE9XwtnWrzY6lGw9BpQL255Vh4/4CAEBanBFTh3XAlCEZiI8MXjs8QzQ/MuqEfyBrLLYgr4SIKHRE6DTYt2hc0B7bXwYPHuzyuc1mw7PPPos1a9bg5MmTMJlMMJlMiIqKqvV+zjnnHPmy1DpVUFDQqDXt378fw4YNc/kt34gRI1BRUYETJ06gX79+uPjii9G3b1+MGzcOY8eOxXXXXYeEhAS0adMGt9xyC8aNG4dLLrkEY8aMwaRJk5CWltaotRCRK772uWrKa9+RI0dgsVjkUAsAdDodhgwZgv379zfg2Xh3wQUX4OjRo/jll1+wefNmfPfdd/jHP/6BJ554Ao8++ih+//13aDQajBo1yuvt63otzszMBOD5tdyxYwe+//57REdHe33ODNGIKJRsP1aM617b6nLMqFOjxmKHWgUsntgXndpGY8mGAyisMCO/tKZRIdp/dp/Csu8Py5/rtWrEGLTIK63Bc9/8hX98exAb5oxCRpvIJj+nxmCI5kdGrViJZmWIRkQkUalUfmsrCib3N4gvvfQSXn75ZSxduhR9+/ZFVFQUZs+eDbPZXOv9uA/lVqlUsNsbV8HscDg8yuSlWUgqlQoajQYbNmzAli1bsH79evzzn//EggUL8Ouvv6Jjx454++23cc899+Cbb77BmjVr8Mgjj2DDhg0477zzGrUeInLia5+rprz2KV/X3I9Lx2JjYwEApaWliI+PdzmvpKQEcXFxda5v5MiRGDlyJB5++GE89dRTWLRoER566CGPDRa8ra+212KJ+9fSbrfjiiuuwHPPPedxn/yFBhEFw6c7TuDj7cdxorgKFrsDHRIjseqWcxFj1OHrPcJmNOlxRrRLiMDZKgsOF1QAAK4e0A6d2gq/EEiOMaKwwozT5TXohdgGr2HNtuMAgF5psYg2aDH/sp7omRaDr3bnYdXP2dBp1WifUPvrciCF/7/sIURu5zQzRCMiaul++uknXHXVVfj73/8OQHgzdOjQoWZtiezVqxc+++wzlzdwW7ZsQUxMDNq1awdAeAM3YsQIjBgxAo899hiysrLwxRdfYO7cuQCAAQMGYMCAAZg3bx6GDRuGf/3rXwzRiMinYLz2denSBXq9Hj///DNuuOEGAMJOodu3b5c3MOjatSvUajW2bdsmt68Dwg6kJ0+eRPfu3Rv0mL169YLVakVNTQ369u0Lu92OTZs2ye2c7ufW9VrszcCBA/HZZ5+hQ4cO0Gr5toyIAqfGYsPt7+1AerwRiyee4/WczYcLcf8nu12OnSk34X978/G3Qe3xrdhW+dgVvXFpn1R5Xtmv2cV4aLzzNTY1zoh9eWVy22dDHCusxK/ZxVCpgLduHoz0eGdYdt2g9rh2YDuUVFmCOsM3+FPZWhBnOydnohERtXRdunSRq7z279+P22+/Hfn5+QF5rNLSUvz+++8uf3JzczFr1iwcP34cd999N/766y/8+9//xsKFCzF37lyo1Wr8+uuveOaZZ7B9+3bk5ubi888/x5kzZ9CzZ09kZ2dj3rx52Lp1K3JycrB+/XocPHiQc9GIqFbN+doniYqKwh133IEHHngA33zzDfbt24cZM2agqqoK06ZNAwDExMTg9ttvx3333Ycvv/wS2dnZ2Lx5M6ZMmYKePXti7NixPu9/9OjReP3117Fjxw4cO3YMa9euxfz583HhhRciNjYWHTp0wM0334xbb71Vvu8ffvgBH3/8MQDU+Vrsy5133oni4mJMmTIFv/32G44ePYr169fj1ltvhc3GX8oTkf98sv04Nh08gw9/O44dOWddrtt8uBD//PYQHhADtKv7p+PzWcPxfyM6AAC+2ZuPI2cqkFtcBb1GjZFdkwAAarUKk87NwEuT+iE5xijfX0qsAQCQX9bwEO2DX4Vd4i/o2tYlQJOoVCokNKJF1J/4Kw8/kivR2M5JRNTiPfroo8jOzsa4ceMQGRmJ2267DVdffTVKS0v9/lg//PADBgwY4HLs5ptvxjvvvIO1a9figQceQL9+/dCmTRtMmzYNjzzyCAChvenHH3/E0qVLUVZWhqysLLz00ksYP348Tp8+jb/++gurV69GUVER0tLScNddd+H222/3+/qJqOVoztc+pWeffRZ2ux033XQTysvLMXjwYKxbtw4JCQnyOS+//DLS0tIwf/58HDt2DMnJybjwwgvx0Ucf1VrpNW7cOKxevRrz589HVVUV0tPTcfnll+Oxxx6Tz1mxYgXmz5+PWbNmoaioCJmZmZg/fz4AoF27drW+FvuSnp6OzZs346GHHsK4ceNgMpmQlZWFSy+9tNbwjYioIaw2O17/8aj8+fLvDyMuUgeT1Y70OCPe/Clbvi4rMRLPTOyLSL0WMQYt3t58DD8dOoMvdgk7F5/XORFRhtpjpJRYIVA7XWaq9xodDgeWfXdYXsuUIRn1vm1zUzmkhv1WoqysDHFxcSgtLZVnJ/jLe7/k4NEv92Jc7xS8ftPgum9ARNTC1NTUIDs7Gx07doTRaKz7BkSNUNvPWSD/nafWrbafLb72UWvEn3ui8PDlrpOYveZ3xBi0KDdZvZ5zSa8UtInU4//O74Aeqc5/4y5+6QccOVMJvUYNs82ORVf1xtRhHWp9vI9+y8XDn+/BRT2SseqWc+u1xk93nJBbSW+/oBMeHt+j2Vs26/t/SFai+ZFRy3ZOIiIiIiIiImo8s9UOvbbhFalmqx3P/u8v9M+Mx5X90mG12fHKd4cAADNHd8aWI4XYfLgIiVF6XNgjGVsOF2Lm6M4+g7EJfdPwz+8Ow2yzo1NSFK7sl17nGqRKtPx6zkQrqjDhqa/3AQDuubgr5l4S2jsTM0TzI7md08J2TiIiIiIiIiKqW43Fhm3HiuFwAO9uzcH3Bwrw6g0DcWmf1Abdz5e7TmLV5mxgM3C6tAZxETocPVOJhEgdpg7Lwvg+qXj/l1zcPDwLWYlRdd7f38/LwrZjxeiTHoe5Y7vVa9dpKUQrKK9fiPbcN3+hpMqCnmmxuPuiLvW6TTAxRPMj50w0VqIRERERERERkXdr9+Thsx0ncO+Yrnhh3QH8dKjQ5frfsosbHKJ9tC1Xvvz02v3y5TtGd0aMUYcYow6PXdGr3veXEmvER7cNa9AaUuOEEK2wwlxnRV2NxYb/7D4FAFh0VW/oNKE/D5Ihmh9Ju3OaWIlGRERERERERAqvbzqCz3eexJX907F040FYbA58f6AAdgdg0KrRLj4CRwsrAQAlVeYG3feh0+XYmVsCjVqF2y7ohNVbjqHKbENanLHOOWb+lBCpk2eoFZTXoH1CpM9ztx4tQo3FjrQ4IwZnJfg8L5QwRPMjtnMSERERERERkbtduWfx7Dd/weEAXlh3AABchv0vmdQfl52ThjXbcvHQZ3twtoEh2kfbjgMALuqRjIcu7YF7L+6KHTln0SEpSs4qmoNKpUJyrAEnzlbjdJmp1hDt+78KAAAX9khu9o0EGoshmh8ZtVKIxnZOIiIiIiIiotbmZEk1XvjmL+SX1eCZa/qiU9toWGx2zPt8DxwOoFtKNA4XVKB7aiw+mD4Uq7ccQ6e2UbjsnDQAQEKkHgBwtspS78fMKarE+7/kAACmDMkAIBT5jOiS5OdnVz8psUYxRPM9F83hcOA7MUS7qHtycy2tyRii+ZHUzlljZSUaERERERERUWuydk8e5n78u1xYc83yLXj9pkHYkXMWf+WXo02UHh/dNgx2hwMxRi0MWg3muO1GmRAlhWj1q0RzOBx49N9/wmS1Y0SXRFwYAoFUaj126DxcUIETZ6uh16oxvEticy2tyUJ/alsYYTsnERERUd2WL1+Ojh07wmg0YtCgQfjpp59qPX/Tpk0YNGgQjEYjOnXqhNdee83jnKVLl6J79+6IiIhARkYG5syZg5oa53/eH3/8cahUKpc/qakNG9hMRETky+c7T+Cuf+1EjcWOIR3aoF9GPEqrLbh51W/4x7eHAACPXt4TbaL0SIo2wKD13mKZEKkDAJytrF+ItvVoEX48eAZ6rRpPXtUnJNoi28YYAABnKkw+z/kluxgAMLRjm3rt+hkqGKL5kTNEs8PhcAR5NURE1NxGjx6N2bNny5936NABS5curfU2KpUKX375ZZMf21/3QxRoa9aswezZs7FgwQLs2rULI0eOxPjx45Gbm+v1/OzsbEyYMAEjR47Erl27MH/+fNxzzz347LPP5HM++OADPPzww1i4cCH279+PlStXYs2aNZg3b57LffXu3Rt5eXnynz179gT0ubYWfO0jotauxmLDI1/uhd0BXH9uBj687Tysue08jOmZApPVDrPVjpFdk3B1/3Z13le82M5ZVmOF1Vb3qKitR4oAAJf3TUOnttFNeyJ+EmMUQrEqcd6bN4dOlwMAeqXHNsua/IUhmh9J7ZwAYLJyLhoRUbi44oorMGbMGK/Xbd26FSqVCjt37mzw/W7btg233XZbU5fn4vHHH0f//v09jufl5WH8+PF+fSx377zzDuLj4wP6GNTyLVmyBNOmTcP06dPRs2dPLF26FBkZGVixYoXX81977TVkZmZi6dKl6NmzJ6ZPn45bb70VL774onzO1q1bMWLECNxwww3o0KEDxo4diylTpmD79u0u96XVapGamir/adu2bUCfa6jja1/9NOa1z1e4N3v2bIwePbrW277++uvo168foqKiEB8fjwEDBuC5555r0OMTtTYWmx13frAT87/YA7u9/gUtdrsDH/yag6NnKpr0+JsPF6LKbEN6nBGLJ/aFRq2CUafBir8PxA1DM9E7PRbPXNO3XlVi8RE6+XJpdd1z0XbmngUADOoQOrtbSpVlFSbfXXoH8oUQrXtKTLOsyV8YovmRcscLEzcXICIKG9OmTcN3332HnJwcj+tWrVqF/v37Y+DAgQ2+37Zt2yIy0veORP6UmpoKg8HQLI9F1Fhmsxk7duzA2LFjXY6PHTsWW7Zs8XqbrVu3epw/btw4bN++HRaL8Obi/PPPx44dO/Dbb78BAI4ePYq1a9fisssuc7ndoUOHkJ6ejo4dO+L666/H0aNHfa7VZDKhrKzM5U9Lw9e+0LNy5UrMnTsX99xzD3bv3o3NmzfjwQcfREVF097g10b6e0QUzv79+yl8vScP//o1V26drI8vdp3Egi/2Ytrq7bA1IHxzt3G/MCD/4p4pLkGZTqPGM9f0xdf3jERGm/q9Lmo1asSKlVx1bS5gtdnxe24JAGBQVuiEaNEGIRupMnuvRHM4HDgoVqJ1Y4jWeuk0amjUwl8Ybi5ARBQ+Lr/8ciQnJ+Odd95xOV5VVYU1a9Zg2rRpKCoqwpQpU9C+fXtERkaib9+++PDDD2u9X/eWpkOHDuGCCy6A0WhEr169sGHDBo/bPPTQQ+jWrRsiIyPRqVMnPProo/IbnHfeeQdPPPEEdu/eLc90ktbsXvWwZ88eXHTRRYiIiEBiYiJuu+02lzdht9xyC66++mq8+OKLSEtLQ2JiIu68884mvZnKzc3FVVddhejoaMTGxmLSpEk4ffq0fP3u3btx4YUXIiYmBrGxsRg0aJBcKZSTk4MrrrgCCQkJiIqKQu/evbF27dpGr4VCU2FhIWw2G1JSUlyOp6SkID8/3+tt8vPzvZ5vtVpRWFgIALj++uvx5JNP4vzzz4dOp0Pnzp1x4YUX4uGHH5ZvM3ToULz77rtYt24d3nzzTeTn52P48OEoKiry+riLFy9GXFyc/CcjI6MpTz0k8bWv8a99K1asQOfOnaHX69G9e3e89957Dbq9L1999RUmTZqEadOmoUuXLujduzemTJmCJ5980uW8VatWoXfv3jAYDEhLS8Ndd90lX1fXa7FU1bdq1Sp06tQJBoMBDocDpaWluO2225CcnIzY2FhcdNFF2L17t1+eF1Eg2e0OvL7piPz5P749hB8PnvF5vtlqx3tbj+F4cRXW7skDAGQXVuJ/e/Ma9fjCLpPC37GLe/pnqH99Nxc4cLoclWYbYgxadE0OnTBKqkSrNHvPRQorzDhbZYFKBXRJDo0W1PoKn+ltYcKoVaPSbEO1jx8WIqJWx+EALFXBeWxdJFCPsnmtVoupU6finXfewWOPPSb/BvGTTz6B2WzGjTfeiKqqKgwaNAgPPfQQYmNj8fXXX+Omm25Cp06dMHTo0Dofw263Y+LEiUhKSsIvv/yCsrIylxlCkpiYGLzzzjtIT0/Hnj17MGPGDMTExODBBx/E5MmTsXfvXnzzzTfYuHEjACAuLs7jPqqqqnDppZfivPPOw7Zt21BQUIDp06fjrrvucnmz/P333yMtLQ3ff/89Dh8+jMmTJ6N///6YMWNGnc/HncPhwNVXX42oqChs2rQJVqsVs2bNwuTJk/HDDz8AAG688UYMGDAAK1asgEajwe+//w6dTmhZuPPOO2E2m/Hjjz8iKioK+/btQ3R0eP2niurPvZ3F4XDU2uLi7Xzl8R9++AFPP/00li9fjqFDh+Lw4cO49957kZaWhkcffRQAXFr++vbti2HDhqFz585YvXo15s6d6/GY8+bNczleVlbWsCCNr30AWuZr3xdffIF7770XS5cuxZgxY/Df//4X//d//4f27dvjwgsvrNd9+JKamopNmzYhJycHWVlZXs9ZsWIF5s6di2effRbjx49HaWkpNm/eDKB+r8UAcPjwYXz88cf47LPPoNEIFSOXXXYZ2rRpg7Vr1yIuLg6vv/46Lr74Yhw8eBBt2rRp0vMiCqRv/yrAoYIKxBi0GNs7FZ/tPIF5n+/BhrkXyGHOxn2n8fDnezB7TFcUlNXgle8Oo2tyDnKKnK/Tr35/BJf1Tav3YH6LzY7rXtuKo2cqUF5jRaReg/M6+WeXyfhIPXKKqurcXGCnWIXWPzNeLugJBVFSJZqPmWjSPLSsNpEuHX3hgCGanxl1GlSabaxEIyKSWKqAZ9KD89jzTwH6qHqdeuutt+KFF17ADz/8IL8JWrVqFSZOnIiEhAQkJCTg/vvvl8+/++678c033+CTTz6p1xvJjRs3Yv/+/Th27Bjat28PAHjmmWc8Zvk88sgj8uUOHTrgvvvuw5o1a/Dggw8iIiIC0dHR8lwnXz744ANUV1fj3XffRVSU8PyXLVuGK664As8995xc1ZOQkIBly5ZBo9GgR48euOyyy/Dtt982KkTbuHEj/vjjD2RnZ8tBw3vvvYfevXtj27ZtOPfcc5Gbm4sHHngAPXr0AAB07dpVvn1ubi6uvfZa9O3bFwDQqVOnBq+BQl9SUhI0Go1H1VlBQYFHtZkkNTXV6/larRaJicKblUcffRQ33XQTpk+fDkAIySorK3HbbbdhwYIFUKs9my+ioqLQt29fHDrkve3HYDA0rU2Qr30AWuZr34svvohbbrkFs2bNAgDMnTsXv/zyC1588cUmh2gLFy7ExIkT0aFDB3Tr1g3Dhg3DhAkTcN1118k/x0899RTuu+8+3HvvvfLtzj33XAD1ey0GhNbq9957T54L+N1332HPnj0oKCiQf+5ffPFFfPnll/j000/9PuOOyF9sdgdeWn8AAHDjeVm45+Iu+OVoEU6WVOOFdQfw2OW9sC+vDHd/uAvVFhue+nofNGJIdqhAqFJtFx+Bs1Vm7M8rw87cknq3RX61+xR2Hy+RPx/ZNclvgZC0Q2dJHe2cO3OEeWgDM0OnlROouxLtQJi2cgJs5/Q75Q6dREQUPnr06IHhw4dj1apVAIAjR47gp59+wq233goAsNlsePrpp3HOOecgMTER0dHRWL9+vc8dBd3t378fmZmZ8ptIABg2bJjHeZ9++inOP/98pKamIjo6Go8++mi9H0P5WNJQasmIESNgt9tx4MAB+Vjv3r3lCgQASEtLQ0FBQYMeS/mYGRkZLpU6vXr1Qnx8PPbv3w9AeKM5ffp0jBkzBs8++yyOHHG2Xtxzzz146qmnMGLECCxcuBB//PFHo9ZBoU2v12PQoEEe7XwbNmzA8OHDvd5m2LBhHuevX78egwcPlisZq6qqPIIyjUYDh8Phc8d0k8mE/fv3Iy0trbFPp0Xga1/DX/v279+PESNGuBwbMWKE/FrXFGlpadi6dSv27NmDe+65BxaLBTfffDMuvfRS2O12FBQU4NSpU7j44ot9rq2u12IAyMrKctlYY8eOHaioqJC/x9Kf7Oxsl9dqolCxessxjHrhe8z9+Hf8lV+OuAgdbr+gEyL1Wiy6qjcA4O3Nx3Dhiz/gmuVbUG2xQatWocZiR6XZBoPW+W/Glf3T0T1VCHMKK0z1eny73YHXxBbSiQPaYdr5HbFgQi+/Pb+EyPq1c+4SNxUYGELz0ABFJZqPmWgHTwsBZjiGaKxE8zODuENnjYWVaEREAIS2ovmngvfYDTBt2jTcddddePXVV/H2228jKytLfqPy0ksv4eWXX8bSpUvRt29fREVFYfbs2TCba//PjcTbG3n3doFffvkF119/PZ544gmMGzcOcXFx+Oijj/DSSy816HnU1hrnMuxWp/O4zm5v3C+BfD2m8vjjjz+OG264AV9//TX+97//YeHChfjoo49wzTXXYPr06Rg3bhy+/vprrF+/HosXL8ZLL72Eu+++u1HrodA1d+5c3HTTTRg8eDCGDRuGN954A7m5uZg5cyYAoY3y5MmTePfddwEAM2fOxLJlyzB37lzMmDEDW7duxcqVK13mcl1xxRVYsmQJBgwYILdzPvroo7jyyivlsOT+++/HFVdcgczMTBQUFOCpp55CWVkZbr755sA8Ub72AWi5r311tSTHxMSgtLTU43YlJSVeW1Hd9enTB3369MGdd96Jn3/+GSNHjsSmTZswePDgWm9Xn9diAC5BIyC03aalpbm0fEq4KzOFmrzSajyzdj9MVrvcjnnPxV3lOWIX90zB/Ak9sHTjIRwTr++fEY+FV/TC5Nd/gdlmx8uT++OFdQeQU1SJK85Jl8Mos7V+rwU/HCzAwdMViDZosfDK3oiL0NV9owZwhmi+K9HKaizy8zunXd2vK81JrkTzsTunvKlAKkO0Vs+olSrRGKIREQEQ5vLUs60o2CZNmoR7770X//rXv7B69WrMmDFDftPx008/4aqrrsLf//53AMIbjkOHDqFnz571uu9evXohNzcXp06dQnq60OK1detWl3M2b96MrKwsLFiwQD7mvmueXq+HzVb7vzG9evXC6tWrUVlZKb9R2rx5M9RqNbp161av9TaU9PyOHz8uV0Ds27cPpaWlLl+jbt26oVu3bpgzZw6mTJmCt99+G9dccw0AICMjAzNnzsTMmTMxb948vPnmmwzRWqDJkyejqKgIixYtQl5eHvr06YO1a9fK85/y8vJcKpA6duyItWvXYs6cOXj11VeRnp6OV155Bddee618ziOPPAKVSoVHHnkEJ0+eRNu2bXHFFVfg6aefls85ceIEpkyZgsLCQrRt2xbnnXcefvnlF59zp5qMr30AWuZrX8+ePfHzzz9j6tSp8rEtW7a4fE169OiBbdu2uYS0DocDO3bs8GhlrUuvXkJ1S2VlJWJiYtChQwd8++23XltH6/ta7G7gwIHIz8+HVqtFhw4dGrQ+oubw/YEC9EqLRUqsEf/YeAgmqx3t4iNwuqwG3VJicNN5rq/lt13QGRMHtsfmw4XonhqD7ikxUKlUWHnLYOSV1GB8n1QM6dgGZ8pN6JkWC4P4Pt5UzxBN2o3zukHt/R6gAc52ztpmou07Jewa3S4+Qg4QQ0WUHKJ5r0TLLqwEAHRpG37zbxmi+ZlRrkRjOycRUbiJjo7G5MmTMX/+fJSWluKWW26Rr+vSpQs+++wzbNmyBQkJCViyZAny8/Pr/UZyzJgx6N69O6ZOnYqXXnoJZWVlLm8YpcfIzc3FRx99hHPPPRdff/01vvjiC5dzOnTogOzsbPz+++9o3749YmJiPOY23XjjjVi4cCFuvvlmPP744zhz5gzuvvtu3HTTTT7nTtWXzWbD77//7nJMr9djzJgxOOecc3DjjTdi6dKl8jDrUaNGYfDgwaiursYDDzyA6667Dh07dsSJEyewbds2OQiZPXs2xo8fj27duuHs2bP47rvv6v21pfAza9YseZ6UO/edIgFg1KhR2Llzp8/702q1WLhwIRYuXOjznI8++qjB62wt+NrXMA888AAmTZqEgQMH4uKLL8ZXX32Fzz//XN70ABAqH2+++Wb06NEDY8eORXV1Nd544w0cOXIEd955p8/7vuOOO5Ceno6LLroI7du3R15eHp566im0bdtWboN9/PHHMXPmTCQnJ2P8+PEoLy/H5s2bcffdd9f5WuzLmDFjMGzYMFx99dV47rnn0L17d5w6dQpr167F1VdfXWcFHFEgfbv/NKat3o7ObaPwypQB+Hj7cQDAK1P6o0tyDPQaNfRaz0lVSdEGXNW/ncuxkV3bulyfFC28jki3N9VztvnRM0I74jntA1MBFl+P3Tn3nhSqXfu0iw3IGppCauestthgsztcNj2osdhQLIaD6fHGoKyvKTgTzc+kmWj1/ctHREShZdq0aTh79izGjBmDzMxM+fijjz6KgQMHYty4cRg9ejRSU1Nx9dVX1/t+1Wo1vvjiC5hMJgwZMgTTp093qZIBgKuuugpz5szBXXfdhf79+2PLli3yzoKSa6+9FpdeeikuvPBCtG3b1qWlTRIZGYl169ahuLgY5557Lq677jpcfPHFWLZsWcO+GF5UVFRgwIABLn8mTJgAlUqFL7/8EgkJCbjgggswZswYdOrUCWvWrAEgzKcqKirC1KlT0a1bN0yaNAnjx4/HE088AUAI5+6880707NkTl156Kbp3747ly5c3eb1EVD987au/q6++Gv/4xz/wwgsvoHfv3nj99dfx9ttvY/To0fI5kyZNwjvvvIPVq1fj3HPPxdixY+V5c7VVP44ZMwa//PIL/va3v6Fbt2649tprYTQa8e2338obadx8881YunQpli9fjt69e+Pyyy+XN8io67XYF5VKhbVr1+KCCy7Arbfeim7duuH666/HsWPH/BpAEjXGZztPAACOnKnElDd+gd0BjOudgkFZbRAXoUOEvunD/KUZafVt5zx6Rqik6hSgSqr6bCzwp1iJ1ic9tFo5ASDK4KzXqnbr0isoE+bOGbTqgFTxBZrK4WvaajNZvnw5XnjhBeTl5aF3795YunQpRo4c6fP8Dz74AM8//zwOHTqEuLg4XHrppXjxxRflf1TqUlZWhri4OJSWliI21v+J7a3vbMN3fxXguWv7YvK5mXXfgIioBampqUF2djY6duwIozH8frNE4aG2n7NA/ztPrVdtP1t87aPWiD/31BzKaywY/NRGlzbLKL0GG+8bhbS4CL89ztyPf8fnO0/i4fE9MHNU5zrX1Pfx9QCAPx4fi1ij/4OgzYcLceNbv6JrcjQ2zB3l9ZwxSzbhcEEF3r7lXFzYI9nva2gKh8OBzvPXwu4Afpt/MZJjna8Rv2UXY9LrW9EhMRI/PNC0HY39qb7/hwxqJdqaNWswe/ZsLFiwALt27cLIkSMxfvx4nzvxSLMHpk2bhj///BOffPIJtm3bJm9nHnTVJehqO4xElLKdk4iIiIiIiKgJNu4/DZPVjo5JUeiSLFR9PTCuu18DNADyTLT6VKJJ87ySog0BCdCAujcWqDJbcURsKe0dgu2cKpXKORfN7FqJll9WAwBIiQ3P8D2oIdqSJUswbdo0TJ8+HT179sTSpUuRkZGBFStWeD3/l19+QYcOHXDPPfegY8eOOP/883H77bdj+/btzbxyHz65BfOOz8RFml3cWICIiIiIiIioCb7anQcAuLJfOt6bNgRvTh2Mm4d38PvjGBowE83Zyhm4zWMSoqR2TrPHLsf/2HgIl73yMxwOoG2MAckxoRlGRYpz0dw3F8gvrQYApMaF5rrrErQQzWw2Y8eOHRg7dqzL8bFjx2LLli1ebzN8+HCcOHECa9euhcPhwOnTp/Hpp5/isssua44l1y2hAwAgQ1XASjQiIiIiIiJq1bYfK8bE5Zvx6Jd7G3zbGosNmw8XAgDG901FWlwELumVIu8e7E9yiFaP9/HSpgKdAxiiSbPCrHaHSyWXw+HAik2H5Wq4wVkJAVtDU0mVaFXulWilwky01DCtRAva7pyFhYWw2WwegypTUlKQn5/v9TbDhw/HBx98gMmTJ6OmpgZWqxVXXnkl/vnPf/p8HJPJBJPJJH9eVlbmnyfgjRiiZaoKcIAbCxAREREREVErtXrLMSz8z58AgF3HS3Df2G6IF9sU62NnzlmYrHa0jTGge0pMoJYJQLGxgK3uEO2IWInWOUCbCgjrcW6WYLHaAXEz4oJyE2osdmjUKiy+pi9Gd2/r4x6Cz1cl2mm2czaNe4rscDh8Jsv79u3DPffcg8ceeww7duzAN998g+zsbMycOdPn/S9evBhxcXHyn4yMDL+u30WbjgCEEI3tnERERERERNQanS6rwbP/+wsAoNeq4XAAvxwtbtB9bD4iVKGd3yUpINVnSgadEPjUpxJNmkUWyHZOjVoFjVp4zspgL6eoCgDQLj4Ck87NcBnYH2qcM9Hc2jnFEC2N7ZwNk5SUBI1G41F1VlBQ4HMb5cWLF2PEiBF44IEHcM4552DcuHFYvnw5Vq1ahby8PK+3mTdvHkpLS+U/x48f9/tzkbGdk4gIADxmNxD5E3++KFTxZ5NaE/68ky9Wmx3Pf3MA1RYbBmUlYNLg9gCAX44WNeh+fj4snD+iS5Lf1+hOr6nfTDSLzY5jReJMtKTAVaIBgE4jhmhWZYgmPHZWYmRAH9sfogxiO6fJvZ1TrERjiNYwer0egwYNwoYNG1yOb9iwAcOHD/d6m6qqKqjVrkvWaITE2NeLuMFgQGxsrMufgBFDtLaqMjhM5YF7HCKiEKXTCfMbqqqqgrwSasmkny/p540o2PjaR60RX4tbJ7vdgf/tyUNhhcnr9W/9dBS9Fq7DZztPAAAWXNYTwzsLIdiPh87g9ve2Y+G/656PVlptwZ4TJQCAEV0S/bP4Whh0dbdz2u0O3P/JbtRY7IiP1KF9gn93CHUnBXvKNeUWC3/vMtqEfogWqRfbORWVaHa7Q27n5Ey0Rpg7dy5uuukmDB48GMOGDcMbb7yB3NxcuT1z3rx5OHnyJN59910AwBVXXIEZM2ZgxYoVGDduHPLy8jB79mwMGTIE6enpwXwqAmMcTLo4GCyliKk+EezVEBE1O41Gg/j4eBQUFAAAIiMjA15+T62Hw+FAVVUVCgoKEB8fL/8ijSjY+NpHrQlfi1u3z3aewAOf/oFrB7bHS5P6uVxntzvw+o9HYbbaEaHTYOaozhiYmYDiSjMAYVdLaWfLRy/vBa3Gd03PlsOFsDuE4f1pcYENqwBFJZqPjjKHw4HH/rMX//79FLRqFV6e1L/W9ftlTVoNAKtbJZoQomWFQYjmbWOBokozrHYHVCphZ9FwFNQQbfLkySgqKsKiRYuQl5eHPn36YO3atcjKygIA5OXlITc3Vz7/lltuQXl5OZYtW4b77rsP8fHxuOiii/Dcc88F6yl4qIxsD0NpKWKqTwZ7KUREQZGamgoA8ptJIn+Lj4+Xf86IQgVf+6i14Wtx6/SzuFvmgdOeG/btyyvDmXITIvUa7Hz0EhjFOWNtovTokRqDv/Kd3VrVFhtiagmhvtglvJ8e09P7qCd/q6sSbcmGg3j/l1yoVMCSyf1xYY/kgK9J762dU6xEC4d2Tm8bC0hVaEnRBugCHEIGSlBDNACYNWsWZs2a5fW6d955x+PY3XffjbvvvjvAq2q86ugMoPRPJJhOBXspRERBoVKpkJaWhuTkZFgslmAvh1oYnU7HqgcKSXzto9aEr8Wt1/ZjZwEAJ89We1z3/V/CLxFGdEmSAzTJhL5pLiFaldmGGKP3VuCiChO+E+/r2kHt/bLuuki7YXqrRDtxtgr//O4wAOCpq/vgyn7N0wWnF3cMtSiCveNiiJbZJnCbGviLt0q0vNLw3lQACIEQraUxxQhVdIkWhmhE1LppNBr+B5uIWh2+9hFRS5VXWo2TJUJ4drbKgkqTFVEGLZZuPIgN+06jQqw4ushLldZdF3bBpMEZGLNkEypMVpdgxd1/dp+C1e5A33Zx6JYSE5gn46a2jQX25wnhX8+0WNw4NKtZ1gM4QzSpEq28xiK3xmaGUSVahZdKtOQYhmgkssRmAgDaWrzvFkpEREREREQUbqQqNMnJkmqUVFmwdOMhl+Oju7f1uK1arUJqnBEReo0Yolk9zpF8KbZyXjuwnR9WXT9SO6fJ6lmJdqhACNG6pQR2N053OreNBaR5aIlRekQbQj/Kkdao/F5LgVpcRPhuSBKeTaghzB4nJNMptvwgr4SIiIiIiIjIP3bkuIZoxwor8ciXewAIM64A4Jz2cbVuBCDt2FjtoxLNZndgX54wb+3iZpqHBjjbOc1eQrTDpysAAF2TmzdEc69Ek3bmDIcqNACIFNs5K03O77VUgSj9HISj0I8vw028UImW7DgT5IUQERERERER+cf2nGIAQuuj2WbHO1uO4eDpCrSJ0mP9nAuwM+cseqTV3n4ZIc5K89XOmVdaDYvNAZ1GhfT4wO/KKZECK++VaEKI1iW5eVpLJe6VaKfEVtr2CeERokXppe+1sxKtWrwcEcYhGivR/EwbnQgAMMIMWE1BXg0RERERERFR05itdvwlzga7oJvQrrnlSBEA4Mp+6WgTpceYXil1BjyR+tpDtFyxZTEjIRIatcova68Pg48QzW534LAYonVt5nZOg9vGAmU1QgAVHyatkJEGz0q0aotwOULHEI1Ehqh42B3iX/bqkqCuhYiIiIiIiKipcooqYbU7EG3QYkjHBJfrzu3Qpt73I7X4VVu8z0TLCVLLorMSzTXcO1lSjWqLDTqNClltmnlNGs+NBQAgxhgeDYXeKtFaQjsnQzQ/i4nQoxxC2amlsjjIqyEiIiIiIiJqmoOnpZbGaI9qs8EdErzdxKuIOirRpOH5zR1YGdzmj0mkKrSOSVHQapo3PtG5hWgVYiVadJiEaPJMNMX3WpqFx3ZOksUYtSh1RAEAqsuKgrwaIiIiIiIioqaRdqjsmhyNdopZZZltIpESa6z3/dS1sUBucaVwv4lRjV1qo0gbC5isdjgcDvm483k37zw0QLGxgE1YT7kYosWEwc6cABBlEL6mlSbFTLQW0M4ZHl/9MKLVqFGuigZwBtXlxYgN9oKIiIiIiIiImuCQWInWLSUG7RKcIVpDqtAA3zPRXlx3AMeKKuXKr2ZvndQ664vMNrscqh1SVOA1N/fdOSvEMCrGGB4z0aLEsK/KbIPd7oBarVK0c4ZvFBW+Kw9hVeoYwAGYy9nOSURERERERKHhVEk12kTpYWxgJZBUkdUlJRqJUXoYdWrUWOwNmocGABE6Z7AicTgcWPb9YZfzspp5JppBGaJZnSHa4TPB2VQA8GznLBdDtOhwqURTBGVVFhuiDVrUSJVo+vBtigzflYewao3wF4wz0YiIiIiIiCgUHCusxPnPfYeZ7+9o0O0sNjuyC4U2y67J0VCpVBjeOQkxBq28U2d9Ods5nS1+7jtiqlRARpBmoinX43A4cFisRAtGO6f77pzSxgLhMhNN+TWVwjMpPJXC1HAUvisPYSZdLGAFbJVng70UIiIiIiIiIuzLK4PdAezKLWnQ7XKKKmGxORCl18jz0Fb8fSBqzHbERTastdDbxgIVJtedOlNjjQ2ulGsqlUoFvUYNs80uV36dLjOh3GSFRq1Ch6TmDfUA5Uw0140FwmV3TrVaJVcsSjPwuLEAeWXRCZPQ7NUlwV0IEREREREREYDTZTUAgNJqC0qrLfW+3YF851wwlUoFQBjE39AADVDMRLM4Q7RKtxAts5mr0CRS5ZRUiSa1sGYlRsrtnc1JpxG+1h4z0QzhMRMNgByGSpVo0sYCkQzRSMmmjxMu1JQGdyFEREREREREAArKTfLl48VV9brNvlNlWPTfPwEAvdvFNXkN3nbnVFaiqVTAyK5JTX6cxtDLIZqwtkNyK2fzz0MDAL1G+FqZbXZYbXa5ei9c2jkB5y6cNRYhCKwS23i5Oye5sBuFFxeNiSEaERERERERBV9BmWuI1qeOUMzhcGDGu9txusyEbinRuOeirk1eQ4Re2ljAGZxJbYqdkqLw+azhiA3S7pMGt90wDxUEbx4aAOi0zkq0SpMzdAyXjQUAZ1hWbRF26JTCtHBu5wyfr344McYDALRmhmhEREREREQUfAXlNfLl42edlWg2uwMatcrj/JIqC06WVAMA1tw2DAlR+iavIcrLTLRKMVCLNmoRH9n0x2gsgxj4SO2ch8V2zmDszAkAeo1zY4EycVMBg1YtV8yFA4MiRKuxOr/nbOckF+rIBACA3lIW5JUQERERERERuVai5RZXweFwYNFX+9DvifXYmeu5Kd6ZCuH8+EidXwI0wNfGAsLlKH1wa3yk0MpstcPhcOCg2M7ZuW1wQjRlZZw8Dy2MWjkBIEInPIcai83le24Mwow5f2GIFgDayHgAgNFaHtyFEBEREREREQE4raxEK67Gs//7C6s2Z6PCZMW27GKP86XQrW20wW9riBSDMpeZaGI7Z1SQ2xQNOudMtMIKM0qrLVCpghei6TTeQrTw2VQAcIamNRab/D036tRQe6l8DBfhFWOGCX1MGwBAhJ0hGhEREREREQWXyWpDSZVzR85fs4uw6eAZ+XObw+FxmzMVQuiWHOvPEE2qRHPORKsMkSoreXdOi11ud02LNQZtfpfUtmm22VEutnOG0zw0wFlxVm22KXbmDK/n4C68Vx+ijFKI5qgGbFZAwy8zERERERERNS+73YF3tx5DZmKky3FpwLtEGqavJFWiJccY/bYeadB8lZfdOaMMwW3xU4ZWpWLg2CY6eDPa9Ip2znKxWi/sQjRFJZr0PQ/nnTkBhmgBYYxJdH5SUwpEJfo+mYiIiIiIiCgANu4/jce/2gej2KqYFmdEQbkJNrtQedanXSz2niyDxeYZop0pF9s5Y/xfiWay2uUNDSpNIdLOKVZNmSx2lDjMAIC4iOC1T+oUGwtIQWN02M1EkzYWsMvtnOG8MyfAmWgBERdlRLkjQvikpiSoayEiIiIKNcuXL0fHjh1hNBoxaNAg/PTTT7Wev2nTJgwaNAhGoxGdOnXCa6+95nHO0qVL0b17d0RERCAjIwNz5sxBTU2NyzkNfVwionCTW1SFXYpNAvblCZvdSZVnaXFGpMcLlWWd2kZheOckAD4q0cqlSjT/z0QDILf3yfO+ghyiSRsLmKw2uRItPiIEKtFszkq0YLe8NpRRsbFAtUV4DuG8MyfAEC0gYow6lEEol3VUe+5yQkRERNRarVmzBrNnz8aCBQuwa9cujBw5EuPHj0dubq7X87OzszFhwgSMHDkSu3btwvz583HPPffgs88+k8/54IMP8PDDD2PhwoXYv38/Vq5ciTVr1mDevHmNflwionDjcDhw06pfce2KLdh3SgjPDhVUuJyTEmtEF3FQ/vXnZsjBkcXmZSZaACrRjDo1VOJMeWkuWkWoVKLJGwvYUVIthGixQaxEMyg3FqgJjaCxoaRKNGU7pzHM2zkZogVArFGHMkcUAKCm3HOXEyIiIqLWasmSJZg2bRqmT5+Onj17YunSpcjIyMCKFSu8nv/aa68hMzMTS5cuRc+ePTF9+nTceuutePHFF+Vztm7dihEjRuCGG25Ahw4dMHbsWEyZMgXbt29v9OMSEYWbE2erkVNUBbsD+GTHcQDA4dOuIVpyjAELLuuFRy7rif8b0VFuGTR5rUQTqnn9GaKpVCpni58YqoROO6ciRJMq0SKD2M6pdQac4d/O6dydk5Vo5MGoU6MMUohWFOTVEBEREYUGs9mMHTt2YOzYsS7Hx44diy1btni9zdatWz3OHzduHLZv3w6LRXiTc/7552PHjh347bffAABHjx7F2rVrcdlllzX6cU0mE8rKylz+EBGFsh05zi6of/9+CjUWG7ILK13OSY41oktyNKaP7ASdRu0yvN7dmQC0cwLKHTpd2zmDPTRf+bUoq5baOYMXoukVlWhl4u6cMcbgrafeHA7hDwCDohLNuTtnE0K0/D2Apabu8wKIIVoAqFQqVKpjAACmcrZzEhEREQFAYWEhbDYbUlJSXI6npKQgPz/f623y8/O9nm+1WlFYWAgAuP766/Hkk0/i/PPPh06nQ+fOnXHhhRfi4YcfbvTjLl68GHFxcfKfjIyMRj1nIqLmogzRiivNWL3lGMw2O4w6Z1jmHojpNEJvpfvGAjUWG8rEFsK2ftydE3AOlneGaMLHYIdo8sYCinbOUNhYwKRo5/Tr1+jsMeC/c4ET2+s8td6sJuC1kcC7VwFw3Vigye2cNaXAa+cDi9sB1SX+WG2jMEQLkBqtEKJZK9nOSURERKSkkgbiiBwOh8exus5XHv/hhx/w9NNPY/ny5di5cyc+//xz/Pe//8WTTz7Z6MedN28eSktL5T/Hjx+v35MjIgoSKUTrlCR0Rb36/WEAQJfkaIzvkwqNWoUBmfEutzH4qESTqtD0WjVi/dxCGKkT7i902zltKKkSducMZjunXuvcnTMgGwv85x5g+0rg7fHA7//yvL66BCg64nrM4Tk7z8W+fwOn9wDZmwCHQw5Mq81+aOfM3yt8jEkDIuIbdx9+EF4NtWHErokArIDFVBXspRARERGFhKSkJGg0Go/qr4KCAo8qMUlqaqrX87VaLRITEwEAjz76KG666SZMnz4dANC3b19UVlbitttuw4IFCxr1uAaDAQaDf1uYiIgCpcJkxV/5Qtv5MxP74oY3f5Erybq0jcaz156DRy7r5THfTBnUKCl35qztlxyN4axEE9ZXGYLtnKVyJVrwdudUBpzyDqb+CtFytghBFwDYzMCXdwgVaZVnhGqyMQuBf10PlJ8CZv0KJHUBCg8Bqy4F2nQEJrwApA/wvN/D3zovO+zy7pwmq7OdM6KxlWh5u4WPaf0ad3s/YSVagKh0wouT1VQd5JUQERERhQa9Xo9BgwZhw4YNLsc3bNiA4cOHe73NsGHDPM5fv349Bg8eDJ1OqBCoqqqCWu3631qNRgOHwwGHw9GoxyUiChVWmx2HTpfLVbje7D5eArsDaBcfgfM6JeLKfunydV1TYmDUabxuECC1DJpt7pVowtwpf89DA5yVSFKoUh4iQ/OV7Zyl/mjnLDwMvNgd+GlJo26u0zgDTufcuEaux+EADnwDnNwhXP5hsXB84M3ABQ8Kl7evBPb/Bzi0DlgxAijNBexW4OD/hOt/eBaoKgRObAPevAj45TXPxznynfOy3eayiYRUiRahb+T3mSFaC6eLAADYzKxEIyIiIpLMnTsXb731FlatWoX9+/djzpw5yM3NxcyZMwEIbZRTp06Vz585cyZycnIwd+5c7N+/H6tWrcLKlStx//33y+dcccUVWLFiBT766CNkZ2djw4YNePTRR3HllVdCo9HU63GJiELVC+sO4JKXf8TaPd5nOALA7hMlACC3a951URdIBWRdkqN93k6v9b47p9TO6c+dOSXKjQUsNrvcShrd2HDFT5S7c0ohWpPaOf9YA1TkA9veqrsN0gvpe2O1O+T11Ktaz/2xzFXA5zOADycLlWTfPQVk/wiodcAF9wMXLQCuWwUkdQMGTgXiMgEo7uPId0Ig+OfnwuddLgEcduCbh4DvFzvPK84GKgsU67DJ88+qLTZ5JlqD2zkL/gIqzoRMiMZ2zgDR6IThizZzcHeOICIiIgolkydPRlFRERYtWoS8vDz06dMHa9euRVZWFgAgLy8Pubm58vkdO3bE2rVrMWfOHLz66qtIT0/HK6+8gmuvvVY+55FHHoFKpcIjjzyCkydPom3btrjiiivw9NNP1/txiYhCkcVmx5rtwkzGzUcKcdk5aV7PO3y6AgDQI1WYzd0lOQb3XNQVW48WYUSXJJ/3r9d4b+c8WSJVovl3UwHAWYlUZbbJrZwAEGVowq6NfiCFViVVZlhsQojUpBAtZ7PwsewkUHwUSOzcqPUAwkYRQD3aOcvzgTdGA5nDgIlvAhqtEJrt+US43mYGfnpRuHzB/UB8pnC5z7XCHwAoPQn8shxI7gX8e5bQ+vndk0Jw1u1SYMpHwOalwMbHgZ+XAOdOA358Ech12+3a7gzRhN05he91ne2clUXAp/8HaI2AWgsc+FpYZ+kJ4XqGaC2T1iBUotmDvP0qERERUaiZNWsWZs2a5fW6d955x+PYqFGjsHPnTp/3p9VqsXDhQixcuLDRj0tEFIp+PlyIkiqhCulgfrnP846cEUK0zm2dVWdzLumGOXXcv87HxgI/HToDAOjbPq6hS65TlFSJZrLKA/ONOjW0mgA1yn0zH6guBq5cJoRKPkiVaAVlQhWeTqNq/PwuS43rrpcH1wHFR4BOFwLdxwM7VwMpfYCMIZ63LT0J5G6FrsulHlfF1RXq/fU1UJ4nVI1FtQUmPA8c/UG4buzTwJZ/CtVxqX2Bkfd5v4+4dsC4p4WKtu+eFO5v35fCdRc8CKhUwPlzgD+/EKrDPpwCnPSyw6fDLn/9aix2RTun29f0+Dbh9udOBzQ64PunnfPaJCXiL9eiU4CY1Nq/BgHGEC1AtIZI4YKFM9GIiIiIiIio4f67O0++fFCci+Ztp+EjZyoB1N666Y3BSyXaqZJq/HmqDCoVcHGP5MYu3cluF4IXcd3yxgIWGyrNAdpUoKpY+FhxGvjlVeFyr6uB7p7BlESq/CoQ58HFRegbv6nCye2AzeT8fOPjwuc73wP6TQZ2vgsY44HZfwBGMai0WYG19wE73hHWc8mTAJzVa1q1CjF1fZ1yFNVgv70OdBoFnPlL+LzPRKFCbesy4MIFQmBVG5VKCP12izt3Dr0DaD/Ief2Am4QQTQrQBt8q3P/nM4TPHTa56lDZzukSTNrtwCc3C9V6BfuEx9jxtvh4M4WZbFqjsGYg6FVoAEO0gNEbhRDNYWUlGhERERERETWMyWrD+j+dc9DKaqw4XWZCapxri2V+WQ0qTFZo1CpkJUY16DG8VaJ9u/80AGBQZgISo5s4E81cCbx2vtAaeP0HABQbCyjaOaP8GaJVFQOvDhVCoO7jncd3vgvUlAClx4GR98uhnkTaWOBsVR3z0CoLAbUGiEgQPi/YD6x9QNjVsqZUqPSKaitcF9seKDvhDNRsJmEdgLCWDQuF28emAVHJcoAGAKrqYug1XeVNH+IjdbWHeg6Hs4W0bU/gzH7gxxcAh01YT0waEJsO/O3t2r56rrpfKoRoiV2Aix9zva7vdcC6BcJzim0PjFsMaPTOEM1uh1GrbOf0MhPt5HYhQAOEr8uuD4S20Z5XAuOfE45bTULVW9lJhmgtmUEM0VRWUx1nEhEREREREbnalVuCcpMVSdEGxBq1OFpYiYOny+UQrbTKgg9+y0G7eGGUUFZipMscrfqQZqIpQ7QN+4Xh8GN6pTT9SeT9IcwDO5sjVFpptIiUZ6I52zmj6rupgMMB7HofSO0DpA/wfs7PLzsH3CtCKRxYK8zXAoB2g4HOFwqtkzveBrqPh0GX4XI3XnfmtNQArw4BtBFCFZlaA3zzMHDsJ9fzakqFj8PudFahtT8XKDwkhGdJ3YHCA86qK6XYdkJgZLdCp1FBLOBCfKS+tq8McDZbaL3U6IV5Z59NA07tEq5L6+cRGtZLzyuBSe8BGUMBfaTrdREJQL/rhdbUMQsBndv8PIcNRr3w8+VSiaYM0fb92/U5O2xAcm9g3DPOc7QG4KpXgS2vCLuJBhlDtAAxRgi/AVDbGKIRERERERFRw+zIOQsAGNIxAQ4H5BDtgm5CldPyHw7j9R+Pyud3aduwVk7A2cJoFgfp11hs2HqkEAAwpqcfWjnPZgsfHTahQiuuvdzOJ2wsIAQr0XUNzJcc/R74z11C6DJ7L6B2Cw3L8oDf3nA9po8Bkro4AyUA2PuZUJH2n7uFz3N/QeKY911uFu8tRCvJBaqKxMs5wq6RR38QBuBf/RoQ2UaovvvqXsBUBvSYAOT9Dhz4Brh8KWCuAA5vBIbfA6y8RGi1bNtTCNmKjwotjLoIIQi026DXqlEphk8Jdc1DOyZWobUbBHQZA6jUQlUX0PgKLpUK6HWl7+snvCA8l6QuittohO+33SYHZg4H0L/yZwzVFCJCNxzywf1fCZcvXQwkdBBaW+OzPAO/zhcKf0IAQ7QAiYgQUlqNnSEaERERERERNcyuXCFEG5iZgPIaK/63Nx8HFJsLbDtW7HJ+Q+ehAYBOrkQTgprSagssNgfUKtdNChqtONt5ueQ4ENfeaztnvWeiHVwvfCw7KYRT0cmAPkqoijr9J/DZdMBaI8zmik4WKp16XSW0da65EcgcLuwi+ecXwO4PXdbZOz0O8ZE6eSMHr0P8y044L585APz2pnC5/w3AOX9zXtdxpNBWmtABuOZ1oSVRqtTKPE/4eMMaIUQa8HdArQPy9wjXfS/uLG23yd8foB6VaFIrZ9ZwICIeSB/onFeW1r/22zaW1uAaoAFieGcDHHZ5d04AeNT6KuJ0lThivgNAApD/hxBEaiOE0E/fsFbkYGGIFiCRUcIPgNZuDvJKiIiIiIiIKJw4HA7szC0BAAzMSkB+qTBr++BpZ4gWbXQNeRoTehnkSjShYqlCEWo1eqi+0llFiFYqBFDyxgJmm/x49Z6JduRb5+WfXgIObQASsoAbPwVWjgPM5cL8r/HPC8czhwPnTBIqxB7OBfTRwJJeQlUcIMz6KjoMmMuhUaswsmtbfLX7FAAf7Zxlp5yXj/3sXM/5c13Pi0hwzkxTqTxbHQEhYBt+t/PzrGHCR7X4tbBbXdpza61Eqy4B9v9XuNxxlPCx02hFiNaMs8TUGsBuARxCCKhVq2C12xGnEja/iLKJra65v4jrvSBsAjQACNAeshQVJbyA6R0mj+2CiYiIiIiIiHw5VlSF4koz9Bo1eqfHoltKDABg94lSXLN8M46cqUBBmesmdo2pRJNCGovYzllR4+fdMpWVaKXHAcA5E81icwnt6lSSCxQedH7+13+FNsjCg8C7VwkBWlo/4I6tQNo5QmvgeTOFAA0QPldrhF0qASFQu06cSVZTCtgsGC22ygJAfISXyi9liCZVsrXtAbTpWPf660slVm+5hWi1VqJte0t4/sm9gA4jhWOdLxI+RiYC8Zn+W19d5PU7d+PUwSZfnaAVf26lXUNTejff2vyAIVqAREYKSapBZUFptSXIqyEiIiIiIqJwsVOch9a3fRwMWg06t43CNQPaQaNWYVduCT78NRenxRAtMUqPHqkx6J4a0+DHkdoFbXYHbHZH03bLPPK9MKhfyUslmrOd06po59SgTofFqq+2PZ1BjfvjjHsGiG6LWg27U6jWuupVIKWP0H4IAFVFGNXdeVuT1eZ521JFO6c0Gy1jSN1rbwi1M4TSu7Rz+qhEM1cBv6wQLp8/xzknLmu4UJF37VuN21SgsaT1i/PYDDoN9HBmIgarUJGGMweEj217NN/a/IAhWoBodMIOKQYwRCMiIiIiIqLaFZTV4Lu/TsPhcGCHPA8tHgCgUqnw8uT+eOzyXgCAgwUVOCvO7vr2vlH4ZvYFLvOn6ktZ6WSx2Z2VYfUd9C/J/RV472ph7pikpswZNAFCALX5FWQc/QhAI9o5D28UPvaZCHQYIVweegcQLe4i2vECoMP5dd9PXHvg5v8Ava8WAqcIsVKt8gySog3yae0TIj1vq6xEk2QMrfsxG0Jq53TY3No5fVSibV4KVBUK1Wa9JzqPq1TA0NudFWnNRQolpUo0vdolRIOpTPgoh2jdmnFxTceZaIEi9jwbYUZpNeeiERERERERkW8PffYHvj9wBu9NG4LDpysAAH3axbmc0yFJ6HiSKtUMWrX32V31pNM4K5RMVnvD2iuVcrcIH0/tEnaZbNPJtQoNAA5vAA6tQyYAFd5HtbkB7ZxVxcAhcVOBbpcCff8GHPkOGHCTEJz9/DJw6XMNW7Mkqq0QQlUKu5JunHsBvt1fgEmD23ue6zVEO69xj+uLciaapo6ZaAX7gZ+WCJfHPAFoQiDikUI0sRItQqeBGVbn9aYyoLJI+JoDQFJ4hWisRAsUrRiiqSworWKIRkRERERERN4pNxLYn1eG3OIqAEBWouvA9Q6JQnWUFD6lxBrrtwHA988Ay4YA5aeBrcuBJb2BwsMuIY3Zane2c+obGMbk/eG8LA24l+ahGeOFjw7nrHA9rKhqyO6cez4BbGYg9Rxh3lmbjsC50wCtHuh5OTDjWyClV8PWLIlKEj6KVXNdkmNw+6jO0Gq8xCVlYruqTqxSi2gDJHZu3OP6ogjR6tydc/2jwhD/7hOA3tf4dx2NJbdzCpVoRp0GepWiEq2mDCgUq9DiMsNqUwGAIVrgaJ1loGUVVUFcCBEREREREYWyvNIaeQzQwdMVOF0uzDvLSIhwOS89PgIatTM0S431suuju9xfgU3PCcHFX18B6+YBZSeAzUuhUqnkIE1o5xSCj3q3c9aUCm17+YoQ7S8xRJMq0by0WBpgQbXFhvKaerZz7npP+DjgpvqtqyGkEK3yTO3nmSuBmhLhsvScMob6f96YNNPMY3dOLyHa6T+Fj+fPbd65Z7Vx21jAqNNA71KJVu7cVKBt92ZeXNMxRAsUrfPFrrKyvJYTiYiIiIiIqDX7K79MvrzlcCEcDiBKr0GbKNfgRKdRo70iWEuONaBWNivw9Vzn5wX7nZf10eJ9CuGLshKtXu2c+XuB5zoAX9wOFB1xHj/+K1Ce7zyW0huISHC5qQFCt1Zhhanux8vbDeTvATR6oO91da+roSKlEK2w9vOkVk59jBjmqYD+U/y/HrkSze7Sbuu1nVMK9aIS/b+OxnKrRIvQaWB0n4kmz0MLvxAtBBpmWyiNDnaooIYDFZWVwV4NERERERERBcm3+0/jdJkJNwzN9Hr9/jxn4cWpUrEKrU2ka6vmrvcBQywy26Qgp0jodkqpqxItexNweq/igf7rvBwpDNTXa9WoNNtcNhaIqs9umX9+LrRo7vlE+DwmDYhtB5zcDvzvQeDQBuF4Wn9hmH/1WfmmBpUFcABnysUQrbbKt51iFVqPy+U1+5XczllXiCa2csamA72uBBaeDUz1l6Kd02R1tsB6tHNazYBF7HqTWmZDgVyJJqzdqHPfWKAcqCgQLodhiMZKtEBRqWBTCz/k1VVs5yQiIiIiImqtpq3ejvlf7MGPB723DO7PK/M45rI7ZOEh4N93Ah/fhKw2zkq0lLoq0aSwQtqBsiLfeZ0YwEhzt5QbC9Rrt0xLjevnqecAYxYKl/f9W7j/zGHCRgBxGS6nxmqFKqWyGh8z2Ow2IPsnYYbbno+FYwP+XveaGiOqnpVopYoQDQhc+6QiRJO+H4DrTqoAnFVoAGB03YAiqKSvizgDz+GAaztnTZnw8wwASQzRSMGmFn4rUF1dEeSVEBERERERUTDY7Q758v/25nk9x1uIltlGEaJJM6QAdGzjDM7qrEQzi+9F2w8G4Bb6mITrpHDGbHO2c8bUJ0QryXH9PO0coOMFwNA7hM9VGuCyl4QZX93HC+2YonidzeWmLu2cOVuB5ecBqy8H/jlQmLsW2x7oNLruNTVGfdo5a8qEnUcBIK5dYNYhkSu5rKgy2XyfV10ifDTGOVsoQ4FbO+fZKrPbxgKlzjA30F/LAGCIFkB2jfDiVlPNSjQiIiIiIqLWqNriDEK2HCnyuL7GYkN2oTACKFExAy1DUXGGcmcFWYc4Z2BSZ4hmEttEo9oCbTq5XmcWHlMK0Sy+KtGqioHvFzt325S4h2ip5wgfxywEzp0BXPmKMA8NAAZOBR4pABI6AgBitG4hmrKd8/MZQOFBcY1iCNj/hsAFRXW1c5orgVcGANveFD6PbR+YdUjUzsH8lWblQP4K4Nc3nBVxUiVaKLVyAh4bCxRVml3bOUuPA3bxeUW1bebFNR1DtEASd+g01TBEIyIiIiIiao2UQUhOURWmr96Oq5b9jCrx+LZjxbA7gDZRegzMcg7gd6lEk4baA+gQ53wbX+9KNEMMkNLL63XS7pxmm48Q7d0rgU3PAhseFTYLWDYE2P0RUJIrXD/uGWDEvUD3CcLnugjgshc92y9VKkArrDdOZ3W5Sp7BVlkkhCwAcPdOoRU0Jg0YdEvtz7MppCDH1+6cZ3OEgE2tFdbT/4bArQVwtnM6bHJlIAChrfV/DwA/PCN8LlWiRcQHdj0N5VaJVlRhdm3nlL6/xng5Mwkn3FggkMQdOi0M0YiIiIiIiFol95a8jftPAwB2Hy9F2xgD7v3odwDAqG5t3SrRFCGaouqrfbQKkXoNHA4gtb6VaPpoILk3sP8r53XulWje2jlP7hR2xgSAA98I1WaFB4AfnhXa8gAh4NJH1b4OiRiaRGucA/N1GhUMWjF4KRR3bYzLBBI7AzesEYZqBWr+GOBs56wpBWwWQOO2C6b4dUJsO2E9gaaYiVZpVvzsSPPtpIrAMKlEmzosC7k/fO95XnRyMy7Kf1iJFkAqnfACYTVVB3klREREREREFAwuLXkK+WXV+OSjdzDT9Db6p0dh0VW9kZXoDM4ylBsLnD0mXzTAgg+mD8X704ciQl9Hi6NJqkSL9lKJJoZoYiVaZP52PFX2CLqoTjgr0TY+7jxfHwmcEUOus2KQE5lU/wANkCvRojXOr4nLPDRp9pty18ZABmgAEJEAqMRopMqz3Vau5tNHB3YdEkWIdnEPIWjqmRbrXIe0S2ioVqJJX0txY4G7L+qKWSMzPM+LTmnGRfkPK9ECSK0XK9FM1XA4HK7bExMREREREVGLVy1WE7WLj8DsMV3xxa6T2HKkCHmlNZhY/Aa6a3NwybCbEGPUITNRCKSSog2uAZkiRIO1GgMyE1AvZkUlWpdLgB6XC6HU/q/kUEbanfO8H4Q2xXf0z6PGcKMwCy17k/O+akqB3K2u95+QVb91SHSeIZpL6+gZcRZa22bctVGtBiIThXbOyjNATKrr9WZFENlc6wEAuxWLJ/bFoKwEXNk/HfjhM+F42SmhOi9UK9HUriGaXqtG9yS953lhOA8NYCVaQGnFEE3nMLmWYRIREREREVGrIL0XjIvQ4W+DMzAgMx4AcLigArEOIeRK1QndS0M7tsElvVJw14WdnXdQU+ZaIWWpqf+Dy5VosUIl2fUfAMPvFY6ZXXfnlLRXFSLGqAXy/xAOJHQE4jOFy1IVlEQ6Xl9iJVpkQyrRmkNtO3RK7ZwNqbhrCrkSzY74SD2mj+yE5Bijcx02s/DzELKVaK7tnAAAq8nzPFaikTuNGKIZVBacrTS7vjgQERERERFRi1clD+sXwoXUOOF94o6cs4iFEIxEOIQ52kadBm9OHex6B8oqNACwNiREEyvRlFVUUhgkhjJSJZpSlEEL5O0WPknrB5jKnBsJKMU3sBJNnIkWpXbu1uhaiSa2i7bt0bD7bSq92Dpr8TKKSZ4r19whmlsbsFQRBwhhZshWorluLADAR4jGSjRyJ75AGGBBcaU5yIshIiIiIiKi5iZVokXqhXAkTdwM4ERROaJUYrggBTXeNCVE8zbPyy1EM2g9Y4FIncY1REvs4rxSrRi839hKNLWXSrSaMqBc3IU0qVvD7reppOfkHlwBikq0mGZai48QTfkzUnYqvCrRbF7ykChuLEDuxN05jTCjuIohGhERERERUWtTZXavRBOCpBhUOU9qSIjmrVrKF5OXeV5SoGapAuw2j3ZOAFCrVb5DtC4XAxpxxlVDZ6KJhSYR3kK0QnEeWkxa8wdD0o6cdovndc3dzimHUO6VaJXOy6Fciea2sQAAtnNSPSkq0c6yEo2IiIiIiKhlyNkCxLarV4hUaRIqciJ0YiWaFKKpFCFaTanrjaxmoPQ4kNgZKD7idl1j2jljnceUYZC5EjqNlw3wasqAosPC5bR+ABzO61L7AgkdgGObgfZD6r8WQFFoomznFEOjgv3Cx+aehwY4q79s3irRpGq+5mrnlNoh7a7HXdo5Q7gSzVs7p7dKtDBt52SIFkg650w0tnMSERERERG1AGePAW+PF8Kl23+s83T3SrQ2UXroNWrE2mupRPvvbOD3D4DJHwCHv3O9rr4hmsPhujunRGsQQiO7FTBXeq1Ew+m9wsfY9kBUEpDY1XldUnfgnL/Vbw3uxEITo8oZokUbxCqwvN+Fjyl9GnffTVFrJVpz787payaashItL/Qr0ewtsxKN7ZyBJL1AwIyzbOckIiIiIiIKfyXHhY9led6vt1QDB9fLbZdSJZo0E02lUiE1zohYlY8Q7WwOsPtD4fI3DwOluUIFV7fx4v3XM0SzVDurmZQBkErlMhdNrxHCPZsyHlC2cgJAXHt5nhnaNmFemXgfRjjfH0dLlWgndwgf2w1q/P03ljQTzVZbO2eQQzST28YCYVWJ5iVEi2IlGrkTXyCEjQW8/GUkIiIiIiKi8GIqEz56q64BgN/eADY8Box5Ajh/NqotYiWaXiOfkhpnRGyJW4iWt1sI34qPOsOvUjGw6zrGOdjeWs+ZaMr2P51bK6I+WmghNVdAJxZ/WKCFRgq3Sk8IHxM7CR/VGmDsU0KLZ+o59Xt8b8TH0sNtd05LDZAvVr8FI0TT+AiuAGd41dztnMq1OByu38+SHMAihnshV4nmZWMBq1tRUUQbZ/VfmGGIFkiKEI0z0YiIiIiIiFoAqWrMV1tl+Wnxo1CpJleiGZxvv9PijNC5V6KtfwTIVrSHxqQ7d6vseRWQu0V8XB/hna916mMAtVsTmlyJVgGDRhhDZHLoYFSZ5ePybSVDZtTvcWsjvkfWKyvRjFogf4/QShmZ1PAdP/1BnokWSpVoyhCqxrWyS7nZhDGuWZZVb942FpAq0bRG4blEh+fOnADbOQNLKlVVsZ2TiIiIiIioRagRK9FsJqFCyJ0UdoghmzwTzb0SzX13zrM5zs9TzwHGPydcVuuAbmPlofz13p1T3lTAS/gjBULmSug0QixgVtbYBGpHSp0YojmU7Zxa11ZOlZeNDgJNXdtMNC9z5QK6Fi+VXMpWTiVDnPP8UOGtnVMKfiOThI9hHKKxEi2QlLtzMkQjIiIiIiIKf1I7JyDsOii+75NJ4YfYwua1Ei3WiLPuIZoU1kx6D+g4UthR8/y5QGIXodpIDKDqvbGAXE3mLURzVqJJGwuYoGivM1e5nucvYqGJThGiRem1wBExRGs/2L+PV1+aelSiBXNjAel7qYsUvieVZ4TPI0KsCg1QbCzgJURr0xEoOwEkdGz+dfkJQ7RAknbnhJkz0YiIiIiIiFoCZYhmrfEM0aQKHLGFzVsl2sCsBGxXtnNWn3WGaJnDgIgE4fKYhc5zpMH+9a5Eq2VXSUUlml4jVH6ZHVpAKgKrLYBrCvE5aO2KEM2gBU5uFz5pN9C/j1dfciWal5logarK87mWWkI0fTRw7jTgh8XiFUGo2quLXImmbOcUv9/9bwAGTgU6Xdj86/ITtnMGkrw7p1CJ5vBW6ktERETUyixfvhwdO3aE0WjEoEGD8NNPP9V6/qZNmzBo0CAYjUZ06tQJr732msv1o0ePhkql8vhz2WWXyec8/vjjHtenpqYG5PkRUQun3EnTfWA64Aw/xOqbSrMQqrWpPgZ8Og3Y+irOaavBjQPinbeRAjSogMg23h9XCtHqmolmswB/fQ2czRY+N8R4niMFQlv+iSmbLsQ5qiMwu1SiScFRZO2P1VDie2SNw/kcYoxa50YGSd39+3j1paklRDMFKFD0RaVoh5QyBGWQd/4c57nVZ5tnTQ3hdWMB8fttiAHOmQREh+fOnAAr0QJL2lhAZYbN7kBZjRVxEeG5AwURERGRP6xZswazZ8/G8uXLMWLECLz++usYP3489u3bh8xMz2HS2dnZmDBhAmbMmIH3338fmzdvxqxZs9C2bVtce+21AIDPP/8cZrPzjWxRURH69euHv/3tby731bt3b2zcuFH+XKMJsTkyRBQeatwq0dzZxQocMTioMlkxRLUffb/5B2AuA/Z+Cvz8MgxJ3TxvG5Hge8aV2OlU6+6cNaXAB5OA4784j+lrCdEKD8II4H7tx64z0WpKXM/zF/E9ssambOdUOyuVdH4O7epLqkRzb+dU7orZ3DPRAKGaS6VxrSrUGoDp3wKf/B8w4p7mWVND1LWxQJhjiBZI4g9IhEpIs89WmhmiERERUau2ZMkSTJs2DdOnTwcALF26FOvWrcOKFSuwePFij/Nfe+01ZGZmYunSpQCAnj17Yvv27XjxxRflEK1NG9eqjY8++giRkZEeIZpWq2X1GRE1nUs7p5eqMPd2TosNL+peg9ZcBqT1B0pyhJlWVcWet41K8v24UtuoxcdMNHMVsPpKIO931+Pe2jndqtMKEYdIKJ5LZaHwMUDtnGq787GiNIrgyr01trlovLRQAmLrrFgN1tztnNJ61BrPzQ3aDwbm7Gme9TSU140FxJBUo2/+9fgZ2zkDSXyBiFQLLwrF3FyAiIiIWjGz2YwdO3Zg7NixLsfHjh2LLVu2eL3N1q1bPc4fN24ctm/fDovF+8zZlStX4vrrr0dUlOsbnkOHDiE9PR0dO3bE9ddfj6NHj/pcq8lkQllZmcsfIiIAru2cNi8hmls7Z7XJgvYqMZSa8iGQNUK4rAwZJJGJvh9X66MSbcc7wP8eBj6fIQRoardamdo2FhAds6fCpowHpEo0f1eGSSGatQad20Yho00E2ugdHtc3O1+VaFIVGtB8VXLuIRqgaOdspmq4pvDWzilXogUpJPUjhmiBJL4AGFXCX8SzlQzRiIiIqPUqLCyEzWZDSkqKy/GUlBTk5+d7vU1+fr7X861WKwoLCz3O/+2337B371650k0ydOhQvPvuu1i3bh3efPNN5OfnY/jw4SgqKvL6uIsXL0ZcXJz8JyMjoyFPlYhasjrbOaXdOU0wW+3Q2qqhVolBkTEeaNPJ933XFqLpfMxE+2Ye8OsK4K//CgHGTV8CcYrXrNpmoonM0EIFLzO8/d7OKYQoKqsJa+8diQ1zRjk3GVBpnBVhzU2eieYjRNNHA+pmik+U7ZxSiCbPZWumarimkL5OrEQLjIYOljWZTFiwYAGysrJgMBjQuXNnrFq1qplW20Dii5wBYiUaQzQiIiIiqFSuu4k5HA6PY3Wd7+04IFSh9enTB0OGDHE5Pn78eFx77bXo27cvxowZg6+//hoAsHr1aq+POW/ePJSWlsp/jh8/XvcTI6LWoa6NBeR2TjOqzTZEQ6gcc6g0wlyzxM6u56sUoUmt7ZxiJZpyd06HA7CIu3waYoHLXwY6jgRSejvP8bo7p2sYo4YdGti9nBeYdk5Ya2DQamDUaZxBpDTzLRik6i/3SrRghFculWjiz5K5lp1WQ41cieZtJlr4V6IFdSZaQwfLAsCkSZNw+vRprFy5El26dEFBQQGsVi87aIQCaWMBCC+sZ9nOSURERK1YUlISNBqNR9VZQUGBR7WZJDU11ev5Wq0WiYmuFRtVVVX46KOPsGjRojrXEhUVhb59++LQoUNerzcYDDAYwv8/+0QUAKZS5+VaK9FqUGm2IlolhF4qQwygUgFt3EK0mDSgTNydMrIeM9GUj6kMfebsBYxxwuWU3sDBb4TLXjcWcD2mgR3q5qhE81ZNJz2fYAYsvnbnVO6K2VxUilon9xDN2/cy1Cg3Fljzd+F7Lc3x04T/v6tBrURTDpbt2bMnli5dioyMDKxYscLr+d988w02bdqEtWvXYsyYMejQoQOGDBmC4cOHN/PK60l8EdA5hPCsiJVoRERE1Irp9XoMGjQIGzZscDm+YcMGn/+fGzZsmMf569evx+DBg6HTuW7Y9PHHH8NkMuHvf/97nWsxmUzYv38/0tLSGvgsiKhVczjcKtG8zUSTQjQzqsxWxIiVaDDECh/d2znj2jsv11aJJu/OqQzRFO8xla1yDaxE06jsULtXoqnU/g+2FJVoEKuKnSFaEHdu9FWJFoxZZCqVcz0eM9HCoZ1TrESzVAL7vwIOrXdujKBlO2ejNWaw7H/+8x8MHjwYzz//PNq1a4du3brh/vvvR3V1LVv8BpNYbqt1WKCGHWcrTK4ljUREREStzNy5c/HWW29h1apV2L9/P+bMmYPc3FzMnDkTgNBGOXXqVPn8mTNnIicnB3PnzsX+/fuxatUqrFy5Evfff7/Hfa9cuRJXX321R4UaANx///3YtGkTsrOz8euvv+K6665DWVkZbr755sA9WSJqecyVQoWNxNvGAmI7p9lUjc93npQr0eTZZDFpztZMlQaIUVTi1rqxgBgyWeoTovVxXvYWvOhdh+Sr4PAM0fTRQqDjT3Io53Cu3RoCrX5qXzPR3HbFbC5yS6TbTLRwauf01urcAirRgtbO2ZjBskePHsXPP/8Mo9GIL774AoWFhZg1axaKi4t9zkUzmUwwmZwvbM26s1JEvPAD5LAhCaWYdvh54A0HcNsm12GBRERERK3E5MmTUVRUhEWLFiEvLw99+vTB2rVrkZWVBQDIy8tDbm6ufH7Hjh2xdu1azJkzB6+++irS09Pxyiuv4Nprr3W534MHD+Lnn3/G+vXrvT7uiRMnMGXKFBQWFqJt27Y477zz8Msvv8iPS0RUL8oqNKDWSjRTTRWW/3AEl6rdQjS1GmjTESjYBxhjnRVqQP1CNG+VaCq163tMZctoVbHnfVlcC1E0sEPj3s4ZiKonZbWZtUYIzuRKtCDORJM2NLD5aOds7vBKrRUCWrkSLUhhXmNIP4feWp05E63pGjJY1m63Q6VS4YMPPkBcnNDrvWTJElx33XV49dVXERHh+Zdu8eLFeOKJJ/y/8PpQa4CYVKDsJHqpc9Dd9AeQD6A8z7Vkl4iIiKgVmTVrFmbNmuX1unfeecfj2KhRo7Bz585a77Nbt27yhgPefPTRRw1aIxGRVya3ogxvQYFYiaaHEIDEqKTB/4p5Vm06CSGawS1Eq7WdU6pEUwRgUojmXuGj0QJZ5wPHfwG6jfO8r8xhQiAjztrSwA6VeyWaLtLzdk2l0QNQAXA4A0hLCMxE81mJFqQ2SqmdU6p6DEZbaWNJM9FsXirRWkCIFrR2zsYMlk1LS0O7du3kAA0AevbsCYfDgRMnTni9TdB3VopNBwAMVB90HnMfVkhEREREREShr8Y9RPNdiWZQWQA45N05XUI0aYdOY5zr8Vo3FhCLRuwW59w1aYaXxsusqalfAvcf8l7AEdkGmLsPGHoHAB+7cwYiOFKpPCvqQmEmmvvGApZq4M2LgP89KHze7CFaOLdz1hKitYB2zqCFaI0ZLDtixAicOnUKFRUV8rGDBw9CrVajfXvvlV0GgwGxsbEuf5qVGKINVilCNEuIznAjIiIiIiIi3zwq0WrZWABCNZrXEE1qt4yIdwvRamnn1Lm1QiofX6PzPF+jE8IyX4xx8qD3aweko2OiW2dXoKqe5F1GTa4fg1qJ5raxQN5u4OQO5/XNvStmS9hYwP3vhlortDKHuaA+g4YOlr3hhhuQmJiI//u//8O+ffvw448/4oEHHsCtt97qtZUzJMQIIVp/9RHnMYZoRERERERE4acB7ZwAoIfFc2MBAOh1FdD3b8CIe53H9dGuQZk7ZaWW1AIpt3M2ctdDsWooKVILncp9JloA2jkBxQYJ4tdF+hrqgjkTza0STRGEAgh+JZo8E62Zw7zGkDYWcK9EawFVaECQZ6I1dLBsdHQ0NmzYgLvvvhuDBw9GYmIiJk2ahKeeeipYT6FuYiVapEqRwnp7oSUiIiIiIqLQ5r6xgLeWNcX4HqESTXz/p5x9FhEPXPuWcHnv58LH2qrQACFYUeuEdk6rGEDJ7ZxeKtHqQwo8HDbXXUeBwAVHHpVoITQTTfp6ur9n97YLa0DXI1WiiWFeS6hE0zYy6A0xQd9YoKGDZXv06OHRAhrSxBDNhaWq+ddBRERERERETeMxE00IWypNVmw9UoQLeyRDo6hienx8Zww8ZAROwLUSTSlB3CU4sUvdj6+LAEwWZ0AhhXiNDaCkwMNhd6mgAxDAds5Qnokmhmju4Wh0avOuR65EE78nYTUTzUeIxko0qpfYdp7HLKxEIyIiIiIiCjvulWhiUPDKt4fw+o9H8chlPTFdEUa1j1WjfYRYmeYrREsfCNz8XyCpW92PrzUAJjhbIaUKqUa3cyrCGvcdjgNV9SS1rIbkTDTxeyWtKToVGHQzMPj/grMeu1X4XkvfZ2Oc79uECl8bC7SQSrTwn+oW6rxVorGdk4iIiIiIKPz42FhgZ+5ZAMD3BwrgUFSipUapncGbrxBNpQI6jgRiUup+fGmHTuk9ZZPbOcVIwGHznAOmC/BMNOk5SIGgNhRmorlVoiX3AC6c3/xtlCrFTLTqEvGYOjxmokmbB7i3wLaQSjSGaIEWk+Z5jO2cRERERERE4Udq5zQIFUFfbDuK3cdLcKhAaLfbfuwsbFbnTLSkCEfdIVpD6NwCqKZuLCAFHna7l5logd6d022H0ZCoRJNmokkVfkFak7Qehw2oKREuG+PCY3dLuZ3TvRItiO26fhQG34Ewp9UDUcmux9jOSUREREREFH6kSrTotgAAjcOMVZuzUVIlhC8mqx0VNc4KHL3D6ryNcmOBxpJ3tnQLoJrazul1JlqgNhYIwZloarfdOeU1BakFUe2lEs0YH5y1NJS0dvdKNPeQNkwxRGsOsW7VaNJOKkRERERERBQ+pKqyKCFEM8CCb/bmu5xSVaOowLGZ/FuJJgdQ7rtzNrYSrbbdOQPVzhmClWgat0o0ecOGIAV7yt05pUq0iPjgrKWhfG0sYC73PDcMMURrDu6bC1gYohEREREREYUdMfixiVVlBlhgsgrhk1olnKJs54TVzyGazq0SrantnNJMNLuXmWgBa+eU5rpJGwuI7491QZyJ5lGJFux2zjCuRPO1sYD7phxhiiFac3DfXIAhGhERERERUfgRw5UqldDqqIdFvuqSXsLGABqVIoyqKXUOq/dLJZr7xgJSiNbYjQWUlWjNtDtnSFai+dhYIGjtnIrdOcOtEk3tYyaaqaL51xIADNGawzmTUZU8AJttvYXPGaIRERERERGFHzFcKXUIrY4GlTNEG9MzBfPG90CUVuU8v6rIedkflV3uAZQc9jQygFIHcSaaJZRmokntnKFSiaZo5wzbSjS3dk67xfPcMMQQrTlkDEHZjd9gi10I0RwM0YiIiIiIiMKOqaYKAHDGLFQoGRSVaF1TYnD7qM6IMyreZleeET7qo/2zs6LU8ii9p2xyO6cY+Nm9zETTBbgSTQpZQrESTV5TsDcWCMOZaPLGAubazwtTDNGaSZsoPWog/AU0iy+8REREREREFD5KK4SWtB9yhOqpaK2zeqtLslhpppwtVlkofPRHKyeg2FhADHnkjQWa2s5p9zITLUAhmnIOG+AMBLUhMBNN3lhACtGCvbFAOM5E0wR7BQGlDfYCWgu9Vg21PgJwAOaaCgQxYyciIiIiIqIGcjgcUInVNYUWI6ADEvQOdIqMQoekKEQbFC14kio/h2g6ccdMi1iY0dRKNJd2TvfdOQMUoqndvk6hVIkGh7AuuZ0zSJVoKsXGAuFWiabyUauV3Lt51xEgDNGakSEiCqgCLKxEIyIiIiIiCiuFFWboHWZABZSLM9GMKgu+u3+064nK2WKV4kw0f4VoBrHazSwOaW9q2KNStA0210w0KUSTHi+UZqIBQjVaU2fN+Ws9DsVMtIiE4KylodRulWhp/YAelwP9pgRnPX7Gds5mZIgUXvCsJoZoRERERERE4eRQQTn0EAbPl0FoPdQ6vMx9CmQlmnQ/pnLho9zO2dgQTYwEHG4z0TR6/2yE4I2yVRFQVKIFMURTtsPaLc5gL2gbCygq0arPCpfDtZ0zOgUY9SAQnxGc9fgZK9GaUZQYojksDNGIiIiIiIjCwfPf/IW1e/Jwae9UDBU3EpAq0dTehqdL4RDg/5loHiGan9o5lWsetxiISQ3cUH1pgwU5RJNmogWxndO9Es0qVaIFa2MBRctruLVzum+goW5ZsRMr0ZpRdLSY5Etb+RIREREREVHo+fV14F/XIye/CK9tOoJjRVV4d/NhaFQOAECFSgjRVFYv7+2UbZHVxcJHQ6x/1qWXQrQy4aMcojV2YwExErApQrR+1wN9Jjbu/urD10w0XTA3FlAEPXZrCGwsoKxEKxEuh2slmnt7Z5hjiNaMYmKEF06vL7REREREREQUVHa7A1uOFML+00vAwf/hf+u+ht0hXqmoOnt2ygjhgsPmGkA5HJ4D+gH/zbPyVYnW2CouOayxeB4LFI8QTZqJFsRKNJXKuS5lJVqwNhaQ1mKudAZ64VKJ5r6xACvRqLHiY4UQTWtniEZEREREROShogDY8k9nG2Qz27j/NG5+c7OwDgA7DuYCEDIWPZxB04CuWc4bSSEH4D1AA4CoJP8sUA7RxI0FmtrOKVeimT2PBYpy50m7TREEBnEmGgCoxWo+l0q0IM9Ek/4eqNTOKsRQ5x7CMkSjxkqQQzRTHWcSERERERG1Qr+9Cax/BNj+dlAe/vCZCqSoiqGGUH4W4ajCuR0ScH6XJHlTAYdKA+ginTeyKt7fKWeLKUW19c8CfW4s0Nh2TjHwUFbTubfj+ZtyYwHl1y6YlWiA82tot4bAxgLi10jamMIY5zlrLFR5tHM28mczRLWsSDDEtUmIBwAYHGbY7Q6o1argLoiIiIiIiCiUSOGQNPOrmZ0pN6EdiuTPY1TV6NQlCckxRhw7/CcAQKU1AhqtEBY4bG4hms39LgWR/qpEE+ds+31jAUU7Z6Ar0ZTzvpSjjrRBnIkGeG/nDNbGAiq3SrRwmYcGeKlEa1kz0RiiNaM2cUIlmkFlwZnyarSNi6zjFkRERERERK2I1A7pqy0ywM6Um5CmcoZo0ajGgMwEDOnQBicPJQKH4QxWtAbAUuUaBDl8hGh+a+cUNyiwVAqBnRTgNbZiSq5EC8JMNIfduX6VRggmg0muRLOEwMYCUiWauDFFuMxDAzgTjfxHa3CGZgVnS4O4EiIiIiIiohAkhVBBDNHSVc55bNGqavRvH48IvQYPXNxROCgFVlL7oXKemK9KNH/PRAOEarQmt3NKM9GCVYlWLVwO9jw0wNl2GEobC1SFYSUaQzTyG0V5anFJSfDWQUREREREFIqk8MxXGNVQpSeBX99wDuJXqjjjcehMhQnpikq0dhEWxEWK4YrVbdi8FPwoK9EC3c6pNTiDHXOFH9o5xUigWds5vcxEC/Y8NMBZCReKGwuEUyUaNxYgv1GrYYHwAlxcGpwefyIiIiIiopBl93Ml2o8vAP97ANj7qevxPz4GXuwCbFvpclho5yyWP8+IUoRi7sGKFFwpZ6J5a+fURgD6qMY+A0/KzQWavDunezunStiKNJBcQjQxgNQFeR4a4FaJJrXJBrkSTQo3w6oSrWXPRGOI1sysYulvSSnbOYmIiIiIiFzIM9H8VIlWfVb4WFXkejx/j/CxYJ98qMZiQ3mN1aWdM8WgqNBynz8mV6LVsbFAVJJ/gym9YnMBWxMH4CtbK5WfB5L8mPYQq0RTzEQL9rrcvw/KNt5Q5772xrYahyiGaM3MphES9tIyVqIRERERERG5cDjEj36qRJMqrJRBF+AMnxSzwM6UC+ekKyrRkvWKeWdysKJ3/Wg1Cesuz3eGUUqRiY1evlfS5gKmMj9Uokkz0cyunweSShHcWUJpJpqiKk+qAAv2xgISKTgNBx6VaGznpKYQ/xKWMEQjIiKiMHL48GGsW7cO1dXCGx6H9EaXiMifpAo0f81Ek8IQ5dwy5eeK0KuwwoQoVCNOVSkfM9qclz12bFTORNu6DHipO/Dn555riGrblGfgyaWds6kbC7i1c7oHIIEQqjPRpHZOs+J7HrR2Trfvgz/bgQONGwuQP6n1UiVaeZBXQkRERFS3oqIijBkzBt26dcOECROQl5cHAJg+fTruu+++IK+OiFocuZ3TT0G9r0o0q/dKtDSVW9unqdzzNlKwIoVoNhNw5i/h8mlne6jMXztzSuQQraLps7uksEYKL5ujEs3bTDRtCMxE03gJ0YLWzuleiRZGIZraPUTjTDRqAo1B+OGvqCyH3c7f4BIREVFomzNnDrRaLXJzcxEZGSkfnzx5Mr755psgroyIWiR/z0STKs18VqIpQrQKE9pJIZpUkeUSokmBj5eNBWzS41R7rsHv7ZxeZqJpGhn2eFQNNUclmviYDhsr0XxxrwgMpxCN7ZzkT3qj8J9Prc2EwgpTHWe7sduANX8HNj0fgJUREREReVq/fj2ee+45tG/f3uV4165dkZOT06j7XL58OTp27Aij0YhBgwbhp59+qvX8TZs2YdCgQTAajejUqRNee+01l+tHjx4NlUrl8eeyyy5r0uMSURD4e3dOKWTyqEQTPxcr0fJLa1BQZkKyStyIoE1H4aO5wvO+tO4bC9Qowjov7/FCup3TLRII9M6cgKISzeYMHUNhJppGXJf0Pdfom+fr4U1YV6IxRCM/Uolb9xphxvGzXn5LUZvCQ8D+r4R+eyIiIqJmUFlZ6VKBJiksLITB0PDKgTVr1mD27NlYsGABdu3ahZEjR2L8+PHIzc31en52djYmTJiAkSNHYteuXZg/fz7uuecefPbZZ/I5n3/+OfLy8uQ/e/fuhUajwd/+9rdGPy4RBYkUnvlrJprPjQXEz+1WrP8zH+ct/hb/+PYQDBDPj04RPpornGtx351TCq5sFmeIZvHyHi9g7Zx+2FjAPfDgTDRnJVowg71wDtE8KtG4Oyc1hRiiRahMOHG2qmG3lV7ovf12g4iIiCgALrjgArz77rvy5yqVCna7HS+88AIuvPDCBt/fkiVLMG3aNEyfPh09e/bE0qVLkZGRgRUrVng9/7XXXkNmZiaWLl2Knj17Yvr06bj11lvx4osvyue0adMGqamp8p8NGzYgMjLSJURr6OMSUYDt/Qx47Xyg6Ijrcb+3c/qaieasRFu/77R8WAcxDItIcJ4rVSbJ7ZxiYKUMg9wr0ZTBQaS/QzTF7pzS82v07pzuIVqQZqLpQnAmWrBaOQEvGwuE0+6cnIlG/iQOTDTAjBMNrURT/haFO2IRERFRM3jhhRfw+uuvY/z48TCbzXjwwQfRp08f/Pjjj3juuecadF9msxk7duzA2LFjXY6PHTsWW7Zs8XqbrVu3epw/btw4bN++HRaLxettVq5cieuvvx5RUVGNflwiCrBPbwXy9wBf3uF6XA7R/NXO6Wt3TqkSzYJduWflw3KIZohxhijSXDS5nVOsUJJCF5cQTXyPp1NU8Bpjm/gk3EiVaFXFzmNaP1WiNUfgIQV3dlvTN0bwJyncM4vf72BWx4Xz7pxs5yS/0gkvuEZYGhGiiS/acLhsxUxEREQUKL169cIff/yBIUOG4JJLLkFlZSUmTpyIXbt2oXPnzg26r8LCQthsNqSkpLgcT0lJQX5+vtfb5Ofnez3farWisLDQ4/zffvsNe/fuxfTp05v0uCaTCWVlZS5/iCgAKk67ft5c7ZxiqGa1mHHkjHOQvBbi46q1rrPHlPchtXNKYYFLO6eXyirpfvxFqkpShmiNrkRzn4nWHJVoihCtqe2o/hRSlWjh3M7pXonWskK0lvVswoH4GwmXds6NjwM5W4Cp/669jFUO0SC8gDd2eCQRERFRPVgsFowdOxavv/46nnjiCb/dr8ptULPD4fA4Vtf53o4DQhVanz59MGTIkCY97uLFi/36nInIB/cZYnIlmp86b+w+KtHE91aVNUIwltkmEqmxRsSddgAOCAGKIQaoKvIM0TzaOW2eu4DqIoCEDkDVWSCpm3+ei0SuRCtyHvNbiNbMM9GaujGCP3nMRAtmJZp7iBZG7ZysRCO/0kqVaGacPFst/AX9+WXg+K9CkFYbm6JlQRmoEREREQWATqfD3r17aw24GiIpKQkajcaj+qugoMCjSkySmprq9XytVovExESX41VVVfjoo49cqtAa+7jz5s1DaWmp/Of48eP1eo5E1EDuIZq8O2fzVKLViCHawMx4fDBjKKYPF3cilkI0QJg9BjhnVEvtnGplO6fb5gNqLXDnb8D9B/0fxriHaCp149swPTYWaOaZaFL4GApBi7w7ZyiGaOFUieb2MxUKAakfMURrbmKlWSRMOFFSDfuxrc7r6vrtgUslWo3v84iIiIj8ZOrUqVi5cqVf7kuv12PQoEHYsGGDy/ENGzZg+PDhXm8zbNgwj/PXr1+PwYMHQ6dz/Y/5xx9/DJPJhL///e9NflyDwYDY2FiXP0QUAOZK18+bbSaa8N7KZBKOD8hMgE6jhsYhhjoarWKAf7nLbeT3bXIYZPGciabWCCGMLgA7PErrkkI0TRPCHo+dFJuxndNhC60QTZ6JJm4k0ZSva5PXovi+qLWh0e5aXy18Y4EQ+EltZWJSAQCZ6gKYzXZUH/wecqZsq2PXTfd2TiIiIqIAM5vNeOutt7BhwwYMHjxYHtYvWbJkSYPub+7cubjpppswePBgDBs2DG+88QZyc3Mxc+ZMAEIF2MmTJ+UdQWfOnIlly5Zh7ty5mDFjBrZu3YqVK1fiww8/9LjvlStX4uqrr/aoUKvP4xJRkLhXnEmf+2smms/dOYXwzGwR3mP1z4gXjtsUu116zESTducUwxWNl7ZEaSZaIEMhecaaw7nWxgrKTDRFG2wotnOaxBAtmJVoynBTHwX4qSK8WbTwds6W9WzCQVo/AEBfdQ4ABxzZPzivqysYYzsnERERNbO9e/di4MCBAICDBw+6XNeYNs/JkyejqKgIixYtQl5eHvr06YO1a9ciKysLAJCXl4fc3Fz5/I4dO2Lt2rWYM2cOXn31VaSnp+OVV17Btdde63K/Bw8exM8//4z169c36nGJKEQ08+6carsVqbFG9EoXq7uUg+7dQzS5nVPaWMDLTDTpnECGUQa3+VhNCaA82jmbYyaatLGA1RlyqkMgRAvVjQV0YdTKCXipbmxZsVPLejbhILkXoNYizl6OnqpcRBX96byurhZNVqIRERFRM/v+++/9fp+zZs3CrFmzvF73zjvveBwbNWoUdu7cWet9duvWTd5woDGPS0QhQvp7HOCZaA6bCSoAOtgw44JO0GnUrud73Z1TCtjcQzSrZ+VcIFvY3Hf7bFIlWhBmoqkUIZpN0T4bbNL30yJuABgqM9HCaR4a4NkS3MJCNM5Ea25aA9C2JwBguvZrqKD4z16dlWgM0YiIiCh4Tpw4gZMnTwZ7GUTUkskbC/hhd06Hw/vunHYbVGLlmF5tw5QhGc7rGtLOKVVP2RQz0SSBrOgyxDo3NwCcu4U2hkfrXXPuzqmciRZKlWgh0M6pdmvnDCcelWgtayYaQ7RgEFs6r1Fvdj1eZyWasp2TIRoREREFnt1ux6JFixAXF4esrCxkZmYiPj4eTz75JOx2P7VbEVHr46viSWrj9MdMNGWwZTM5gzlFQUKU1oFIvaJSRtnOqXffnVO8Tg7RvLQlSgJZfaNSAfGZzs+bVInm1pbfnDPRHDZFO2cIVCtJQZ70MxgqGwvoo32fF4o8NhYIgYDUj0LgJ7UVSjsH+B1Qq8QX8ZS+wOk9rEQjIiKikLNgwQKsXLkSzz77LEaMGAGHw4HNmzfj8ccfR01NDZ5++ulgL5GIwpEu0lnxo+TPmWg2t2DLWgNUFLi0Q2rhFtZJoY5G2c4prlN6D+bRzmnzrEQLdPVNfCZQKM6pDLd2TuXXRtqIIRTaOd3X0JQKv6YK63ZOzkQjfxMr0QBgm707BqWeA/XpPYCluvbbKUM0bixAREREzWD16tV46623cOWVV8rH+vXrh3bt2mHWrFkM0YiocXQRPkI0m+vHpnCvDvt5KbDpWWDM4/IhjcMt/FK2c2ojhMtyO6e0sYAYrkjtf3aLZ+VcoMMol0o0f24s0MwhmtSNFQrVSu5rCGolWhiHaC18YwG2cwZDSh84IJTN/st6ESps4g9VQ3bnZCUaERERNYPi4mL06NHD43iPHj1QXFwchBURUYugi3ReVgZQfq1EcwvITu4QPp76XT6k9gjRlO2cYnghhX3y7pziPDKXjQXcK9ECHBzEKea4NakSzb31rhlnogGK6r4QCNHc18CNBRonGHP2mhFDtGAwREM1/C5s1Q7FWvtQlFjEPnTuzklEREQhpl+/fli2bJnH8WXLlqFfv35ebkFEVA/KEM1c6bzsz5lo7t07VYXixyL5kGeIppjRpdG5HvPYnVO5y6T7TLRmaOeUhF07pzJEE7uxQrESLWQ2Fgj3mWgtqxKtUc/m+PHjUKlUaN++PQDgt99+w7/+9S/06tULt912m18X2GKNfQrvndkB0558FNeokQk0bCYaNxYgIiKiZvD888/jsssuw8aNGzFs2DCoVCps2bIFx48fx9q1a4O9PCIKV8r5U+YKwBgrXLb7sRLNvZ2zUgjRHFVFkMbpqxx24THV4ht/ZTunFKpIVWY2t3ZOeXdOL5VogdydEwDis5yXmxKiebRzNnMlmjQTLRSqldxnogWznVMVzrtzqgCoAIgz4FtYiNaomPmGG27A999/DwDIz8/HJZdcgt9++w3z58/HokWL/LrAlqxjkvCXobCmvpVobOckIiKi5jVq1CgcOHAA11xzDUpKSlBcXIyJEyfiwIEDGDlyZLCXR0ThStopE/BeiRaIjQUqzwh3rahEA+C2i6einVOjdb1eer/mtZ3TrXKuOSvRmsK9aqg5KtGUjyF9TUOhndM97OHGAo2n/PkPhe+tHzUqEty7dy+GDBkCAPj444/Rp08fbN68GevXr8fMmTPx2GOP+XWRLVXntkJZZn6VeKBBlWjcWICIiIiaR7t27biBABH5lzIkkwb3K48HandOAKqqs67H7RYAesVlCAGa2lc7p1SJVttMtACHaNHJzstiONgoHlVDzVARplIJlVYOm/M9cCi2c4bMxgJh1s4JiJV04t+JUKgy9KNGxcwWiwUGg/ADtXHjRnm3ph49eiAvL89/q2vhuqUIWyafrBD/geBMNCIiIgoxb7/9Nj755BOP45988glWr14dhBURUYugDMlcKtHEii5/zERzb+cUqexuBQnKsM2lnVMKySxC5ZzczimGK5paQrRAt0WqVM7LFaebdl/KkEN5v4EkfW2lmWjurZTB4LGxQDAr0ZTtnJG+zwtVyvWznRPo3bs3XnvtNfz000/YsGEDLr30UgDAqVOnkJiY6NcFtmRdkqOhVgElZvHbUGclmuKFmSEaERERNYNnn30WSUlJHseTk5PxzDPPBGFFRNQiuIRoFZ7HA1GJ5ovPdk7FTDS71bkmKUSTwgGbxTOwa87qm6aGaMrArzlmogHOr488Ey0UKtHc2zmNwVkH4BaihWE7p4ohmovnnnsOr7/+OkaPHo0pU6bIOzP95z//kds8qW5GnQYdkqJQ4xAT7oZUonFjASIiImoGOTk56Nixo8fxrKws5ObmBmFFRNQiKCvNlJVo0nGHP3bnrGeI5lKJJrWg6RQhmdW1iEHjHqJ5GbXTXGGUP7hUojXDTDRAUYlW4/p5MLlXohlig7MOoAW0cyp+jkLhe+tHjXo2o0ePRmFhIcrKypCQkCAfv+222xAZGYalhkHUPSUGpiLxL2tDZqJZORONiIiIAi85ORl//PEHOnTo4HJ89+7d7EAgosZrjploPto5az1PrkTTOcMlu8X1vZhciSa9j/NSDNEcwcHlLwP/nQOMW9y0+3EJPJq5Ek2axRYKw+eV1XCJXYAuY4K4lnDfWIAhmovq6mo4HA45QMvJycEXX3yBnj17Yty4cX5dYEvXLSUGe/fV8uKr5PIbElaiERERUeBdf/31uOeeexATE4MLLrgAALBp0ybce++9uP7664O8OiIKWz5noomhir0Z2zlt3kI0vXM+mM3iWjElBUDSR68hWjNUdA2+FehxBRDdtmn3owpiJZqvz4NBq9hI4Jo3gjuLTPn10IVhoVILbuds1LO56qqrMHHiRMycORMlJSUYOnQodDodCgsLsWTJEtxxxx3+XmeL1T01BjuknWAaVIlWR+BGRERE5AdPPfUUcnJycPHFF0OrFf7raLfbMXXqVM5EI6LGk8IywG0mmtTOGaSZaNJlZWWUXbGLpHLHRikcsHh5b9Zc7ZxNDdAA18Cvudbt/jihUImWOQzo/3eg0yig/aDgrkUZZuoigreOxuLGAq527tyJkSNHAgA+/fRTpKSkICcnB++++y5eeeUVvy6wpeueGgOTQ3jBcEg7k/ii/EfAagayfwJO7gjg6oiIiKi10+v1WLNmDQ4cOIAPPvgAn3/+OY4cOYJVq1ZBrw/izmVEFN6UM8+8biwQuN05PXitRNMpNhZQtHMqd2yUrvf2Pi6cggNVEHfnlD8PgRBNqweufhU4Z1KwV+IaQikr5MIFK9FcVVVVISYmBgCwfv16TJw4EWq1Gueddx5ycnL8usCWLqtNJOzibzNs5uravyHKSrTKM8B71wCGGOCh7ICukYiIiKhr167o2rUrrFYrampYEU9ETeQyEy3Yu3P6aOeUquWU7ZzKHRvl4fheOoqac3fOpgrqTDTp85YVtDSZ8uthjAveOhqrBW8s0KhKtC5duuDLL7/E8ePHsW7dOowdOxYAUFBQgNjYIO5gEYa0GjXSk+IBADZzA3bnLD0hvNhXF/tnXgARERGRwtq1a/Hee++5HHv66acRHR2N+Ph4jB07FmfPng3S6ogo7PmaiSbtzmlveiWa2VzPOdLSjpx2m3Ndyt05Hcp2TkUlmhQEWbxUonF3zjoe0y1Y0bSsoKXJ9FHAVa8CV/4TiEio+/xQI7cIq5pnPmAzatSzeeyxx3D//fejQ4cOGDJkCIYNGwZAqEobMGCAXxfYGnRNTwIAOLz10ispf5NSVei8XN8yZSIiIqJ6evHFF1FWViZ/vmXLFjz22GN49NFH8fHHH+P48eN48skng7hCIgpZNgtQVVz7OS4hmp8q0RwOYPdHQP4eAEBpRVX9bie9n1K+39LoXIMdKehTttbJLYiK+W7ydWEUorm0cwarEi0E2jlDzYC/AwOnBnsVjSP9HIXCrDs/a1SIdt111yE3Nxfbt2/HunXr5OMXX3wxXn75Zb8trrXo3l4I0TT2BmwsUK34zW99y5SJiIiI6mnv3r0YPny4/Pmnn36KSy65BAsWLMDEiRPx0ksv4auvvgriCokoZL1+AfB8R6DslO9z6gzRGlGJlr8H+OJ24N93AQDKKuuYOS2R3k8p329p9K7BjhSiuVSi1VI9FU4hmsvGAsGqRGt5YUurJu9g2/IqDBv9NyQ1NRUDBgzAqVOncPLkSQDAkCFD0KNHD78trrXok5kMANDBihpzLYGY8kVdiZVoRERE5Gfl5eVITEyUP//5559x0UUXyZ/37t0bp07V8gaZiFqvgn3Cx8MbfZ+jbNf0NhOtMSNrKs+IH4WunfKqeoZo0vsp5S6dyo0FAGfLZn1DtHBq5+RMNPI3FUM0F3a7HYsWLUJcXByysrKQmZmJ+Ph4PPnkk7BzPleDSTPRAGDf8TO+T/RVcWazej9ORERE1Ejp6enYv38/AKCiogK7d+/GiBEj5OuLiooQGRkZrOURUahSvh+sbSC6Q9ECqZyJ1pR2Tmn4v0Vo46yoqm87pxjoSUULKrUQ8igDAIuXSrTa5niFUyWaKggz0dxDxhYYtrRq0s9ROP09qKdG/aQuWLAAK1euxLPPPosRI0bA4XBg8+bNePzxx1FTU4Onn37a3+ts0VS6CPnynmMFGNg53fuJrEQjIiKiZnLddddh9uzZmD9/PtauXYvU1FScd9558vXbt29H9+7dg7hCIgpJNSXOy7WGaD7aOaVAqzHtnFK1mBimVVXXcydh93ZOKShTawCoADgUlWiK6rSWUonmsrGAqpkek+2cLVoLbuds1DNavXo13nrrLVx55ZXysX79+qFdu3aYNWsWQ7SG0uhghwZq2LDveIHv83xWojFEIyIiIv9auHAhTp06hXvuuQepqal4//33odE432h9+OGHuOKKK4K4QiIKScrZzbUFSQHYWKCqsgKRAGCpgsNuR2V1A9s5pfdVylloGp0Qrskz0ZQhWi3BTziFB0HZWMDt68ONBVoWuRItjP4e1FOjnlFxcbHX2Wc9evRAcXEdu7CQVw6tAbBW4cDJM3A4HFB5+w2Ar0o0hmhERETkZ5GRkXjvvfd8Xv/9998342qIKGwoQ7TaqsmU1ylbQOWZaA2vRDuWX4Re4uX3fz4ATY1QiWbVREBrEwM1ldozoJPG40jvq9yDMpu54TPR1M3UFukPqhDYWKAFtv21anIlWssLRxv1N6Rfv35YtmyZx/Fly5bhnHPOafKiWiO1zggAKK+oxKnSGtcZARK2cxIRERERUSirUhRV1FZN5vASnAHOcK0RlWglZaXy5VfW7UVZpTATzaGPdp4Ulex5w+qzwNblwNls4XNvc8/EOWuuAVstwU9YtXMGY2MB5WPqmq+NlJqHvLFAGP09qKdGVaI9//zzuOyyy7Bx40YMGzYMKpUKW7ZswfHjx7F27Vp/r7FVUGmFEM0ICw7s/wPtfpoC9JsCjBNbY+0237/JYSUaERERERGFAmUlWm2bznkLzlyOO4TCggaEK+Xl5fJlja0aUFkBLaAyRAPV4gZuMalARb7rDX9/H8jbDaT1E2/sZe6ZHKIpA7aW2M4ZhEq0cPpaUf204HbORv0NGTVqFA4ePIhrrrkGJSUlKC4uxsSJE/Hnn3/i7bff9vcaWwetAQBggBkdt8wDqoqArYpqv9qCskBUohX8BZSe8P/9EhERERFRy1XdhEo0926cuqrRvl8MLB8OVJcAACornbPVsmLV0EFo09QYY5y3iUlzXtaKG7yVi6Fa2Snho7e5Z2YvIVqt7ZxhVIGjDvJMNG4q0PJwYwFP6enpHhsI7N69G6tXr8aqVauavLBWR6xEM6gsyCj/3fP62oIyqYffX8rzgeVDhcuPl9Z+LhERERERkcSlnbO2mWiKgEyaf+Y+B81uqz2M+uMj4Owx4MQ2oOslqK5yhmgLx3fAzi+E+1MZYp23iUl1XjZEA9ZqoEZ8zyNV0XmrNpMq0epbQRVO7ZzBnonWAoOWVk/VckO0MJp22MKJlWjtVIXQwss/Ns1ZiXZ6r3/vj4iIiIiIWgeXjQV8VJK5t3n62pGzrko0sQIN1SU4W2mGylojX9UryYCJ/VKETwyKmWjKSjRpVpp0O7tYnKCuZztni9lYQBH4Nde6lWEdK9FaHunnKJwqMuup5cWC4UqsRLtCvwNQVjHbrMIwS1+bCgD+n4lmNfn3/oiIiCjsLFq0yOvxuLg4dO/eHWPHjoU6nN4kElHzULZz+tph0yMcE+efeYRotVSy2e3OCrKaEhwtrIRRpXjPZKlCpEa8P+XGAspKNOVxJW8z0by1c6pUQgDlbZ3hVIGjDvZMNIZoLY4UzLbAgDTof7OXL1+OF154AXl5eejduzeWLl2KkSNH1nm7zZs3Y9SoUejTpw9+//33wC800MRKtFGO7a7HTWVAZBtniKYxADa3kMvu53ZOxW9w6iyhJiIiohbpiy++8Hq8pKQEJ0+eRO/evbFu3TokJ3vZ6Y6IWq/6VKJ5O+6we4ZRtVWimcogVx9Ul+DomQpEQRmiVTuLDQzKmWhu7Zze1NbO6R4KqLWAzUuIFrbtnEGYicb3my1PC95YoEHPaOLEibVeX1JS0qAHX7NmDWbPno3ly5djxIgReP311zF+/Hjs27cPmZmZPm9XWlqKqVOn4uKLL8bp06cb9JghS6xE81BTCkQkOP8BEMM2lyAtkJVoditf1IiIiFqhXbt2+bwuLy8PN9xwA+bPn4+33nqrGVdFRCGvPjPRvIVjdpvncV+VbABQU+Jy+Wh1Jc5VhmjWaufYm4h44aNGD0QlOc9pSCWat3ZO6Vz3IgcgvN5DBWUmmuLr0wKrlVq9FryxQIP+hsTFxdX6JysrC1OnTq33/S1ZsgTTpk3D9OnT0bNnTyxduhQZGRlYsWJFrbe7/fbbccMNN2DYsGENWX5ok8IxdyW5wMu9gX/fKXyu0XmeW1urp9LhjcDPS2vfahoQfmsj33cAdv4kIiKisJaWloannnoK3333XbCXQkShxmV3Tof3c7yFaw57w2aiSfPQAKD6LI6eqYDRVyVaXAZw/hxg7FOAXlGVpo/yft/KUEeuRKv2vA7wHZaFU3igfA7NFf6xnbNlkzcWCKMwuZ4a9Df77bff9tsDm81m7NixAw8//LDL8bFjx2LLli21ruHIkSN4//338dRTT9X5OCaTCSaT8zcDZWVljV90ICkq0UwqA07Y2qCzOg/I/hEoOyn8AYTffKjUgPKXHfVp59y6HFg3T7jcYSTQfpDvc90r0YiIiIjctGvXDgUFBY26bUPHeWzatAlz587Fn3/+ifT0dDz44IOYOXOmyzklJSVYsGABPv/8c5w9exYdO3bESy+9hAkTJgAAHn/8cTzxxBMut0lJSUF+fn6jngMR+aAMt+o9E0085n5+bSGashKtugTZhZWIUPkI0TR6YMzjwuXSk85z6tPOKc9EqxSv89LO6U1zVXT5gyoYM9GUlWhhFDhS/bTgSrSgPaPCwkLYbDakpKS4HK/tPzOHDh3Cww8/jJ9++glabf2WvnjxYo//MIUknTNEq4nKgJz1FR9xPU+j8+xTr6taLGeLM0ADAFNp7ee7zERjiEZERESedu/ejQ4dOjT4dg0d55GdnY0JEyZgxowZeP/997F582bMmjULbdu2xbXXXgtA+OXsJZdcguTkZHz66ado3749jh8/jpiYGJf76t27NzZu3Ch/rtG0vN+QEwWVzSLOKhM1aCaazbNyrZ6VaI7qszhWVAWj2i1Ek9o5lcGXcj6a3vU1QuatSkquRHNr5/RVRRVOFThBn4nGSrQWhzPRAkelUrl87nA4PI4BgM1mww033IAnnngC3bp1q/f9z5s3D3PnzpU/LysrQ0ZGRuMXHCiKSrSIlM4oK8sDAJgLDsHlZVqjB9TuswLqCNEK9rl+XlfoJv2WBWCIRkRE1Er5qt4vLS3Ftm3bcN9992H69OkNvl/lOA8AWLp0KdatW4cVK1Zg8eLFHue/9tpryMzMxNKlSwEAPXv2xPbt2/Hiiy/KIdqqVatQXFyMLVu2QKcT3oxlZWV53JdWq0VqaqrHcSLyE+WmAkDDZqJ521ignjPRLJXFMFvtMBrcZqJJ73uUb+SNscCIewGofI/UcdlYQLytNPfMI0TzVYkWRiGay+6cnu/FA0L59WmBQUurxxDN/5KSkqDRaDyqzgoKCjyq0wCgvLwc27dvx65du3DXXXcBAOx2OxwOB7RaLdavX4+LLrrI43YGgwEGg48Xx1CieAHXJ3WC7mQ1UAOgONv1PI3e8zc0dYViyhlnQN0z1MwV9b9vIiIiapHi4+O9/mITEH4Jevvtt+PBBx9s0H02ZpzH1q1bMXbsWJdj48aNw8qVK2GxWKDT6fCf//wHw4YNw5133ol///vfaNu2LW644QY89NBDLtVmhw4dQnp6OgwGA4YOHYpnnnkGnTp18vq4YTMShCiUeIRovirRvMxK87axQD0r0exVwuNGq63yhp0e7ZxKlywSPm563vt9u7RzulVJtcSZaMpKtGDMROPGAi0P2zn9T6/XY9CgQdiwYQOuueYa+fiGDRtw1VVXeZwfGxuLPXv2uBxbvnw5vvvuO3z66afo2LFjwNccUMrdOdt0RGLiaeAkoLdVup7n7QWmrmoxc5Xr53WFaCZFiMZKNCIiolbp+++/93o8NjYWXbt2RXS0j1lCtWjMOI/8/Hyv51utVhQWFiItLQ1Hjx7Fd999hxtvvBFr167FoUOHcOedd8JqteKxxx4DAAwdOhTvvvsuunXrhtOnT+Opp57C8OHD8eeffyIxMdHjccNmJAhRKFHuzAn4riTzdtzh8BKi1a8STSOOq4lQmV1DNG/tnEq+3uArZ3S5n1NXqCafF66VaEGYidYCg5ZWT8UQLSDmzp2Lm266CYMHD8awYcPwxhtvIDc3Vx4UO2/ePJw8eRLvvvsu1Go1+vTp43L75ORkGI1Gj+NhSVlKnNABaSnHgJNezpM2FlCSfsNSUwb8/i/gnElAZBvn9Ra3IM5aVyVaufMyQzQiIqJWadSoUQG77/qO86jtfOVxu92O5ORkvPHGG9BoNBg0aBBOnTqFF154QQ7Rxo8fL9++b9++GDZsGDp37ozVq1e7jP6QhM1IEKJQUu0WojV0JlpDNhZQVKLp7DXQwwKDcvc1ZSWar5lbvgIwb+2c3q4DWkY7Z7BnorESreVhJVpgTJ48GUVFRVi0aBHy8vLQp0+f/2/vzuOjqO//gb9mz9z3DQHCfaMG5BLxRFFrrWjVelZti3gB9Vurtt+qv2/VtlapVbFVEVutovWoB1Xx4FBAuUEIdyABEkLuO3vN74/PzM7MHtlNCNls8no+Hnns7uzs7OwSsrPveR9Yvny5t4dFWVkZSkpKIrmL3cdsDKIlpaQFWS9AEE09w/LNImDNn4H1zwHzdVl7Hc1EY080IiIi0qmtrcXLL7+MoqIiSJKEUaNG4bbbbkNycnKHttPRdh4AkJOTE3B9i8XizSDLzc2F1Wo1lG6OGjUK5eXlcDgcsNl8vvQCiI+Px7hx47Bv376Azxs1LUGIepKwyzmV5ZIZgJKBpv7ohdkTDQCS0QirJ0gQLWi2WBhBtJDlnEG+Ups4nbNdhkw0BtF6nV6ciRbx/9nz5s3DoUOH0NbWhk2bNuHss8/23rd06VKsXLky6GMffvhhbN269dTvZHdw6gJdKQMAe5CDUrPVvwGmWwl0HfpGXNaWAE2Vum13sCdaG3uiERERkbBx40YMGTIETz/9NKqrq1FZWYmnn34aQ4YMwebNmzu0LX07D70VK1Zg2rRpAR8zdepUv/U/++wzTJw40TtEYPr06di/fz88Hu0L+N69e5GbmxswgAaInmdFRUXIzc3t0Gsgona4Wo23gwXBvEE0kxa0CdgTLUDvNIWz0Riwy5LqfPYljHJO3ywzVXtZUuFmokVT8MAQ0IrEdM4oytqj8KgZ5MH+j0WxiAfRSNFQpl23xgIxQYJoJqsxaw3QgmIpuhKDbW9o133LOTsyWCDU5E8iIiLq1RYsWIDLL78chw4dwrvvvov33nsPxcXFuOyyyzB//vwOb2/hwoV46aWXsGTJEhQVFWHBggV+7Txuuukm7/pz587F4cOHsXDhQhQVFWHJkiV4+eWXcd9993nXueOOO1BVVYV7770Xe/fuxccff4zHHnsMd955p3ed++67D6tWrUJxcTG+/fZbXHXVVaivr8fNN9/c+TeHiIx8g15BM9GU4Jo+iBYoE62dnmiOJmPpaLbkU0oaTjlnWJlovuWcYWaiRVU5ZyQy0VjO2auxnJNOuaT+xtsxSYHXM9v8M9HUQJc+fXrTq8DUu0QEuMODBfQ90dpJoSYiIqJeb+PGjXjxxRdhsWiHjRaLBb/61a8wceLEDm+vo+08CgoKsHz5cixYsADPPfcc8vLy8Mwzz2DOnDnedfLz8/HZZ59hwYIFGD9+PPr164d7770X999/v3edI0eO4LrrrkNlZSUyMzMxZcoUrF+/3vu8RNQF/HqahchEM5m1wFvAIFrwnmie5lrD7cH2Bm2oAOBTzhlsgEAXB9FMFq0dTjRlVxl6onVTEM0wEZRBtF6nF5dz9r5XFK2m3CHq+kcrk0mDZaKZrUBsqrhujRdZZu4AQbSqfUBdqSgNVUtFbYliaECoEk19TzSWcxIREfVpSUlJKCkpwciRIw3LS0tLkZiY2Kltzps3D/PmzQt439KlS/2WzZw5M2Tp6NSpU7F+/fqg97/55psd2kci6gTfoFnInmgmeCNfcoByznZO6JsdonzzhJSOTLkK45ObgVrdCobpnIHLuoNnonWgnFN/vzUOaKtXth1FX7VNEQiiMROtd+vFmWgs5+wp7AnARb8H8s9UbreTiXbWAmDW/wGn/UQsU892+DbyVPuiqUE0NfjmakO7DOWcHCxARETUl11zzTW47bbbsGzZMpSWluLIkSN48803cfvtt+O6666L9O4RUU/iG/QK2hNNCZxJJu3LtiyHP51TlmF3ieoZd7LIJp01wKeUNKzpnMEy1NobLODbE02XcWaN1a53VzCqK0iR7onW+wItfZ43Ey2KMjLDxN/Wnqq9cs6UfGDa3cBXj4tl3ky0Wm0dtwNorhK31XLO2GSgDu2Xc7pdxoag7IlGRETUpz355JOQJAk33XQTXC5xcs1qteKOO+7AE088EeG9I6IeJdxyTG8mmqQtCzhYIEgQrq0BZijrphUAtZsR03zcuI4rjHLOsHqi+QQB2ivntMQEf1xPZopwTzQG0XqfgdOALa8BAwIPDYpm/G3tqYKUc1a2yshQb6gfBh4n4PFoY57ThwEVO7UgmjpYICZFXLZXouloMN5mTzQiIqI+zWaz4S9/+Qsef/xxHDhwALIsY+jQoYiLi4v0rhFRTxNuOaf6HUOfASV7wnq8LMtwNVXDCqBVtiIuVZmw21BuXDGccs5gZYTtlRr6Bt5MPuWcqqgaLKAv5+yuTDTd87Ccs/cZfTkw8jJjqXAvwSBaT2VLBCABkIHEPKDhGABgQ0kDLpZlSJKk/cF2u0Ttvfohkz7YGETzZqKlKOu3U87Z1mi8zZ5oREREBCAuLg7jxo2L9G4QUU/m6Wgmmu4LdsCeaP6Pv/fNrdi/fS2W24F6xCMjNUvcoXxfgjVOtLNxNocxnTNIwKjT5Zz6TLQo+qodkemc+hLSKHqvKHy9MIAGMIjWc5lMgD1RBMcyhno/FErr3Xhn81FcVdhfi9h7nFo/NGsckNRPXPf2RGsRl95MtHbKOfVDBQD2RCMiIuqjrrzyyrDWe/fdd0/xnhBR1Ah3MIA+iKaWdAaczml8vMvtwQfbjmGqSXxnaTIlIEtNFFATCGLTRADN0QTv0IKgGWdhlHP6DRZo57ZF1xMtWss5I9ETjZloFEUYROvJYpJFEC19GFC8GgDggQmPLy/CBaOykOLNRNMF0WJTgbh0cb25SjTodPpmorVXzumTicYgGhERUZ+UnBxkUjgRUTCdmc6pD6KFyGQ7XC2+1yRBBNEscana8DRVXCpQf8T4PSZYkMYwWTNea4OjX+7XE803E033lTpqBwvoyzml4Ot1JUNPNAbRKHowiNaTqRM604d6F+XEelDV4MCfPt2D3+cr/3x+QbQ0cb25SslCU87AqJlo7U3nbPPticYgGhERUV/0yiuvRHoXiCja+E3XDJGJZjJDtLBB4MECPtvbd1yc8B+VJgGNQP/cbCApz/gY36Aa0E45p255fAZQGyiIFqqcM0gQLZoy0SLRE01iTzSKTlEUHu+DEjLFZcoA76LpA0Sd/b++K0FJnZJR5vENoimjB5qrtCw0QBtW0JFMNPZEIyIiIiKicISdiaYOFjBpARxZDjndc3+FOOHfL1E8RrLEAGmDjY+JTfN/vqCZaLoAWEKWbnl75Zw+eShBg2hRlK8SkemcESghJeoCDKL1ZBc8Apz7G2DYLO+iTLsLV57RD7IMvL1FmUBjyERL0co5myq1IJrZrk2L6VBPNAbRiIiIiIgoDGH3RFMqZSRJV87pDhCE88lEqxAn/HPilcdY7CKJwK4rP7fFi+8+KskUPEhjyETLCrzcNxgWdjlnFAWGpAj3RGM5J0URBtF6srzTgJn/A1h0f6gdzXjwklFIirGguFrNRHMBLbXiemyqSEUGRCaaOpnTFqf9wW8viOZXzhnkg4+IiIiIiEjPr5xTDryevieaGrQJMFjA5XZj7f5KtDrFdvcrQbTsOCWIZraLIFzaIO1BlhifjLB2AjT6LDO1Csh3eUeCaL1hsEAkMtFYzklRhEG0aONoQkaCHb+6eCScEH94HI62wIMFWmq0oJg1Xvvj1G4mGss5iYiIiIioE0JM1/RbT1/OGaAn2qrd5fjJS9/ir1/ug9sje4NomWqsyqJknKUWaA+yxhqDaO0FaPQBsPgwyzn9gmr64QRRWs4ZiZ5ohky0KHqvqM9jEC1anPkLcXneQwCA684cgPx0MXjgWHWDMYjmbaYpA/VHxVVrrPYh024mGqdzEhERERFRJ4Rbzqkul8xa0Eb2+K1fWiW+m2wrrcPRmha0uTywWUxItioZbur3m7ROBtH098UHy0TTXTfb/KdX6jOq1PY5QBRP5+yuTDRd4IyZaBRFouh/dh83+w/Ar0uAQWcBAMwmCddPE1M7G5tbUFWp9EeLTRV/hNRJnHWl4tIWFzgTze0TJPPNRGNPNCIiIiIiCodfOWewwQIBMtFkt1/5Z1VDCwBRxrlPGSowOCMeJnerWEHNGNNnollijGWVgaZ1qvQBsthULaCnD+qYQ/TuMvREi9Etj9JyTlMkBgswE42iB4No0UKStOmaioJscdsCN0qPHhML1Q8JtaSzVgmiWeO1DxmXEkRrrACeHAZ8eK+2Ub8gGnuiERERERFRGNTyTTUoEk45Zzs90aoaRbCsvL4VGw+Lypvh2YlaUoBFCVr5ZqLpe0oX3hJ8f/XBG1scYE8Q1/WDCUJlTOkDa5ZeMFiguzLRDMMMmIlG0YNBtGim/LFJsHgQ7xFnZhxWJdCmDheoLRGXtjjtw0D90CnfDrRUA8VrtG36DhZgTzQiIiIiIgqHegJeDYqElYkmact8gm51TW3e6+9vEW1qxvdPBlxKJpolQCaa2QYc26Ldnnhr8P3VZ5lZ44CzFgJjrwIyhmvLfcs5fQXNRIui7KpI90QzR9F7RX0eg2jRTDkTkpNgRqpJZJA9t64SsixrmWhqOadVX86pBMbU/mcu7cMJrfXiMjZNXLInGhERERERhUMNjqnfOzwhgmgmkxa08fhnoulvl9WJwNnpA1K1yho1Ey2pn/aY5ipgyPni+tirAHti8P3VB8isccBZ84GrXjaWNJpDBdF0QaeYZPF6LDHRFUSLyHTOEGWyRD1UFP3PJj/KHx4L3EgzNQMe4O1dTZBX7MWCuHRIgK6cM077o+9WgmZq6aZbF0RrU4JocekiS4090YiIiIiIKBze4JjFeDvYeoaeaP6DBcwwPt5qljAmLwn4Tvn+olba6INezmbg0ieBA18BZ9zU/v7qA2S2uMDrhCrnNPv0VfvRC0oQLYryVQyllZHIRGMQjaIHg2jRTA2KtdbDpAS76hCPZ77cj7G5LswCgLY6sY4tzn86p6NJXBoy0ZT149KBqn3+gweIiIiIiIgCUYNg6veUcHqiGQYLGINmkk8QbXRuEmKsZu37i7732QUPA5teBc78BZDcD0gbHHp/fTPRAq4TqieaxXh9/I9DP29PY4rEdE7d80RT1h71eVEUHic/6h9xh9bH7H+vnAib2YTvKnz+aQOWcyqPU3sKAMYgGsByTiIiIiIiCo9vOWc4mWjtDBYww4OB6Vpw67T8FHHFG0TT9SA7awFw71YRQAuXyQTYk8V+BJviGbKc0xL4ejSJdE+0aH3fqE/ib2s08/1jY0vANWcOQkWDE1u/yPG5Tz+d07ec0yHGSUuS1hMtTu2JxnJOIiIiIiIKg+90Tk8HMtE8/ploJknGuSOysHTtIQBKPzRA+z4TKKjVUdf8UyQSqN9/fBkGC4TKRIuiiZx6humcUvc8J8s5KUoxEy2a+f6xsYmRzLeeVYDjMUOM91ljtZ4Bslt8SKnlnID4IHK2av3RmIlGREREREQd4S3nDJGJpq4nmbUAToBMNBM8mFyQhrR4G8wmCYUDlSCa+p1FbVdzMgbPBEZfHvx+/eTI3pqJZopATzRDHzYG0Sh6ROn/cgLg/8fGLoJo8XYL5pw3FQ0rYpEotYj7rPHGoJvbqU3nBERJp9orDZKWzsyeaEREREREFA5vJprVeNtvPX0mmqQt8wuiyRiQHodXbpmE+lYn8tOU0k5XFwbRQgk1RbI3BNGkSPRE0wXRzFH6vlGfxN/WaOaXiRbvvXrj1EHYv3IgRrp2AwBkaywk/ZkTd5tWzgmIDyJ1Mqc9UTvLwkw0IiIiIiIKhyyLSzUoot72Wy9IT7QA0znz0+KQFOPzvcflM53zVApVzqlfFq0ZVYZyTvZEI2oPyzmjmV9PtETvVYvZhNzhhd7bO6vcxvRjt9MYRHO3af3QYpK1DwP2RCMiIiIionD4TucM2RNN0k3n9M9Ei7NK/gE0QFfOGeN/X1cLWc4ZgVLIrqbf727LRAuR4UfUQzGIFs18z4Qo5Zyq5IGnea9/tLNWTJ9R/1i5HT7lnG1Aa62ynSTtD2mwDz4iIiIiIiI9v3LOYNM5lQw1k64nWoDBAmmxQTKUvOWcXTBYIJQODRaI0owqfeCsuwKBhnJOBtEoejCIFs18I/a6ck4AQPZo79VtFQ58V1ytnT1xO3wGC7Rq5Zwxydq23cxEIyIiIiKiMKhBMG85Z7BMNHWwgMknE824fkpskIBOt5ZzhspEswZeN5pEpCdaLwg+Up/EIFo0CzKd0ytLC6JJkDH3tU1wqG3wXA7A0aCt63KI0c6AEkRTx1J3sieaLAO7PwZqDnXu8URERERE1LOEqlLxhJuJph8soAbRAmWiBQmiqQPRumOwgLkPZKJFpJyTmWgUnRhEi2aSZPxDbU803h+X5r1qTR+M6iYH6hzK9Bu/cs5WXU+0pPB7ojVXAx/cA5RuMC4/uhl48yfAf+7qwAsiIiIiIqIeafvbwBMDgP1fBF9H7mhPNBOONzq1ZT6DCJKDlnO2isvuns4ZMIjWC4JBUqR7okVp8JH6JAbRop0+fdg3Ew0A5n8P/OwrvDR/Dm6cMtCbidba1upfzhkoE80dIhNt90fA5leBtc8Yl9ceFpf1xzrwYoiIiIiIqEc6tEYMJiv9Lvg6Ht9yzvYz0VpdMjaXiu8gR6oa/YJuyfYAX1c9bq1aplsGC+gz0QKUcxqmc/aCwQLd9Rr0gTsOFqAowiBatNP/0fbtiQYAKflAvzNgNZvw4CWj4DGJP/xLv/pem2oDKJlpSiaaPSn8ck41m00fkAO0IQW+y4mIiIiIKPqoWWbtfT9Qg2ahyjmVYFmLC3ArX0nf2liCFoexCiY5UDmnS/cdJlBQq6uF7InWCzKqItETzWIH4tLFd097gGQQoh4qSv+Xk5ehnLP9Pz6xNjNSE+OBemDr3mJA/xngm4kWbjmnq0Vcuh3iw/D7d4EBk7VtMYhGRERERBT91CyzdoNoHSvndHgAGaLdTH1zG5Z9dwi36FYLmImmTwTolnLOPtATzRBE68bpnLetEL8L3fHvSNRFovR/OXkZMtESg6+nSIiLBeqB/vYWQN9ywNVm7ImmpvGGykRTzwS5WoGDXwHv3g6MuATIGC6WOxpFbwNJCu/1EBERERFRz6N+L2jv+4EaNAuznNPh1jLR4q0SjtU0AbqvN3HWAN8hXMpQAcnUPUErkzL8QPYELjvsDUG0SAwWAID0Id33XERdhOWc0c4UopzTlzIG+qdnJBuXu9p8eqIp2w3VE01t6ulqAxpPiOs1h7VtQQacLaH3i4iIiIiIei5vOWc7Ezr9yjmDZaKJs/ltbsCjfCW9/sz+sJuNQTMpUBBO/f5htnffiXr19bRbzilFb080KQI90YiiFINo0c4cfjmnWF/84e9nazYsrqqr71xPNG8mWpv2gdZcpfVEA1jSSURERAbPP/88CgoKEBMTg8LCQqxZs6bd9VetWoXCwkLExMRg8ODBeOGFF/zWqa2txZ133onc3FzExMRg1KhRWL58+Uk9LxHphJOJ5i3nDNETTVmuD6LlJdkweZDPif5AATu3kolm6YZ+aCr1u1Ggck51WbRmoQE+5ZysICJqD4No0U5/NiTQdE6/9ZU/8i21hsX/WLMHruYacSMmJfyeaGqWmbtNC6g1Vxm372gMvV9ERETUJyxbtgzz58/HQw89hC1btmDGjBmYPXs2SkpKAq5fXFyMSy65BDNmzMCWLVvw4IMP4p577sE777zjXcfhcODCCy/EoUOH8O9//xt79uzBiy++iH79+nX6eYnIhyeMwQLe6ZyheqKJ5W0uGR5ZCdrIHkwcmOKzXjuZaN0xmVOlJi60l4kWzUE0QzknM9GI2sMgWrTTl3PaQ/dE8zZtbKk2LHY7WuFoqhU3DD3R2knXBgJnonmcQN0RbR1mohEREZHiqaeewm233Ybbb78do0aNwqJFi5Cfn4/FixcHXP+FF17AgAEDsGjRIowaNQq33347br31Vjz55JPedZYsWYLq6mq8//77mD59OgYOHIizzjoLEyZM6PTzEpGPcIJoaiaaGlCS5SDrieBYqxvwQA2iuRHjU84ZsBxU7Ylm7sZm9O2Vc8ZniH1J7t99+9PVpAj1RCOKQvwfEu305Zxh9URT/vA3VxkWx5mciHErJZ6GnmihpnPqeqLpx03XHNKuM4hGREREEBljmzZtwqxZswzLZ82ahbVr1wZ8zLp16/zWv+iii7Bx40Y4neI45YMPPsDUqVNx5513Ijs7G2PHjsVjjz0Gt9vd6edta2tDfX294YeoT+tITzRzqJ5oShDNJXvLOSHL/usHykRTp3N250RH9fWYA2SbxaYCd3wD3PJx9+1PVzPpwgLsiUbULgbRop1hsEBHyjlrDIvPzHTDJIkzRRe9sBW7K5Qyzc70RAOMZaAs5yQiIiIAlZWVcLvdyM7ONizPzs5GeXl5wMeUl5cHXN/lcqGyshIAcPDgQfz73/+G2+3G8uXL8Zvf/AZ//vOf8fvf/77Tz/v4448jOTnZ+5Ofn9+p10zUa3RkOqcpvJ5ohkw0j9t/fU975ZzdmYnWTjknAGQMAxKzA98XDQw90RgiIGoP/4dEO3PnpnOi2VjOOT5VpEW3yVbsqXTitx/tFneE6ommfoi5fTLR9JiJRkRERDqST+NqWZb9loVaX7/c4/EgKysLf//731FYWIhrr70WDz30kF+pZkee94EHHkBdXZ33p7S0NLwXR9RbdaSc09tfOUgmmrK8xSXD7c1E8/gH0RqPA1/+Hqg5rC3zlnNGYrBANz5nd5LYE40oXFHc/ZAAaH/QrXHhpd6qH2g+5Zy2VnEm1xqfjCFJ8TheeRywA7LbhXbns3j7oLkAZ5BgWaSDaG5X4NRrIiIi6lYZGRkwm81+2V8VFRV+WWKqnJycgOtbLBakp6cDAHJzc2G1WmE2a8dCo0aNQnl5ORwOR6ee1263w27vxkwXop5ODXC1G0TzLecMlokmAuFthnJOj3/Q7ft/i8vNrwL37RXXIzJYwGq87G1M7IlGFC7+D4l26h/ycEo5Ad3ZE6XJp5pq3XhC3IxNwQs3FMJqFeu5XUomWm0p8N/7gepi4/b0JZytQXqFRLKcc/vbwBP5wL4VkdsHIiIiAgDYbDYUFhZixQrj5/KKFSswbdq0gI+ZOnWq3/qfffYZJk6cCKtVHMdMnz4d+/fvh0dX+rV3717k5ubCZrN16nmJyIe3nLOdnmjq/0FTeD3RZJgge4NoAco5VY3HtetuJRPN0p2ZaFbjZW+jzz4zMURA1B7+D4l26h9ye5hBNN/eAXHiDK73g8mehGHZififS8YCACSPE1/uPi7O/nz7ArDhJePj9SWcrXWBnzOSmWiHvwGczUDJusjtAxEREXktXLgQL730EpYsWYKioiIsWLAAJSUlmDt3LgBRRnnTTTd51587dy4OHz6MhQsXoqioCEuWLMHLL7+M++67z7vOHXfcgaqqKtx7773Yu3cvPv74Yzz22GO48847w35eIgqhQ+Wc6nTO9nuieSDBZtWtG2x9QJv0qX7/6NZMtN5ezmkyXhJRUKxxi3YdzkTzOXsSlw40lmtTbmKSAQAXj+sPfAKYJRkL3tyMr8dXIhHwKwM1ZKK1hchEk2XRiy0+Pbx97QrqdNFg/dqIiIioW11zzTWoqqrCo48+irKyMowdOxbLly/HwIEDAQBlZWUoKSnxrl9QUIDly5djwYIFeO6555CXl4dnnnkGc+bM8a6Tn5+Pzz77DAsWLMD48ePRr18/3Hvvvbj//vvDfl4iCqEzgwUCDQYAvMEyN0yItVoABwIPFtBrqgQSMrXvH90Z0DrjZmDHv4EBU7rvObuTWs7JfmhEITGIFu06Xc6p8A1oxSSJS5P2q9Hc6sD6PaW4EPDPNutIJtqX/weseRK4+UOg4Ozw9vdkqcFBd4gBCURERNRt5s2bh3nz5gW8b+nSpX7LZs6cic2bN7e7zalTp2L9+vWdfl4iCkEOJxMt3J5oYlsyJNitVhFEk2Vtfcnk/9jKPSKI5i3n7MaehZNuEz+9FTPRiMLG/yXRrqPlnOYg5ZwqJRNNH0TLiDWhrblB3GipNa7vbNGuh+qJVrZNXB7ZGN6+dgX1Q9bNTDQiIiIiok7zlnO20xPNW85pM972W08t5zTBbjVr6/pmsulV+g4W4OCPLhObKv7NEgMPWiEiDYNo0e5kyzkzhhtvq0E03XoPXDwUcRBBqINHjuLHL6xDQ2uAMslQmWjOZnHZWBHevnYFbzmno/uek4iIiIiotwmnJ5o3CGYx3val64lmt9m0Zb6ZbHqV+8SlelzvmxxAnRebAtz+OXDTfyK9J0Q9HoNo0U79gLLFh7e+vpzTGgf0m2i83+6fifaDsVnIjRMfaHZ3I747VI0H3/sesiwbe6K5WmCgPpcaRFMv9dN1nK2nttSSmWhERERERCcvrHJOpfl/yHJOrSdajC3AYAFTgN5caiaaelzPTLSulTsBSBsc6b0g6vEYRIt26geUPTG89fWjoPtP9C8D9ZZzmgFIAADJ48KwFHE9y9ICi0nCh9uO4d2Nh4OnaANAUp64DJaJ5nIAz00CXjxP+8DtamoQjYMFiIiIiIg6L5zBAuGWcyoDB7w90QBlsEA75Zwn1HJOBtGIKHIYRIt2qYPEZfrQ8NbXZ6LlT/FPg1YHCwC6NGwXLG6RZWZ1N2PB+QUAgFdXF7X/XEn9xaXaE82hBNGaKrTL2hKgfLsW7Opqarr3qdo+EREREVFfEE5PNN9yzmAnynXlnCnxMdqy9so560rE9wk1iMZyTiKKAAbRot2UecAda4HCn4a3vj6INmCK/xkcNRMN0D68PE4tAAbgpjPSYLeYcOREbfvPldxPXHrLOZVgmlrOqS73vd6VmIlGRERERHTywumJ5s1E02WXBdDiEO1cPDAhLy1ee6wadAuUiQYAjeXMRCOiiGIQLdqZzED2GMAU5j9lc5V2vf8kwBJjvN8eIBPN7QKcWpAr0dOI2WNzYEeIXmbByjlb60QvNDWoBhivdyW139qp7LtGRERERNTbhdUTTe1p1n5PtIp6UeWSGm9HrE1dV9aCbmZLwMfB1caeaEQUUQyi9TXxWdr1mCRjjzTAmImmK+fUZ6KhtQ4/npgPuxSkRDJrjLjMO11cOhpFIE5fUtlUEToTra0BWPMUUH2w/dfUXj81DhYgIiIiIjp54fRE8/hkogXpiXZCCaLlpsQDkll7rG8QzperVVfOaQu8DhHRKRQkxE+91ugfijTogrPFbd9MNH1PNPXDz9VqDEK11mHK4HSMzrQD9QGe45p/ig9XddiBo8mQyQZADBcIFUT7/h3gi0eAqv3AFc8Hfj0tNcDfZgIjLwUuftz/fnW/XeyJRkRERETUaaF6oskygPCmc1Y2iBP0eanxgGTS1m2vJxogqlm85ZwxgdchIjqFmInW11hswLS7xQhjoP2eaGomWptPpKy1FiaThKeuHB7wKQ63xgGZIwCb0t/A4xLBLr3G4z5BtADlnE2V4lJfguqrbBtQexjY/VHg+73lnMxEIyIiIiLqtFDlnPqAmSl4T7SmNhfqmsSxeV6aPoimn87pk+uh3taf3Gc5JxFFAINofZ1hqo0E2BK1m+qHVWud8THK7RgE/gB9+L/7IcsyYI3XFjZWGFdqrPDpidYMP2oPNWeA+3wfF+jxAAcLEBERERF1hVCDBfQBM28mmezXeuXdzUe8y5JjbaLHM9B+JlpMirh0tXGwABFFFINofZ3+w8eeZBxQECyI1lIrLl2tATf51f46fLWnQjQEVdOsAwbRQpRzhgqQAaEDbd6eaCznJCIiIiLqtFBBNDlQEA1KcEwGNi1FY8k2PP35PpigBMsks7GcU30O355oarWMq0XXE41BNCLqfgyi9XWSpH0A6fuhAdqHX6tvOacSVAuQ3eWU7AAk/N/HRXC6PVpJZ5NvEC2Mck61j5qzJfj+64NongA9F1zdHERb+yyw8onueS4iIiIiou7iHSwQpCeafrnJJ4hW+h3w4b2oefseVDc5kByjfA2VTFoQzePWstZ8p3N6g2ht2nG974A0IqJuwCAaadlo+n5oQMhyzkCZaBZbDNLjbTh4ogmvrT+sBdEaTxhXbDzuU87ZTiZaOOWcwdbzlnN2QxDN2QJ89hCw8nGg4fipfz4iIiIiou7SkZ5o+kw0j9vb49jdLPokj8pJEPfpg2iyrJvOGSyI1qp9B+FgASKKAAbRSAui2X0y0doZLAAgYBBNssZg4SwxcGDR5/vgtiofkI0+QaVwyjnD6Ymmn/rpu55H15y0OwYL6EtWPc5T/3xERERERN1BH+AKp5xTHwSTPd7vDR6XOEZOiVXul0y6nmj6wQK+5ZzK9xRnq3ZynOWcRBQBDKKRdhanCzLRYLHjmon5GJGdiLoWJw43SACA7Xv2ifttuqBaqHJORxjlnPpMNN9AnFsXyHI7/Jqadjl9EI2DDIiIiIiot9CXagYdLKDPRNOVWspu77GxSXbDJAFxVvEdwZiJFmSwgMmqDSwzZKIxiEZE3Y9BNNKVcwbriRYsiBYgUGSJgcVswv/+YDQAoLRJnFly1x0DADTE9hPrNZ0IXc4ZViZaO+Wcvtlnp7ovmr7vGwcZEBEREVFvoQ+cye7AJ6d15Zyyml2mLleOyy2SG/1T42CG8niTWQwXAJQqkgDlnJYY7fuKq0075rfFncwrIiLqFAbRSDdYIEQmmjpaur3pnMoH3PShGXjllklosojH5JsqAQCb6pRMNGcz0FytPa69nmgeV/CeZu2VhLp9SipPdXaYvmQ1yORSIiIiIqKoI/sMEwg0XEBZR5ZM+NHi9cZ1leNwC9wYlBGvbU+SAk/n1GeiWeyANVZcd7VoQTQrg2hE1P0YRKMwBgvUisskJYtMDao5lUCRpDvTpGvwee7ILFxQOAoAkAGxjaOuFLjVX7vaUu1xalbasa3ACzOAgyvb73cWaLlfEM3R/u2uxnJOIiIiIuqNfINmgUo6lXVcsglbjzZoy2WPIYg2OCNey2Tz64mmZqLpg2i6TLTWOm0dBtGIKAIYRCMt8BVssECrMlggKVe57dMTTR9885mSY0vKMtxuQgxqZCUbrf6odocaDPv7TKB8O/D+naEnbwLtr8MgGhERERHRyfMNmgUKoinZZW5ZQkaC9p3gaE2T99jYChcGpcdpgTDJJLLRAGV4ga7MU2Wxa98xWmq05QyiEVEEMIhG4fdES8oTl+42YNNSoKXa/3G+DT7jMww3+2elo1YNounTwh1NQJvujJXZ4tPvLMhwAX22msM3iBbJck4G0YiIiIiol9D1OwMQJIgm1vHAhGsm9fdWnzz1yS64HOJY3gwPCjITdEE0s89ggUDlnDFaEE1tB2O2ie8LRETdjEE0AiZcC+ROAIacb1yuZqK1KZlo8VlaX7QP7wU2/0NcbycTDXHGINqUkfmoRaLfLpSdqETr9x9pC5LzjeWZ4WSi+U749A1kdWcmmu9QAyIiIiKiaOWXiRagJ5qyzA0ThmQmQFK+S3yz/wQ+3HwIAGBVyznVx0umIIMF9EE0my4TTQmiqT3SiIi6GYNoJIJov1gNpOQbl6tBNPXDzBYP3PwhMHaOcT1DEK39TLS0lFT079ffbxcczQ0oXvVPbUHjcQCyfoXA+97udE6foNmpzg5rYjknEREREfVC4fRE82aiSRicmQCTSXzVTIkxo65BVJyY4UZeSqxPOac+E01Z7peJpnzHaFbKOa3xJ/2SiIg6I+JBtOeffx4FBQWIiYlBYWEh1qxZE3Tdd999FxdeeCEyMzORlJSEqVOn4tNPP+3Gve1j9B9egAii5Y4HptxpXN6BTDTY4pGTk+v3VClSI4bUfastaCg3rhA0E60D5ZzhZKLJMlD+PdDWGHpd38cZeqJxOicRERER9RJh9ERrahXH2m6YMDgz3pth9uJNZyBDSRyzSB6YJRiDaPrBAmqwzqQr1TT0RFMy0Wzsh0ZEkRHRINqyZcswf/58PPTQQ9iyZQtmzJiB2bNno6SkJOD6q1evxoUXXojly5dj06ZNOPfcc/GDH/wAW7Zs6eY97yNMPn0GbMoZn/TBxuX2djLR4tKMt61xQKzPMgDJUjNsku7DWC0hVYUzndMZYjpnONlhRzYCL0wHPrwn9Lp6jkbjvjATjYiIiIh6Czl0JtqRKuUktGRCUozVm2GWn2zDrOEpxsd2NBPNqgTR1ONtlnMSUYRENIj21FNP4bbbbsPtt9+OUaNGYdGiRcjPz8fixYsDrr9o0SL86le/wqRJkzBs2DA89thjGDZsGD788MNu3vM+wjeIpk7AiU01Zpi1l4kWk6L1OQBEIM43sKbTgpjAdwQbLGDoieYbRPPtiRZGYKtqn7g8vjP0unr6LDSAQTQiIiIi6j08oQcLHKkRJZuSmllmUoNjMmzQVYi4nbreZ7490dRMNH0Qze7/HYPlnEQUIRELojkcDmzatAmzZs0yLJ81axbWrl0b1jY8Hg8aGhqQlhY8KNPW1ob6+nrDD4UpIdt426b7sEofql1vryeayQTEpWu3rXHG2yZjyeguj09fNkVzU4B/N1k2Zn+FLOf0uQ0ARR8BXz+tjdNWyzj1kzbD4RdEYzknEREREfUSYQwWOFYtTmib1KmZ3gwzt/EEs18mmqSsJ2vH5PrJm/qeaCpmohFRhEQsiFZZWQm3243sbGOgJjs7G+Xl5UEeZfTnP/8ZTU1N+PGPfxx0nccffxzJycnen/z8wEEaCiB/svG2Vdd7IH2Idr29TDTAOFzA5lPOGZemfcACcKcOgRO6zDXFkx9txe2vbsRVi9fiR89/g+omhyjX1KeWh1PO6fuB/9EC4POHgeqD4rZaRtpS07FsMt+gW6D+a20NwIfzgUNfh79dIiIiIqJIC6Oc81iNOKFtMivH8mqGmewxnmD2DaKZdOsFnM5pByw+QTMbM9GIKDIiPlhAUs88KGRZ9lsWyBtvvIGHH34Yy5YtQ1ZWVtD1HnjgAdTV1Xl/SktLT3qf+4z8SQB0/xa2YEG0JO2671kiwCcTzaec05YgfhRnnjbB2GNNYZfb8HnRcWw8XIMtJbVYtqHUv3wzVCZa0QfAY/2A7W+L2x4P0Fwprrcok34cuoECTSf8X0sw4WSifXgvsOkV4LU5/vcREREREfVUYQwWOFYjjqMtvploHjfg0p1gDtoTTTdYQN8TzWxnJhoR9RgRC6JlZGTAbDb7ZZ1VVFT4Zaf5WrZsGW677Ta89dZbuOCCC9pd1263IykpyfBDYYpJBrJGa7et4ZRzBshE0wfRfDPRbPHGM0nJ/WGNT/HbxG1TsnHP+cNwdWF/AMA7m49A9gui+dz2zSTb/zngagH2r1DWb9Q+wNXHtjVo6zd0oKSzyTeIFiAT7ft3lPtY6klEREREUcS3miNAEK2qQfQwNqtBNFOQTDR9TzTJHHiwQLDpnCorp3MSUWRELIhms9lQWFiIFStWGJavWLEC06ZNC/q4N954A7fccgv+9a9/4dJLLz3Vu0kDdCWdhkw0XRDNHiITzVDOGe8TVEvwC6IZgnKKDJsbC88fgt/+YDTsFhP2VzRiT6lPkCtUOaeabVZ3VFzqJ4CqvdXadJloHemL1lJrvO0bKGvUZbXphzIQEREREfV0fkE0422n24M2h6gCMXvLOXUZZm6fnmjq4yXfwQKBgmiBeqIxiEZEkRHRcs6FCxfipZdewpIlS1BUVIQFCxagpKQEc+fOBSBKMW+66Sbv+m+88QZuuukm/PnPf8aUKVNQXl6O8vJy1NXVReol9H79J2nX9ZloaYMDrx8wE00XNLLGi+meKr9MtHxjUE51dBPwxEAkfbcIF43JAQB8sb3YuE6ock5VnVLS26r7vQmUidaRIJpaBhqTIi59s+D2LNeutzOdlIiIiIioxwnRE62uxQkTxFCAwD3R2hssoM9EC1DOaYnxL9+0MYhGRJER0SDaNddcg0WLFuHRRx/FaaedhtWrV2P58uUYOHAgAKCsrAwlJSXe9f/2t7/B5XLhzjvvRG5urvfn3nvvjdRL6P36TdSu64Nd1lhgyHlAYh7Q7wztw6+9TDSTBbDYxLQdte+ZLd7QEw1J/QJmoqFkHeBoAPZ/iWsnieEQq3ceNq7j9A2iBSipBID6Y6Ifmj6Ipj7WoQ+iKSWaJ/YC3/wFcPunrXupwTf1tbp9gmhFH+qei+WcRERERBRFQpRz1rU4YZJEYExSg2dqn2tPuIMF3O0MFmA5JxH1DBEfLDBv3jwcOnQIbW1t2LRpE84++2zvfUuXLsXKlSu9t1euXAlZlv1+li5d2v073ldkDgfO/x1w4f8DrD4fXje8C8zfLgJh6gdbez3R9JlscUo2mr6cMzZNnFXSB9Ekn1/RlhpMG5qBK0/vh1iIQFUrbAAAWT8UAAgeRPM4xdAAQyaaWs4ZIBPtuUnAiv8Ftr0ReHuAlommvlb9gcKhr7U+bIDoy0ZERNSHPf/88ygoKEBMTAwKCwuxZs2adtdftWoVCgsLERMTg8GDB+OFF14w3L906VJIkuT309qqfR4//PDDfvfn5OScktdH1OuEGCxQ2+yEGWoAzGy8lD3GfsFuJyDLyjomLdgmy9pyM8s5iahningQjaLAjIXA9Hv8l0uSlmptFoGsgEE0NTtLn3atBpts8dqHYLIYGmAIoun7pwFAay0A4JEfjkG+ksBWKYvyT1drI1xuj7ZusCAaANQdAVr1PdHUck6fnmjl32u3aw4F3576OLV0VU1Zd7YCH84X1wtmasuIiIj6qGXLlmH+/Pl46KGHsGXLFsyYMQOzZ882VB/oFRcX45JLLsGMGTOwZcsWPPjgg7jnnnvwzjvvGNZLSkpCWVmZ4ScmxnhcMmbMGMP9O3bsOGWvk6hX8SvnNN6ub3HCBF12mf5Sdvtkojl9yjl1PdHU7YbKRGM5JxFFCINo1DXay0TLnQCkFgAjL9OWqRM69eWcyaJM0xhE82nCrwwHSIyxYuE5/cQqKWKaq1V24tZX1qO+VemF1l4Qrf5IGJloFcD2N7XbniA91gBdJpryutQg2pZ/AlX7gIRs4OInlPuYiUZERH3XU089hdtuuw233347Ro0ahUWLFiE/Px+LFy8OuP4LL7yAAQMGYNGiRRg1ahRuv/123HrrrXjyyScN66mZZfofXxaLxXB/ZmbmKXmNRL1OiHLO2hZHgCCaLjimPy73uLWgnF9PNGUbvj3RTGZjYI2ZaEQUIQyiUddQs818M8cAERS7Zwtwqe5gN145aI1JAuxqEK2ftr7vdlWuVsApglCpFhHUSsvq57178/6juGXJd/B45DAy0QL1RNNlotUfA3b8W7vdVBl8e2omWrxPJtr2ZeJy+r1AonIw73G131+NiIiol3I4HNi0aRNmzZplWD5r1iysXbs24GPWrVvnt/5FF12EjRs3wunUTnA1NjZi4MCB6N+/Py677DJs2bLFb1v79u1DXl4eCgoKcO211+LgwYNB97WtrQ319fWGH6I+K1RPtEDlnMEy0dzOID3RdIMFDNM5A1S8MIhGRBHCIBp1jSsWA1e+BGSNDHy/2utANWUuMOEnwLirxc/As4DTfiLuay+IBgAtteJSDXzFpnk/pLPsLmwuqcX7W48Gn84JAHVHvaWhAMR0To/HJ4h2BGgo0243nQi+PXUgQZxusED1QeDIBrFvY68yfvAzG42IiPqgyspKuN1uZGdnG5ZnZ2ejvLw84GPKy8sDru9yuVBZKU5wjRw5EkuXLsUHH3yAN954AzExMZg+fTr27dvnfczkyZPxj3/8A59++ilefPFFlJeXY9q0aaiqqgr4vI8//jiSk5O9P/n5+Sfz0omiW6ieaC26IJrk0xNNHzRTHxtwOqd+sIBPTzTA2BfNd1onEVE3YRCNukbueGD81R1YfwLwo8WiD1r/icBPPwbyThf32ZO09XzLOQFvSae3BNMW5x1acPtkke31xH93Y8V20VtFDlRiWlcKtOl7ojUbA2h66UPFZbAgmizreqKpgwXagO1vi+sFM4HEbJ8gms/0TiIioj5E8jm5Jsuy37JQ6+uXT5kyBTfccAMmTJiAGTNm4K233sLw4cPx17/+1fuY2bNnY86cORg3bhwuuOACfPzxxwCAV199NeBzPvDAA6irq/P+lJaWdvyFEvUWfj3R/KdzSlCGAvj2RFNPfHsfq89EM4dRzqkEz/SBM5tuYBkRUTdiEI16npCZaEoQTR0GYI3zNhe9bsdP8X/xb6GioQ01DeL+BjnAmar6o/490dR+aPozXwBw/v+Ky2DlnK5W7cDCW87ZCnyvNDwe/2NluybArBwEOJmJRkREfU9GRgbMZrNf1llFRYVftpkqJycn4PoWiwXp6QHaSAAwmUyYNGmSIRPNV3x8PMaNGxd0HbvdjqSkJMMPUZ/ll4lmDKq1W87pe9zrcQcZLOARP4Cx/5l6/MxMNCLqARhEo54n3CCaNxMt3ns2SmqpwQ3u93HhyEyMyREfrtVOm/826nyCaM4mLRNNHXQAAJkjRdYcIAYNqGO39fQTPWN1gwWqD4jrBWdr91uVbDSXz4TOmsPG7RAREfVCNpsNhYWFWLFihWH5ihUrMG3atICPmTp1qt/6n332GSZOnAir1RrwMbIsY+vWrcjNzQ26L21tbSgqKmp3HSJSeDw+t/0z0cy+gwXUYJpvJprbqW1PMmltXwyZaLqT2mpWmqEnGjPRiCgyGESjnscQRAswNUvtZaZ+IFvj/D5IX5wzAGOyxAetW3dfmSyCXHLjcWNmmT4TzZ4EnP0rIHkAcO2/tH1wtxmnd3ofqyyzxmtnxVpqtIOLmBRtXYtyv/6MXG0p8MxpwJvX+W+biIiol1m4cCFeeuklLFmyBEVFRViwYAFKSkowd+5cAKKM8qabbvKuP3fuXBw+fBgLFy5EUVERlixZgpdffhn33Xefd51HHnkEn376KQ4ePIitW7fitttuw9atW73bBID77rsPq1atQnFxMb799ltcddVVqK+vx80339x9L54oWoUo56zVl3OGzETT90STdIMF9D3R9JloAYJoNg4WIKLIsIRehaibxYTZE82py0Q7vsO4Ts1h73TOIf1ygBIxfcuZNBDN9U2Ik9rgObFXiyI7mnRBtATgvIfEj8oaL7LVmk4Y9w/QMsjsCVqaubpvktnYs0G9X5+JVlMsDhgq9/u/ViIiol7mmmuuQVVVFR599FGUlZVh7NixWL58OQYOHAgAKCsrQ0lJiXf9goICLF++HAsWLMBzzz2HvLw8PPPMM5gzZ453ndraWvz85z9HeXk5kpOTcfrpp2P16tU488wzvescOXIE1113HSorK5GZmYkpU6Zg/fr13uclonYEGSywv6IBbS4P6lqcGOCbiSYFyUTT90Qz+fREU+l7opkCZaKxnJOIIoNBNOp5bIkAJAByeOWcgUZc12pBNH0Qq39uLkpa6jDIdRAmj0Nb36kPoiX6by8+A6htEtlrqQXAe78A0ocA5/zaWAaq79UAiKw6fTNka4BMNGertg9ERER9wLx58zBv3ryA9y1dutRv2cyZM7F58+ag23v66afx9NNPt/ucb775Zof2kYh0PL6ZaG60udy4+oV1aHV6IEnAaZLPdE41OObwDaIF64nm1o6bDZloSmsWQ080lnMSUWSwnJN6HpMJGHoBkDYESB2kNRNN6icuW2rFpRq8ssYCc14G+p8JDDxLLNNloumDYqbYZPQbNt7vKatqavHylyKb7ftKDz7baWxg7C3pbDoBVO4BdrwFrPqDCIAZMtF8JoHqS1MB7X59Jpp6ds73AIOIiIiIqCcIkIm2/UgdapqdaHG60exww+Q7WMDbE82nnNPt1MpDJVPg6Zwm3ddUtT+aejJaMhsz1YiIuhGDaNQzXf82cNcGccZJ/cDMGi0uW2pEM9KKXeJ26iBg3FXA7SuAwTPFstpD4gMaEMEtVUwyrFkj/J7OLreipOw4AOBQowkPvvc9HC5dSrk+iNagBNhkD1C5V+uJZkvUzpTpns8gUCaaGlDzOLV9JiIiIiLqKWT/wQLfFVcbFpnUnmjeck4lq8yvnNNlzETz9kTzaBlvFl25ZqIy/EPNRLPFGys9iIi6EYNo1DPpm4ymDhJnnAZMEbdbakTwqrVOlHLmjNMelzJAXNYcFhMyAaU8VGFPAtKH+T1dvNSG2cNEWrjbEo/KxjZ8qs9GU8tKmyqBxuPa8opdITLRfPqneTPR2rRl+gMLRxeWdMoys9uIiIiI6OQFyET71ieIZpWC9UTzHSzg9CnnVL+SysoPRKbZ/xwAfrlXOwmtHkezHxoRRRCDaNTzXf828IvVWrCstRYoXS+u9ys0pnOnKM2Baw8HzURDxlDtttJvQYKMKdniQ7t/TjYA4B/rDqGivhVujwwkZIn1myq0TDRACaKpmWgJ/qnlwTLRXAF6ogH+Z+pOxse/BP5YAFQd6LptEhEREVHf49MTze12YdMhYxAt3qr2M/OdztleTzTdYAE9ySROYidma8vUTLRA/ZCJiLoJg2jU8yVkATljgZgUcbulBij9TlzPn2xcN1UJotUd1U3v1AfRkoB0XRAtMUe73iiCY8MH5MBskrDhUA3OfOwL3Pjyt3DHqploJ4DGCu0xx3dpvdnsCSKDTp+NFqwnWrDAWVdmjhWvEqWipd923TaJiIiIqO+RjUG0yromNDncsFu0r5PxNp8MtHZ7oqmln1KQIFqAck21xNPGoQJEFDkMolH0iE0Vly01QImSiaaWeKoScsQgAtkN1BSLZb6ZaPZErbdCbKo2uEAJjiUmpeH6yQO8D1l7oArv7hHll+6GE6g7Ueq9T67Yha+2HxS7Jamp5rrJQWrgTxUoE80wZKCdcs79XwD7Pg9+v68Gpey0/mj4jyEiIiIi8uVTznmsRpxEnj40A+fEH8K39nmY5V4t7vSWc6qZaAHKOT0BBgvoqYE4PW8mGss5iShyGESj6KEG0VrrgGqlRLH/ROM6JhOQki+uq2nivj3RAC0bLSYZsCkp4WqZpj0Bj/5wLPb9fjZevEls/719YtJn8eFDKNq3z7s5qf4oGk8cBgC8tqUaRWX1kM36IFo4mWi6A4tgmWiuNuDNn4ifcLLVHE3awIP6Y6HXJyIiIiIKxmMcLFBWLY4zJxek4ZKEfciWajHCtUfc6S3nVDPRfE4SBxssoBcosObticZyTiKKHAbRKHrEphhvZ47UAmt6al80lW8mGgBkDNNuW5WUcLVMUwm0Wc0mXDg6G7+5dBSqTaKcMw8nkC3VGDZ/umk/AKC81YIbX/4OFbqY2L931sPp1h10eAcL6Hui6a8HyURrrRcZa+42kYkXir5vG4NoRERERHQyfDLRjteJY9bpQzPQL8Gn9FINnnmnc/qWc+qCaKZ2eqL5sjKIRkSRxyAaRQ/fpv2Tbg+8XobP9E27LhNNDaL1UzLY0gq0TDSHbkCAzu0zBuPtB38Cj8mKOKkNgyRRJnnYI4YN9JcqAQCJSamobGxDo0s7m7b2iBMfbNUFsaydzERrq9eut9YFXkfPEERjOScRERERnQSfnmiy24WUOCtG5yZhbJbduK4aAAvWE803Ey1Q6Wag7LQEZciAvqcxEVE3YxCNotfEWwMvz51gvG0LkIk2/hrg5g+Bcx70P5ulD7opEuNiYcocAUBM8gSAogTjUINbzhuHoVkJ8OjKOesRj8WrDsDjUZqnWgL1RNNnogUJoqnDCwBjQC2YRl0Qra4bgmiyDHz7N+Dw2lP/XERERETUvXwy0cxwY+rgdJhMEpKtxlJPmHwGDPhN53RqQbmgPdECLBvzI2DOy8B5v+nECyAi6hoMolF0Gf1DEYi69dPAZ6gA/yBaXBoQlw7EZ2qN/s0WoOBskYXmO+En1accVJU1SrtuicXZs68x3J2SnIZP55+Nobnp3mUuayL2VzTi8yKlyX/ITLQg5ZxtuiBaWJlox7XrLdX+ZwC7WvkO4L+/Av5z16l9HiIiIiLqfh5jJpoFHkwbohzz6odkAbpyzmCDBdxh9EQLMlhg3FVAfEYHd56IqOtYIr0DRB0yZ4nIxIpLC75OxgjAbAPcYhgArPHA3K8BSCJ45ksfRLMnAUn9Am83c6R2PSELcYOnGu+3J8BskgzTOWeOH4qVG4DnVx7AhaOzIXl7ogUJooWTiRZOEE2fiQaIvmjpQ0I/rrOalH5ydaUiKy3QWHKivqp0g/g7kz060ntCRETUObIx28wMN6YMVYJZrjbjumpQzBQkE009RgeUTLQAx408liSiHoqZaBRdzJb2A2jqOhnDdbetQFIekJQbeH19OWfmiOAf2lm6L8CJOeIsWGqBtkwtG9UF0S6fMhp2iwlbS2ux7mCVNpI7WBAtaE+0Bu16RzPRgFM/XEDdP7cjvMEHRH1Fax2w9BLgH5eLADMREVE0Uso5PZLoUZxslzA4QzkR7ZeJZjJe+vINovmua7YziEZEPRaDaNQ7GUov7cHXA4yZaPpss/a2qTY2zT9TW6ZOAVWzzQCkp2fgxxPzAQCLVx7Q7nMG64kWrJxTH0SrDb6PqkCZaKdSq65PmzrllIiA5irxZaHpRHj9DImIiHoipZzToRQy5afYIKmBLt9MNG85p09JplUNugUKounWVU86ExH1QAyiUe+kD3iZrMHXA4yZaFntlFulDNTWVYNo/Qq1+23KQAKLTVkgAbZE/PzswTCbJKzZV4l/bFQyxFytQFOVOCDR90cLlolmKOcM44u4momWqGTfneoJnfrgQOPx4OsR9TX6/9+NJyK3H0RERCdDyURr9oggWr8k3fG1O0g5p282mXriWr9+oEw036FfREQ9CINo1Dvl6IYLmEL8mtv0QbR2MtFMJlHuCQCJShAtZ5x2v28mWkwSYDIhPy0Ov5w1HJIEfL5PlGK6y7YDTw5D9eu3or5RF4AK1hOtw4MFysSlGuTrrnJOgEE0Ij19pmkTg2hERBSllGmabRDBs5QY3fG1XyaaEjzzHRigBtFcAYJoJmaiEVF0YBCNeqch5wIjLwPO/HnodU26YQOZo4KvBwBDzhOXanCq/5liyueIS7TgmVnJRItJ9j5s3jlD8cbPpiAxQWSrmd1tgOxG3YENcLbqAmfBpnN2ZLCAs1Ur+cw7XVye6ky01lOUiVZfBuz9jL2kKHo5GUQjIqLo53Q6AQBtsgiiSfppnaGmc6rU/sFup7bMFGBdZqIRUQ/G6ZzUO5nMwLWvh7euPksrMaf9dc/7LTD5DiAhU9w2W4CbPzSu481ESzYsnjI4HeOvmwy8qi1LkBsRC60vxJ4jFfjk8304b2QWxvXXPb4jgwXUIJbZDmSPEdejtZzzg7uB/SuAWz4GBp3Vddsl6i76cs4m9gskIqLoI8syNhSfwDQAHrVNilLeCSD4dE7fnmjtlnMyE42IogMz0Yj06eOhJgFJkhZAC0YdZGBP9rsrLi7BcDsJTYiRtCBadU0tnv58L37y0nrUtejO0ukz0UI1J1eDWAnZWu+2psr2H3Oy9PvkOxn0ZJTvEJc1h7pum0TdyVDOeYr/HxIREZ0Cu8rqcaJOVE7kpKeIhYYgWriZaO0NFtAdgzOIRkQ9GINoRDPvBwZOB65+NfS64VCDaDH+QTRYYww37ZILJmilioOSgP6psWhodeHVtYeAL/4fsH6xIRNNDpWJpmbWJeYAcenienNV50sia0uAVy4Fdi8Pvs6pKOd0NGlTRpuru2abRN3NMFiAmWhERBR91h2oghkeAEBcrFJq2V4mWqA+Z4DWPzhQJpqhJxrLOYmo52IQjShlAPDT5cCYK7pme+rZs9gU//ss7Z9Zy43z4FcXi+EGH63ZAKx5EvKnDwEttd51qipP4I3vSoJvRM3aSh2oBdFcrcGHFoSyezlw+Gtg0yvB1zGUc3ZRoECffdZS0zXbJOpuHCxARERRbv3BKpiUIJr3ZHF7PdEC9TkDALsyyT5gJpq+Jxoz0Yio52IQjairjb4CGHoBcPqN/vf5ZKL5cTTj0nG5GJwZj5g2UfolyW40lu/zrpKIJjz28S5UNzkCb8MbRBsk0ubNysFOc1WHXoaX+rj2Hm+YzlneuefxVV2sXW9hJlpPJcsy7nhtEx58b0ekd6Vn0mWiVZSXor7V2c7KREREPce+4w2oaGjFt8XVsEAJmnmDaGFkovkG0eKzxKVbPYaVtDJODhYgoijBIBpRV8sYBtzwDjBwqv99ITLR4GyC2SThsR+Nw5lZ2hm+BJeWiWWXXHC0teCZL/YF2oIuiFYgDkz0JZ0d1OxwobVeyZ5pr6RSX87ZUuN/MNUZNfogGjPRwtHmcuO6v6/Hh/96Fnh+KlBRdMqf82htC/77fTn+9W0J2lzu0A/oa3QZoA1V5fjnusMR3BkiIqLwHKpswuy/rME5f1qJhlYXbGalLYj5ZIJoSl9htZxTf79hsECIk85ERBHEIBpRd1LP3gXjEF+4pwxOx2/OCT7AIAnNWLr2EG5e8h1OfPcO5LV/xbr9lahpcmjBp9RB4lINojWFH0ST3S589skHOOfxT7ByixKIaS8bTJ+JBnRN2Zo+E4090cKy61g91h2sQsqet4GKXcC+z075c9Y2OwNeJ4WuxCVTqsORmpZ2Vu7ZSqub8fuPd6GsLnpfAxERheezXeVweWQ0O8QJsvRYJcjlm4kmy8YeZ4BWzunbEy1ByURTyzkNQTSWcxJRdLBEegeI+pRg0z+tcSJjxdUCeDyAydRuIOr1nDewv6oNr+67CGklv4cED/7Q5kFl4iiscZZCArQgWnzHM9G2fPg8Zm39Lfa7LkeSVA+YAbTWAW4XYPb5s+F2an2frPGAs0lM6EzuH/bzBWTIRKs9uW31EfWt4oA2Rz4BSOiW900fOKtuciA7iWePDZxawClJakZdQ0M7K/dsr3xzCEu+KYbNYsL/XDQy0rtDRESn0BdFosetxSTB5ZGRHmcGWuDfEy1Q9UGg6ZxmmzZ0K1AmmonlnEQUHRhEI+oJ4jKAOmVYgLNZTC9qqvRfz54EtNVjeO0aDDcDF5s2eKd7Fpr24rOGREh2N1wmG5YXy6htOYRL5USkA0BzFY7UNCMxxorkWGu7u9N8YB0AYKTpCFKh+9LfUgMk+GTI6Us504cA5du7ZkIne6J1WH2LE4CM/pISgO2GMtiaZq03X02wPn19mU+zZVdD9A4XOFYrAoLH67ugXJuIiHqsuhYnNh4WxxDLfjEVxZVNyNluBaoggmGAlonmO1QA0A0W0GWixaYCJuWrp9tpXA9gJhoRRQ0G0Yh6gtiUAEG0AF+2k/KAE1rQyiTJ3usz4w9Djh8N1ACHXBm4581tAACPpQ23WICSI6U478OViLOZ8f+uGIux/ZLRLyUWMVblAOboZuCjBZBn/xEJDQcAACPjm2Bu1gfRqv2DaOpkTmucyD4r3w40HDuZd0NkvNWVarf15Zx1R8SZTHXCE3nVtzqRhgbESkowq7X2lD9nrS6IVt3MIJofp0/pY1dNr42AE40ieFbVyCAaEVFvtmbfCbg9MoZkxqNwYCoKB6YC232nc6pBtECZaEpAzBQkiOYK1RONmWhE1HOxJxpRT2CL1w4YHE3i0jeIZrZpDVkBYPC5cA29GEtNVwIAJlsP4DfTxDaq7f1wxoAUnDcyC42mJADA19v3wOWRUd/qwr1vbsX5f16FyY99gWe+2IdWpxvYtBQo24rG1c9ikHwEAJAl1Rgz0QL1JlODaPYkVNvyAABufRZZZ9SVioMz9eDK1SKCEY0VwDNnAK/+4OS2r+dsEf08eoH6FpeWhQZ0Szlnja6ck5loAficoTe3dnJKbg9Q0SBeS9DJwEREFHVaHG7c9/Y2fFGkVRF8tVscS5w/Kltb0RNkOmegTDTvYAFdG5PYVMCsVEIEHCzATDQiig4MohFFSsoA7bo1VguiqdP8fINo9kStlwQATPwpLDcsw0Vz/wRZMsHedAymUlGGeebpZ+DdedOx5JZJuHDiaPF0aMDgzHjccc4QZCbaEWczo67FiadW7MUv39oGuXIvAMB26CukSCKQZ2mugE3STVwM1FdNKec84bRh0WZxQHVw947OvCMatR9a+lDtzGRLDVC+A3C3QS7bhqIjXRCMqC0F/jgE+M+dJ7+tHqC+1Yl+kq4MuBvKOfU90Wo4WMCfTyZavLM6KqeYyrKMCqWMs7KRQTQiot7iv9+X4d+bjuCJ/+72LttVJo7tJg1K01ZUg2bmMHqiBS3n9Gknog+ymZiJRkTRgUE0ou5280fAzF8D0+dryyyxgE3NRFODaD490WwJxnHiA6cDAHKzMiBljRHLdrwtLtWhAgBGDBbXB8S04LmfnIH7Lx6JDQ9dgB0PX4SnfjwBVrOEj3eUofmYOHiyu9ppfB6oN5kymfNoixWHZHHGUq4+iOP1Ac5MhqtKlJMifag46AJEQEgp8ZRkD37x3H/wz/WHO/8cAHDkOzEI4fDak9tOD1Hf4hNE6+5yTmYo+VOCaB5ZfFFIR31Uvk/1rS60uUQpTzTuPxERBbavohEAcOBEI1qdbsiyjMNV4mTq4Mx4bUW5M5louq+asan+0zol9kQjoujDIBpRdyuYAZz7ABCnO7tnjRWTLQER1JHlwJlox3dqt+MztOv9J4pLWelXkVag3RcnpnOOTXFiVG6Sd7HZJOHKM/rjkcvHIgUNiHeFzlpqawgw7EAp56yX4zBu3OkAgHwcx19W7PZft+Rb4NA34vqH9wKvXi76n/nyBtGGaO9TczVQW+JdJV+q0EpRO6vuqLgMEGyqbnKgsS3AvgXTVGn894mA+lbfcs5uHizAnmj+lC8XxyGCwelSPaqiMJPrRIP2JanF6UazowP/N4iIqMfad1wE0TwysLu8ARUNbWh2uGGSgPxUXUaYGjQLqyeaEhzz7Ylm9s1EC9YTjUE0Iuq5GEQjipSYFO26NcaYidZaa8w6A0Qm2oTrxPUh5xvvG3Kudj1tCJA/WbsdpwTbApViAvjJ5AH407nhpc2v27HXb5ncWgcAaEQsJk4YD49kQazkwFcbd6CoTDe509kC/PMK4J8/Er3NNi0FilcBJ4r8n6hqv7j0zUQzBNFO4ERDG147mWy0uiPKtmsBj0db3OzEzD9+hYsXrTZkWrXr9auAF2ZoAcAI8M1Ek1vrDa/rVNCXcDJDKQAlE61MFsHgVDSgMgob81foJnJONe1E085PI7g3RETUVQ6caPRe33WsHsWVIgutf2ocbBbdV0X1eMLsE0Rzt1fO6ZuJ5jPTzhBE05V2spyTiHowBtGIIiU2RbtujTP2RFNLOe3J4gcQmWgzFgJXvwpcvdS4rVGXA9e/A/xiNXD3JmOWm5KJhuZqY0Dl0DdA6XcAgAsz6wLsoOS35Hh5Gf6z9ahhWWWV2NdmKQ5Th+XAlCp6vQ3AcTzy4U7IatP+E3vEa3O3Afs/1zZQc8j/qQ1BNOW1tFSLHmaKfElMOfz76oPweDo5GKBefS2yNiABQFF5PRraXDhS04Jfv7NDew3tObFXlDoUr+rcvnSB+lanIRNNggy0Bfq37SItNTij/kvYIAJpPSUTra7ZiRtf/hbXv7Qef/hkN5zuUxtIbJeSiXZMFv8PU6WGqAw2qpM5Y9CGV6x/RPqHt2hDUIiIKCq1Ot3e0k0A2FVW5w2iDcqIN67sl4mm9kQLVM4ZrCdae0E03XVLTLgvgYio2zGIRhQp+iEBlhgRJANExphayhmfASQqk5HsCSK9fcwVQEySYVOQJGDYBUDuBOOZPEALqMluLaByfBfw6mXAyxcCb/9U9AYDgEEztMfljPPb5VSpAfe/sx3fFWu90UrLxDSnpJQ0xFjNQNpgAMBQy3GsP1iNT74vFytW6DLO9q3QrvtO8nQ5gFoluyxtSNBMtAnxdYixmlDR0Ib9urOoHaJmogGGks6Sqmbv9U92luOj7WXtb8fRLMpwAW9gMhL8eqIBp7akc+Uf8L9tT+J6swiK1jT1jMECK/dWYM2+SnyzvwqLVx7Amn0nQj/oVFEy0cqVTLQ0qSEqyznVTLQh0jHESE6YPI6g2a1E4Xj++edRUFCAmJgYFBYWYs2aNe2uv2rVKhQWFiImJgaDBw/GCy+8YLh/6dKlkCTJ76e11fgFv6PPS9SbFVc2QX8ectexehxSgmiDfYNoQXuiBSrnDNYTrZ0gGgcLEFGUYBCNKFIM5ZyxQJ7oJ4bD3+iCaJlAghJEsyV07nksdsCmBuiU4NeGl7T+aTvfBTb/Q1wfdbkIgsVnGUtElX0YFNeKVqcHP/7bOly1eC12HavH4aMiwJSTlSXWVYJoPxwgDqr+7+Mi0besQtcv7MCX2nXfTLTaw2LfrPFAYo4WBGw4DjRowaxBlkqcni8CbPqgXofog2gttd6rh6uNGTbLNpSiXc26wFXJ+s7tSxfwtNQhSRJBmxpZ+X3Rva6uJlcfBABMNon+dz0lE8030+vgiQhmTCln6MuUTLQUNKKyKQrLOZWeaMMkXSbqKfzdot5t2bJlmD9/Ph566CFs2bIFM2bMwOzZs1FSUhJw/eLiYlxyySWYMWMGtmzZggcffBD33HMP3nnnHcN6SUlJKCsrM/zExGgZLR19XqLebr8yVCA93gYAKCprwAHlM3NQuk8gy9OBwQImk/ESCNwTzcTBAkQUfRhEI4oUfSaaNRYYfI64XrxaBIwAkYmmBtHUTLXO8DbnrxLTNLcvE7fP/19jqn3WSOD2L4B560QWmCp9GABgSLwDV5yWB6tZwsbDNbj0r2tgcooDsBED+4t1lSDaGYk1yE2OwdHaFvx99UFjJpou66vu2F5sLa1FfauSxeQt5RwisurUTLTj3wPQTpdmOMswqUC8ro2HOhFEc7Yag1+6jK1DSibaHRMTcaP5M2w5cMQbRAhIP0m1plj0fIuA5FYR4KiTkryZT6cyE83dKIK9E0yiD1yzwy0Cpk2VwFePGzIHu5O+TxsAHKqKYBDNKX6X1HLOqM1EaxCBv2GmwNmbRB3x1FNP4bbbbsPtt9+OUaNGYdGiRcjPz8fixYsDrv/CCy9gwIABWLRoEUaNGoXbb78dt956K5588knDepIkIScnx/BzMs9L1NvIsmxoUaFO5jx3ZBZirCa0ON1YuUccw/iXcypBNN+eaO0NFgiZiebfOgQAM9GIqEdjEI0oUkxmrd+ZJRbIO0NkjLXUaJla8ZlA9mhxXQlOdYq3L1qVCKA5GkVg7KyFwPR7tPUyhouAW3wGkJirWz4UAGBurcGia0/HV/edg8GZ8ZBlIEkSQQJ7fLJhPy01xXjgklEAgL9+uQ8tR78PuGvVR/biiue+wbTHv0RpdbOxHxqgBdHKtgEATsgpAIAYRzWm9BMHchsOdSJQVG/s7RaonPOGxlfw/6xL8b/mf+Dj9ko6m3xKKEu/7fj+nKRWpxv9PGIfmxMGoQ7Kwe8pDHTIyuvOlarRzyT+DWqaHSLTcdUTwDd/OWXP3R51GEROkshAOawrz+12Tp9MNKkJtY0tkdufTlLLOcdYdf8PmIlGneBwOLBp0ybMmjXLsHzWrFlYu3ZtwMesW7fOb/2LLroIGzduhNOpBc0bGxsxcOBA9O/fH5dddhm2bNlyUs/b1taG+vp6ww9RtNpcUoMRv/0Es/+yBu9sEidE9lc0AABG5iRifL8UAIBLqe8sCFrOGaPdluUgPdHUcs4QPdH0J5T1wThmohFRD8YgGlEkxSoHD9ZYwGwBCpSeZPuVnmHxmcDUu4HbPgcKf9r550nKE5dl24Gt/xLXJ/5UnAGc+Wtg2EXAuKu1rDdAlFKqMoaLy5YawONB/9Q4vPWLqfjpxDScZRclfUhWMtEyR4jL4zvwg+pXcfn4XMS6GxHbUh5w1/qbKpEaY0JjmwvPfrnfL4jWYk0Rt5XG/7s8A1AniwO705LqYTZJOFrbgqO1HQxM+AbRlICALMtK5pKM7IqvAQBXm1dhy4avgw8YaI58EK2h1YUCSQQ47DnDUauUc3qaT10mmknXE2t67CEASillpTLF1bffXTdRM9FOy08BEMFMNFkGXGpPtFTv4raG6OslpmZijjDrgmjMRKNOqKyshNvtRnZ2tmF5dnY2yssDf06Ul5cHXN/lcqGyUvz9HTlyJJYuXYoPPvgAb7zxBmJiYjB9+nTs27ev08/7+OOPIzk52fuTn5/fqddM1BN8WVQBh8uD3eUN+OXb27ClpAY7j4ljqyFZCfifi0cY1u+X4hPI8g4WsGnL3I7AmWhqmaZbl3kdKIimTpD3XZeDBYioB2MQjSiSYnRBNEAr6VQPVAZMEQcr+ZNEkK2zRl4qLjcuAY5uEmcIx12tPHcMcP1bwJyXjGn1+kw0NStMN5wgI8GO32V9DauzHsgYARTMFOukDgLOeQAAIK16AouGbcPPRooDoxOyz0AEAFa48c+r+wEA/r35CFrL93qfc/3BKvzq4yOG9Y8iA45E8UUmrukIxuaJbX5X3MHARJ1xu2pAoLbZiYZWF4ZJR2FpFiUNJknGVVV/w8trDgbeltrDTi1xOLq5Y/vSGZX7vU3rATGZs8AkAhzJ/UeiySSCaFWVx0/N8ztbYHZpwalCsyjprGlyasEz3/e4m6iZaKcNSAEAHK1pgcMVgQmdui8WTYhFq0X8rrobK4M9osc60dAGG5zIdh3TFjITjU6C5FPGJcuy37JQ6+uXT5kyBTfccAMmTJiAGTNm4K233sLw4cPx17/+tdPP+8ADD6Curs77U1oaoj8mUQ/me7Lxz5/txeGqZtgsJkwcmIpJg9Jw6/QCACIzzWL2+ZqolnPGpmqT0w+ubH86Z6tuQrg90b8nmlopARi3Y+JXVCLqufgXiiiSspUJmBmi5xiGXgBAAkxW4IfPA0PP75rnGXmpCPA0KmfbC84GErLaf0xcunbGMDFXNPoHtOEErfXAuufE9Zm/MjaHPefXwLkPAQBMX/8Zd40SwZbyuJGosSjPa4n1BufGxtZg5vBMWD2tkI9uAgB8XZ+JW175Dp82DMR+aaB30xdNnYjMAcrZ0uLVmDZUnMX896YgAZsv/09MIPU9U1rnm4kmMrYOV4vSv4tjlR5uWWPglqw427wDOz99Cd8eDBCsU8s5+xWKSzWb7tDXWn+79pR/D/zrWhEYC8fRTcCzhcD7d3gX1bc4MVgS/77mjGGISRQHphUVpyiI5lPCOhYi26Om2SH6wgEiiBYse+8UUgccDM9OQKzVDI8MHKmJQEmnS/vC0gobXPYUAIDUEl2ZaK1ON+qVTEcTdMFIZqJRJ2RkZMBsNvtlf1VUVPhlialycnICrm+xWJCenh7wMSaTCZMmTfJmonXmee12O5KSkgw/RNHqaI34TPrBBFGd8PV+8Tl+7ohMJMaI4NZDl47CH+eMx5NXT/DfgHqC12wHJlwnrm/+h5iqDmhDrADtmFAfRJOkAJlo+iBa9PULJaK+iUE0oki6/Blg/vfaZM70IcDNHwJz1wCnX991zxOTDAy7ULs99qrQjzGZxEFS3hmiRDNJyUzb8W9xufJxEXhKHwqM+ZH/46fdLaZ81pVC+vxhAMC4wrOQmi/6pCFtMJAqzniiphgPXDISl8TuRCzacETOwA0fN6PV6cH0EXnIu3cFkCV6w6WPmAqcdoN43Ld/w08HN8BikvDN/ipsKfEpXdz/ObD6T2IC6b7PjPfVK0E3s1KWoGTVHFZK/2ZalWmiE66F6Zz7AQC/s7yKN77c6P9a1YDSgCnisvE4sG8FsPRS4O1b/Nf3te45YO9/gbVh9hBTM91KtLJREURTsoTShyIlXQQrG2pOhLfNjlJKWN2yyOAY4twHCR401lVqwwycTad0sEEwNU2inDM1zoaBynSxiPRFU/qhuWUJTphhThAB3wR3vTZIIwocrxevY5TVpy8gM9GoE2w2GwoLC7FixQrD8hUrVmDatGkBHzN16lS/9T/77DNMnDgRVqs14GNkWcbWrVuRm5vb6ecl6k3Uk0nXnZmPOJt24vOy8Xne62aThB9PysfYfsl+j/dOdTeZgTNuEtf3/FcbIqTvb6Zmd7b59BFsN4jWzgAnIqIehEE0okgyW4EUnx4rBTOArFFd/1zjlMCZyQqM+kF4j/nhs8DPvxLjzGf8Uixb+Tjw+cPAty+I2xf/wZiFprLGAtPuEtedzSLYNuUOESgEgLQCUfoJAFUHMDInCX8cfQgAsD3hbAASpg1Jx+IbChGXkg38fCVwxzpg8LnA8FnA6CsA2Y2slb/ClaeJL0lPfbobVaW78dXu43j9m32of++X2v7sfM+4f2qpYeZIcalk1ZRUNSMGbRjr3CGWDz4H0lnz0Zo5DilSE0Ycfg1VjT5ZbWpPtLQCETgEgE1LxWXJ2tDZaGoPsZIwe6nVHBKXDceANjFZq7XuBJKVIQ9IG4ycbNHTrrWhKngvNwB46yZg8XTA0cEgU5PIpjog58EDE2LkVmSgHu4qnz5oESjpVDPRUuNsGJQuMigj0hdNyURrhQ2ABGtiJgAgRWrE4coIDjvooGO14ovN6XaRweNRAqcyM9GokxYuXIiXXnoJS5YsQVFRERYsWICSkhLMnTsXgCijvOmmm7zrz507F4cPH8bChQtRVFSEJUuW4OWXX8Z9993nXeeRRx7Bp59+ioMHD2Lr1q247bbbsHXrVu82w3leot7K6fagXDkhMjQzAeeNFMcqsVYzzh8VojJBpWaimcximnv+ZNHmY+trYnlsirauWs7pO2VTkozDBuJ1QTQ3M9GIKDqcRJMlIooqIy8DJt0OZI8xHuiE67SfACXrROr+10+LZWOvAoZdEPwxE28VwSRLDHDDu6KEdOiFwKZXRemqekC27jnA1QrLfpEtdsk1v8C2zEIk2i0wmZSzmRa7NqkUAGb/Adj/BXBsMxaO3YXmbbtxb+k7SH/5KL523oB6xOF66yG0wI5YtAF7PhE9xNT+c2rJZe4EoHy7N6tmX3kNnrM+gxhPiyhjzR4LmEyImfoL4IO7MAH78eG2Y7hlegHgVvZfzUSLzxSluU0VIhNNtf/z4JmFsgxUinIjVO4Rwan4wOVJXrWHtevVB8RrqBavp9qchTRbHPrniTPLMa567DnegJE5AcqQKvcDu/4jrpdtBQZ2IBtD6QN3XE5Fjs2FJMdx9JdOoKnMJ1up7giQOz787Z6kNpcbDkcb5lveR0ZdBgZmiNcdmUw0EURrgR02swnmBPHvmoYGHKxsxLj+Ac7090BldeJ1TJJEifN2eTBOkw7A1VSDwDlARO275pprUFVVhUcffRRlZWUYO3Ysli9fjoEDRel+WVkZSkpKvOsXFBRg+fLlWLBgAZ577jnk5eXhmWeewZw5c7zr1NbW4uc//znKy8uRnJyM008/HatXr8aZZ54Z9vMS9TZ1zU4s+aYYZw/PgEcGbBYTMhLsuHbSAHy0vQw/OqMf4mxhfh1Ue6KpQbBhs4yDlPSZaOrJ1Rn3ASf2GI+BzFbApWzLkIkWYEABEVEPxCAaUV9htgKX/vnktnHJkyKj7NDX4mDq4ifaX9+eCNy9WZQAqAdUIy8BHjwmBho4W4HiVUDRh8B3fxf3J+QA/c9Ecqimsok5wPR7gK9+j5yV9+NZa4P3rmm2A6L8tA54zXU+LjF/h37OSrTs+hSxE64QQS81m2vwOcCWfwItNdhf0Yjhu5/H+eYt8JjtMM15WWtuq5TcjjUV44+bS3HLxExg6SVAY4XW4D8uQ7w/h78B3LqDwX2fBQyieTwyXly+Fr9o0/UMKf1WvEftUfcdEMHA3Amw1oihB5Ux+UgDYE8QTX+T0YQVO48HDqLt/lC7fmJ3x4JoSvZdFZLQGm/zBtEcJ6qN63VzJlptsxOXm9ZivuVdyF8UY9Bp/wAQoUw0pZyzFTbkpsRAUhoxp0oNKK6M0MTQTiira0U8WjDcuQsAsDZmJk5zHEBTXRVSIrtrFMXmzZuHefPmBbxv6dKlfstmzpyJzZuDD215+umn8fTTT5/U8xL1Nn/4dDf+9W0J3tsi+sD2S4mFySThrGEZWPvr85CRYA9/Y7IS+FJLMpN9KiliUrTrknLsFJ8O3PiucT19Sac+iOaJnjYHRNS3sZyTiMJnsQPT7wWuf1scFCVkhn6MJPmXe1pjtMtrXhNZamPnACkDgLPvC38q05R5InDlEAE0OUMMHDgvuwnnZ4vMo6R+I7DcLTIRvnj3RTzw7g58tuJjZf3hQIrIQGhtqMajH+3CTGkLAMB0yR+BQdO158ocCdkSgySpBfXH9mD/K7cDZduAhjKtwXp8hjYkQu/Al8D6F8T6rfXACzOA9+ZifXEVVq79xrhu6Xpx6XZqmW6+arQMDXUYQWyDKKOsi1MyKmJTAQDJUiNWFAUpJy36SLt+Ym/gdYJRsu+q5SSY0sRzDrJUIdOpTm9UMgjruneaXU2zA4UmkdknlW3H0BTxu1RUVt9+WeupoJRztslW5CXHer8spCK6gmjHalsw2VQEi+wCUgfBmn8GAMDV1P397oiIqH2yLKOioRUtDjc+3Co+k0uUoUn9UmK96+WlxMJm6cBXQX05JwAk5RnvN/REC9DmQ2UIomWE//xERD0EM9GIKPKGnt+5SaT2BOCyp4DPfguctQDSgCnA81Mg1RzylgVcc+EMbD42CfhyOaZiG+7+7hByLCsxywJ8WJmDt/+1B/8E4GisxrqqMoywK0GfwecYn8tsgZQzHjjyHR60vI6h5Vv8dudAcyxykwrg7QCS1F80OciWvwAAK6JJREFUym2uBD65H0jMA87/X1E+Wr4dK5quxBBlGIBTNsMquSEfXg/J4wFeniUCVfPWidepaqkB9JlrVSJglNh0CADQlDBILI8TWU/pqMeBI+Uoq2tBbrJ28Iz6Y8BR3ZCEE7vDestVjoYTsAGokpOQkJUPHADGJ9QjoaFCrJA7XgQNA2WitdaLwGPKgA49Zzhqmpw43aSU6spujDcXw2Y24Xh9G4ormzBYKhPBLOX9OaV0mWh5KbogWhRmos0wKT0Ch5yHkSni39viqGv/gURE1O2WfHMI/++jXZiQn4KGNuPJuP6psUEeFYJHN5lZDYK1F0QL1CtXJekCd3Eh2lcQEfVAzEQjoug2+ofA/O3AxJ9qgwra6rWeZ2kFOGP6LMi2RKRLDfjfQhfOTxSBsu+cQ7CrRvwZTJKacUVuNWySWxwIpgTokZN3GgDgArMIoG31DPbe1Qobzn92Iy59vdy4/lnzRTDNEiMGAax8zHu3Z+8n3iDaSqlQLDu6Cdj9EXBsM1BXIkpD9WoOG28rr7Nfkyi1a0lVBiWkDATSh8IuuXC5eS0+2HrM+Lg9y8WlXSnzrOxYJlpzjXidrphUxGSKKatDbdUYYFKy3gbNEJeBgmjv3Ab8tVDrBdeFGuprMULSMvXsZZtw+oAUWOFC6/IHgGcnAv8MME32VNANFshLifF+WUiTGlB8oqn7M+M66VhtC842bRc3hpyHcUMHAQDiPY2oqG+J3I4REZGBLMt4/VtxnLCttNbvfn0mWod4dME4NQiW1M+4TriZaI5G7Xp3nNAiIupiDKIRUe9hjRXDAADRu0Myi54dZiukgrMBAD/NPoCxsgje3H7t1Xj+9vO8D/9ToZJZkztBG8+up/RFA4AWcyLudd7lve2WJdjMZpTKmXDK4uBxo2MAGgvvABbu1Kaj1moBnpnyJoyzi6BTwthLUOTJh1l2wfXeHdpzHlxl3Ad1qIDae6RyP1pPHEaq6wRcsgnOXGUfJQko/CkA4HrzF3j2q32oaNCNj98rhjgcGXqduF1/VGSIhcnVIAYLxKbkeDPKch3FyIXoidaQO1Ws6BtE87iB4tViClfx6rCfL1xS+TaYJV1w6sgGTB2Sjt9a/onRxaI/Gsq2AnVHu/y5/SiZaC2ymomm9ERDIxraXKhsjI5JZO7aoxhiKoMsmYBBM5CSJsq4rZIb3+3t3nJdIiIS1uw7gev+vh43vPQtfv/xLuw8VoeisgYcPNEEdSaTJAFXnq4Fu/p1NhNN7YcGaFlm1hhjJplhOmc729JP4TRzPA0RRR8G0Yiod1Gz0QAgub92gDZUCZZtelWUEprtGDRqEiYPzQFsSrlksRKwyp0QeNu6IFrsWfPwwPXaAIB4qQ3bfjcLH957Lo6YRbPdv+xOwmmPfIarX1iLdxxTvOu6zOIgdrppJ8ZKopfZ5ElT8XWWGD5gcWpnaeVinyCamolWMEOcDXY04PN3XwQA7DMV4Owxutd/2k8gm+0YazqE8x2r8Lv3dqDV6RaDEJQA1h3bhqBCThHrdyAzzNRcBQBIzsgFUsTrjWkuh0mSUezJxqy3lNfQUCb6u/37NuAvE4BjW0WJKyDKWrtYfIXIEqy1KP36jmzA1II0zDZ/BwCQbYkAgKf//iJ2HjvF5YiGTDStnDPdJN6baCjpbGpzYbBjDwDAkzlafEmyxsEtiXKeb3YciODeERH1XX/9cj/WHazC1/sr8eKaYlz6zNe46w0xfOPC0dl44YYz8LcbCnHVxP7ex/RPjQu2ufbpM9H0Pc30JZ36TDR9+ScRUS/DIBoR9S6pBdr1NN31IUoQrU7JBOs/CbDYxHU1q0vN+so9LfC2M4aL7cdnAZPn4uKxud4G/gAQazNjVG4S8n/6MjaNeQhHU6fA5ZGx4VANfrUpGSdkUTr5Zts0lHoyYZeciHGJQI4pczhu/tlCVFmyAMAb2JKOf49rnv4IFzy1Cm9tKIWnWgTdPOnDUWsXB68jj74DAEgafhZS4mza/salQVIy4BbZnseiA7NR9dhoHPn3rwFXC+qtWdjh6od9HnGWuq18V/D31dkKvH8nsPEV8VqdIuMsK6efKFfVnXb+Pm4SylwJaJFtAGQRnNv5npgq+u1ibZvlSp+t5mrg7+cCX/y/4M8fprRaEZjbnn2FONBvPI7T3duQKdWhTbbiI8uFAIAB9Rvwy7e2weloE73bTkVppVMLovXTlXMmoREWuNCwd7U3I7CnKqtrwQSTCJSZ+4uSY0gSZOX/zLZ9h3A4EpNPiYh6uboWJx56bwfWHqj0u0+WZRQdE9nj/3PRCMwemwMAOHhC/D2+bHweLh6bi1ljcnDGgFTE28wwmyQMyuhsEE2XiaYv1dSXdOqnc7pPItNa4tdTIurZ+FeKiHoXfeDMEFAbDGSPFddzTwMue1q7Ty1BUMsVgmWimczA3DXAXd9pfTyuexMw24FZv/euZsmfiMKrf4Uv/+dcrP6fc/H4leNw6YR8vBt7FaqRhM/jL8XB/j/Utjv0AiAuDTa7HamXPgwA2DTgFuyRRYZX2onvsL+iEb96Zzu+2SjOMj+3xYmPGsU00qEm0e+s37hz/Pd59h+AqXfBaU2CXXKin1yO/nuWAgCWt40HIOGwSZylXrl6FUqVCV7i/ZCBbW8CJ/YARR8CW18DPvsN0NaIWFkEiPL7DxDBSN3Z6Mt+dCNumjoIe2Wx3S/feUF7b3d9oG3/+E4xgXTX+6IH3LpnvYEnL49HlF4e2SiCnPVKbze3E1j9JPDkCGDts97VsxuLAADVmZO8wVDbKvFvs00ejDdqRc+4s0w7sbu8HkX/mA/87Wxg1R/937v2tNaLoKD6PgWYpNraIjLO2mATQx1i0wAlE26MdAjT18+F/MY12LR9u8gQ7GqyDDSUn1SA8FhtKyZISrZZv0LvckucCB4nyk1Y8nXxSe0mERH5e/bLfXj92xL87j87vcvqWpzYeKgaR2pa0NDmgs1sws9mDMbiGwrxyOVjAACJdgvOH5XlfUyM1YxXbz0TL900EVmJMZ3bGTnAYAHAGETTD0EKJ4hmDRLQs3RyH4mIugmncxJR76IPnOlLOwHgJ8uA6oPAwLMAU5DpULYEIG1I8O3bE423B0wBHjwGmAP/OR2QHocB6QNw3ZkDAJwO4GksBURgo/YeUf6gK4EwnX49MPISzI5JQc279cCOJfjlkKM4fdhI/G3lfgxxi0y6dTUJKDX/GNdY1sPqatL2JdD+XvR7WM//HRorS3Hi9Z+hoGETAOBz1wSM65eM0wefDWz4FOfVvYsnnrJjQ851mDEsAxebNmDcN3fBnTwAptwJItfM0YhD332IQQAcshmD85UD6JQBoq+a2QapYAYeHhaHLSVjgKqDGH78Yy1Rzd2m7ZurVUwX3f+FdrtkncgalGURuPvkAaBe11ctNhW4dxuw7Eat/Pazh4D4TGD4RUh1ih5zrswxQPYcMYH0qHi9CUPPwozcS+BZ/ySy3TWYJO3B8NK3xb6t/iMw4uLgAVRfb98sSmJv/xzY+i9gy2vAbSuAnLHeVRobGxEDwGOOQbxd+f3odwZQvAo/tXyCGCUQ+eqby1Az5HK8+tMzYTK110imA9a/AKz9q3jvxl8LXPG8CAJXFImS4BEXh7WZstomXGI6qO27Sgk8J0uN2L3xSzjK7obt7Pli0AcREZ2UmiYHXv9WfN7vq2hEaXUzzCYJ1/59PUqqm5VjCmBYdgJsFnE8c/O0QRiTl4R4uwVxNuMxycRBJ9nA31DOqTt+0pdz6oNfHv8TS3705Z8AcNr1wNbXgXN+3bl9JCLqJsxEI6LeJS1IOScgeqQVnG08AASAmfcDwy8WmTbnPuR/fyhBAmjtkiQgdaAIRvgOMYhNBSQJqeMvBQAMrVmDn59VgO9uiEOeVA2nJR6Xzf4B/nHvD2A9937xmJSB/uPm9Sw2JOQMQcEv3oQzaQCa7FnIOe0i/Onq8Rh10c9QN/hSWCU3fmt5DZajG/DcVwdQtlqUbprrSiDt/tC7qaYVTwAAyq35SIhRes4pwwUwcBpgi4fJJKFw8kwAQH/JvxTFDVEOsnPDV/Ac+Eq748CX4vKLR4G3bhRBIJNFDIiwJQAtNcC7vxABNGscMPIysf4HdwEHxXaOyulISE4Hxl1tOGM+espFuOPCcTANEEMPlsQ+jRjJKe70uES5qivA2fMDXwJv3QwUr1HegCrgwFfiMV8vAjYtBZzNwHd/NzysqUlkolnsurPt+WcCAC63fOtddLppH9bsq8RLXx/0f+5wtNaJQJ5LCVCWbQM+uV8LPm5/E/jgbhGYfP3HwBvXAKUbwtt02R4kSi1wSHYgc5R2h/LlZ1ZqGRab/gBb+WbInzwYMCOPiIg6ZunaQ2h2aBnK/950xBtAA4BlG0SAbXRukuFxEwelYZTPspNWvgP44B5x3Xfqpj4TzWKH94xZOCek7D77+YNngLnfANPu6fSuEhF1B2aiEVHvos8+Sy0IuprBoOnip6cpOBuwJwONx4HSb2He/iYAwDruSvxkxmixTuo8ABLQf2J420zIgvWub2EF8H82LbiTfOPrwL9vBXa+iz+O2Isl1tNx7r5tATcxxnQIAJA1+cfawmGzgO3LvBNBAQA544LuxpfuCbjQvBnub/8Ok0nrqXVg/Yd4qygLD9Q8BQD4KutGfJJ2I5ySHZe2fojzi58E9v5XrDvgauwb82ucVb4fCbW7ceLzZ5AJYI8nH6lxNiAhHRh2EbDnY7FxJYCFcx8CjmxAorMBAPBb5y34pfUdpBzfgW+X3g9ran+kJich/swbkbn6IUgbXxKPK1kH3L1JCdYpJZK73tde1M73RPmsVQyOaGkWQTRbTLy2Tn+xDybdpLNLU0vxyAngseW7sWZfJa6fPBAXjMqCxRwgmOtsAb5+GsgaBYz5kQiM/fs2YP8K8UXn4sdFYA8ARv0AGPVD4L1fiLP7/Qq1noC7PwTyJwXevsnqDQw7SkSwrTJxFPL0wWKl981VTW96vzNJ9Ufg2vk+LOOv8t8uERGFxeHy4J/rxRChwoGp2HS4Bn/5Qgz+SYmzorbZCY/yETQ6r4sDZoEs/xVQslZcl31aD/hmov36MNDWCCRkISTfTDSzxZDNTUTUUzGIRkS9S1w6kD0OaK4CMoZFem9OjsUGjJgtMom2vaEFbE67XlvHbAWmd/CsrS1AHxJJAiZcB+x8F0Mrv8BjZ58B7HMBKQMg1x2BJHsg554OqWyL9yExp+mCJeOuAkZfYczKyx5jfI6EHKCxHC5LPGwjLgX2bMZ4k+in9ZV7AmaatmOI5xDurH4ckIBXXBfhkZLZQInIZFuBkVhvtyNeaoNLNuGmnWfg6M4t+I2lALdbdiOzRvSLq00YihkDlYEPhTeLIFre6doQiAGTgevfBv51LRrj+uHThktQ25KAv9qexeQjSwAlgWv+lwexyPYSPJDQKCUgqfE4/vPXXyIT1ZgW6H1tq8dzi/+CtlFXIinGgiGVNRgJICZOH0TzD3ZmNu3FLWdm49UNx7FmXyXW7KtEWrwN4/snw2Y2we2RkRZvQ//YNvx4/6+RWyvKU4/WtUGy2JG3fwUAwLPhZVQNuBgZu96HBMBz9v1A9lhI+z6FtONt4PNHtCfd8wlw4aPGHancB7x4vgh+3vQ+vitpQFbFN4AZiC0407iu2kcQQGNMLt5rHIMbLZ/j6PsPo2LTekjj5iB36GnIrf8epowhWg9B1a7/iOEe7WVPEhH1cku/KUaTw4155wyBpGSlr9p7AtVNDmQm2vH7H43FxYtEFrTFJOGft07Gra9uwIkGkXnsm4kW1P7PgZZa8VndEfVl4gQSIIJkvp/rydrkT5htfi0q2hWf0bF9ISLqIRhEI6LeRZKAnyuldkpGUFQb9QMRRNv8qridWhC491lXGHyOyDBqPA58qUzKnPQzSJV7gW1vQLrkj8A/fihKF7PGAJkjjI/3LWu1J4qBDtVKmeLpNwBrnoQleyRm/uAmoPpt4MRuAMDkH90Fx7q/IubEdiRJzTiRNBb1Y36LX1piYLOY4JZllFa3YNPhWTi77kNsiD8b+f1GINvlwfHmM4DG/3qf9vJZF2hZXMMvAq5bBmQON+7boLOAhbuQYLFjrWTFN/tPx4EVOzGkUist/YP1RQDACnch/u0+Gy/ansLFDe+gDTZAArZ6BuM000E4ZDPecJ+Hmy0rcHblG7j+iwJIAD6ybQRMQEyS7otCXBqQPhSo2i+yDK0xkBqP4+FCB24751y8/m0J3tpYiuomB1buOQFATPP8X+truNS0HrGSA25ZglmSkfHpnXDACkiAUzbD6m5D6rIrIEkefOk+Dbf+pQRACS4wDcBLNgBtddp+VO7Bqt9fhkHuQ1iWdAtWmSbjN3WPYKq7Djj8NVa++D94r24Y/mIW2QepE3VZh4DWR9CehIRb30NGsRNt/12FgZ5SDDz8IoqLP8Bf3T/AE9aXsFsajPmJf0ai3YykuFichiLMO7wATeZkJN3zNZDcD0REfc2KXcfx8IdiKrYkAfPOGQoAeHezOJNzxWl5GJGdiIHpcThc1Yy7zxuGcf2TcdGYbLy2XmQVjwonE62hHPjXtYDHKVov5J8ZeL2d7wF7PwVm/R+w+2MxWChzOABZZFHf9L5/OWdirnY93Kmas/4PWL8YuOix8NYnIuphGEQjot7HbBU/vcHQ80UvMEejOLs7+4/+PdS6isUGjLpMNMlvrQOSB4ist5hkcdAbmwIMmAoc+AIY+6PwtpkzTgTRkgcAU+4AyrcDhbeIUo871okyxPpjiDvtKiBjALDln8DQC5A54lLca7H5b69tMbDlLEydcA2mqpllTSOAP+mmo+b6lIMEa6IfI758WADMHJEFDH4D2PNf8X7/62rYlX5pBRfdhZuypqNi5TZkHfsCdrjgNMXg4LQ/Y8SWuagffBlmnvELuN9ch3HOQ1iV+L9oMcUir+0Eau15GHvxbcbn7X+mCKINmCLe86IPgU1LkT/lDvx61jgsuHAYdu/bD8t3iyFLJuQf/xLJTSJjr8w2EIuS7sMVNa9iqnsj7HChDOn4i+lGPCEvgkXy4KAnB4+4bvI+3WrPeNTLcUiSRC+dA55cDDGVYaZTZDf8qvb/YZZnCE4zHYBHlmCSZMwoW4ozEAtIQMu46xHrW/o54VqRuTb5F0DWKMzOAhosi3Fk8wfILF+NAhzHEyZRBjtSPoibq5/BpeZvcVxORYrUALPkxlrPaFzMTDQi6mOqGtuw53gDHnxvh3fZnz7dg4wEO84dkYUviioAAFee0R+SJOHZ687A1tIa7zCBS8fl4bX1JRialYCkmDCOdTYuEQE0QEzBzv+Hdl99GdBYDsRlAO/dAbhaRGuAil1iIqdaxjnmR4At3n/btjjgrAVAU6XWGzWUaXeLHyKiKCXJsixHeie6U319PZKTk1FXV4ekpG7oI0BEdLIOfCUOak+/wb8srquVfAssuUiUP173JpCYbby/cp8oK516V3iZfmv+LIYEjLgEuO6NU7LLAIBnzwQq94iz5A+VKQ2OT8KL54mpnikDgHu2iWETLgfw8QIRZBxzJXD1K8bHlO8A3rgOqCsVty2xwO0r/HvDHdkIvH+H6J9WsRv49AHtPmu8KPk8/r0oSVYl5gFXvSyCmJIkeqEd2yImmmaMEL8XX/0esrMN9WcugMeWAMDbuQ1xy+9CzM5lcKcOQe2Ym5D+9e8gS2ZUDLwUmYc/9vZoOzjkRjhamjDy2LsAAGdMBqz3bOjY7932t4F3bwcAeKzxMDmb/FapThyBdee8gUsL25mE20n8nKdThb9bfUur040jNS1odbrh9shocriw61g9PLKMC0fnYO/xBhypacGg9DhYzCbUNjtwpKYFQzLjMX1oBkqqm7HhwHGUHC1DiyUZdW1uHDzRhN3lDd7nGJIZj4uy61Bf9BWKPAOwTRoOt0fGhVkN+PvVw0T/S3uC+Pyp2i9O/iT1w6e7jmNQejxGJLvEhGt7osgmlz3A5n8Ch1YDg88Vwa/nzgSaRGYzJBNw10bA4wY2vCgG47gdQFJ/4yRsQPTHVINvC3Yxa5iIer1wP+cZRCMiIqOG46JXickcet1QWmpFaWjhT09tw+AP7xVfBjJHAnd+G3L1kPZ/Abx9iwh0nfYTbbksi+mX6UPFFxtfLTXAvs+B5kpRMtOvsP3ncbYA370IHFojApj6ksvscUC/08WXnpn3n1z/sLLtwBvXAmffJ6aWfvZbEdgcPguoLQG+f0eU/Jz7kPgydmKPWJ41MvzsApUsA/+6RvTRuek/wLs/B6r2icBj1migfBsw6/diOu0pwM95OlVO5e/WxpfuwYCjH0OCDAkyTOhTh+d+2nv1UpjvjeyzoXAeJwMQX40kADLUa50hQUYKGmGWZLTJFjQjxnufSZJgNgGxVhPMur/7DtkMSCbY4PRuBbYEwN0mgl2AMvzFJqZPO5tE+wp1uRr08pXUT3xuFa9qZ4dNwMxfA6v+INo1zHlJfA72PxO44rlOvgtERNGDQbQgeHBNRNQL7f8ceG0OMPkOYPYTkd6bzvF4RI+4krXiy9H4awFrTOjH9UQeN+B2iv1vqgSObhalyV0RmA2Bn/N0qpzK363vnrkBZ1Z/2KXbpChhsgD5k+Ep2w6TQ8lSs8SKrLPG49p6tkRRbqkGzVSZo8QJnMZycTs5HxhzBbDzfS0z+rKngZzx4gRHc6XI2h52oWizIHuAzx8WJ1im3Q3UHQHiM08+o5uIKMpETRDt+eefx5/+9CeUlZVhzJgxWLRoEWbMmBF0/VWrVmHhwoXYuXMn8vLy8Ktf/Qpz584N+/l4cE1E1EtVHRBn26M18ERdgp/zdKqcyt+tQ/t3oa6qHDJMSt9LCXKH+1+eon6Zp0z7eV6SzxX1tiQBMiTdbcm73PA4ny2Jt1X3OHW5brVYqwlxdgscLg9irGakxlnFej7/Fq1ON+wWk/e59dweGQ2tTiTHWiHFpYty+IZykXkcSEKW6Dnqcoigmcclpl6arUBTFdBaKwJtKQPEfeo6HrfISEvJF9drS0TfsrgM0YJAlgFHk9h3tZ+ZLIuAmyRpE6uJiAhA+J/zER0ssGzZMsyfPx/PP/88pk+fjr/97W+YPXs2du3ahQED/MtHiouLcckll+BnP/sZXnvtNXzzzTeYN28eMjMzMWfOnAi8AiIi6jHSu76/FhFRdxg0dDQwdHSkd4PC1N6pGjOAFN+FKfmhN2qx+a8Xny5+vBu3igCbL5MZSCswLpMk/7YDknTqe6sSEfVyEc1Emzx5Ms444wwsXrzYu2zUqFG44oor8Pjjj/utf//99+ODDz5AUVGRd9ncuXOxbds2rFu3Lqzn5BlqIiKi3ouf83Sq8HeLiIio9wr3c97Ujftk4HA4sGnTJsyaNcuwfNasWVi7dm3Ax6xbt85v/YsuuggbN26E0xm4kWZbWxvq6+sNP0RERERERERERB0RsSBaZWUl3G43srOzDcuzs7NRXl4e8DHl5eUB13e5XKisrAz4mMcffxzJycnen/z8MNKpiYiIiIiIiIiIdCIWRFP5NuSUZTlgk8721g+0XPXAAw+grq7O+1NaWnqSe0xERERERERERH1NxAYLZGRkwGw2+2WdVVRU+GWbqXJycgKub7FYkJ6eHvAxdrsddjtHNBMRERERERERUedFLBPNZrOhsLAQK1asMCxfsWIFpk2bFvAxU6dO9Vv/s88+w8SJE2G1Wk/ZvhIRERERERERUd8W0XLOhQsX4qWXXsKSJUtQVFSEBQsWoKSkBHPnzgUgSjFvuukm7/pz587F4cOHsXDhQhQVFWHJkiV4+eWXcd9990XqJRARERERERERUR8QsXJOALjmmmtQVVWFRx99FGVlZRg7diyWL1+OgQMHAgDKyspQUlLiXb+goADLly/HggUL8NxzzyEvLw/PPPMM5syZE6mXQEREREREREREfYAkq535+4j6+nokJyejrq4OSUlJkd4dIiIi6kL8nKdThb9bREREvVe4n/MRn85JRERERERERETU0zGIRkREREREREREFAKDaERERERERERERCEwiEZERERERERERBQCg2hEREREREREREQhWCK9A91NHUZaX18f4T0hIiKirqZ+vvex4ePUDXgMSURE1HuFewzZ54JoDQ0NAID8/PwI7wkRERGdKg0NDUhOTo70blAvwmNIIiKi3i/UMaQk97FTtR6PB8eOHUNiYiIkSerSbdfX1yM/Px+lpaVISkrq0m33VnzPOo7vWcfw/eo4vmcdw/er407leybLMhoaGpCXlweTiV0rqOvwGLJn4XvWcXzPOobvV8fxPesYvl8d1xOOIftcJprJZEL//v1P6XMkJSXxP0EH8T3rOL5nHcP3q+P4nnUM36+OO1XvGTPQ6FTgMWTPxPes4/iedQzfr47je9YxfL86LpLHkDxFS0REREREREREFAKDaERERERERERERCEwiNaF7HY7fve738Fut0d6V6IG37OO43vWMXy/Oo7vWcfw/eo4vmdERvw/0XF8zzqO71nH8P3qOL5nHcP3q+N6wnvW5wYLEBERERERERERdRQz0YiIiIiIiIiIiEJgEI2IiIiIiIiIiCgEBtGIiIiIiIiIiIhCYBCNiIiIiIiIiIgoBAbRutDzzz+PgoICxMTEoLCwEGvWrIn0LvUIDz/8MCRJMvzk5OR475dlGQ8//DDy8vIQGxuLc845Bzt37ozgHne/1atX4wc/+AHy8vIgSRLef/99w/3hvEdtbW24++67kZGRgfj4eFx++eU4cuRIN76K7hPq/brlllv8fuemTJliWKcvvV+PP/44Jk2ahMTERGRlZeGKK67Anj17DOvwd8wonPeMv2dGixcvxvjx45GUlISkpCRMnToV//3vf73383eMKDgeQwbGY8jQeAzZMTyG7BgeQ3YcjyE7LtqOIRlE6yLLli3D/Pnz8dBDD2HLli2YMWMGZs+ejZKSkkjvWo8wZswYlJWVeX927Njhve+Pf/wjnnrqKTz77LPYsGEDcnJycOGFF6KhoSGCe9y9mpqaMGHCBDz77LMB7w/nPZo/fz7ee+89vPnmm/j666/R2NiIyy67DG63u7teRrcJ9X4BwMUXX2z4nVu+fLnh/r70fq1atQp33nkn1q9fjxUrVsDlcmHWrFloamryrsPfMaNw3jOAv2d6/fv3xxNPPIGNGzdi48aNOO+88/DDH/7Qe5DD3zGiwHgM2T4eQ7aPx5Adw2PIjuExZMfxGLLjou4YUqYuceaZZ8pz5841LBs5cqT861//OkJ71HP87ne/kydMmBDwPo/HI+fk5MhPPPGEd1lra6ucnJwsv/DCC920hz0LAPm9997z3g7nPaqtrZWtVqv85ptvetc5evSobDKZ5E8++aTb9j0SfN8vWZblm2++Wf7hD38Y9DF9+f2SZVmuqKiQAcirVq2SZZm/Y+Hwfc9kmb9n4UhNTZVfeukl/o4RtYPHkMHxGLJjeAzZMTyG7DgeQ3YcjyE7pycfQzITrQs4HA5s2rQJs2bNMiyfNWsW1q5dG6G96ln27duHvLw8FBQU4Nprr8XBgwcBAMXFxSgvLze8d3a7HTNnzuR7pwjnPdq0aROcTqdhnby8PIwdO7bPvo8rV65EVlYWhg8fjp/97GeoqKjw3tfX36+6ujoAQFpaGgD+joXD9z1T8fcsMLfbjTfffBNNTU2YOnUqf8eIguAxZGg8huw8/u3tHH62B8djyI7jMWTHRMMxJINoXaCyshJutxvZ2dmG5dnZ2SgvL4/QXvUckydPxj/+8Q98+umnePHFF1FeXo5p06ahqqrK+/7wvQsunPeovLwcNpsNqampQdfpS2bPno3XX38dX375Jf785z9jw4YNOO+889DW1gagb79fsixj4cKFOOusszB27FgA/B0LJdB7BvD3LJAdO3YgISEBdrsdc+fOxXvvvYfRo0fzd4woCB5Dto/HkCeHf3s7jp/twfEYsuN4DBm+aDqGtHT5FvswSZIMt2VZ9lvWF82ePdt7fdy4cZg6dSqGDBmCV1991dtAke9daJ15j/rq+3jNNdd4r48dOxYTJ07EwIED8fHHH+PKK68M+ri+8H7ddddd2L59O77++mu/+/g7Fliw94y/Z/5GjBiBrVu3ora2Fu+88w5uvvlmrFq1yns/f8eIAuNxUGA8huwa/NsbPn62B8djyI7jMWT4oukYkploXSAjIwNms9kvyllRUeEXMSUgPj4e48aNw759+7wTlvjeBRfOe5STkwOHw4Gampqg6/Rlubm5GDhwIPbt2weg775fd999Nz744AN89dVX6N+/v3c5f8eCC/aeBcLfM8Bms2Ho0KGYOHEiHn/8cUyYMAF/+ctf+DtGFASPITuGx5Adw7+9J4+f7QKPITuOx5AdE03HkAyidQGbzYbCwkKsWLHCsHzFihWYNm1ahPaq52pra0NRURFyc3NRUFCAnJwcw3vncDiwatUqvneKcN6jwsJCWK1WwzplZWX4/vvv+T4CqKqqQmlpKXJzcwH0vfdLlmXcddddePfdd/Hll1+ioKDAcD9/x/yFes8C6eu/Z4HIsoy2tjb+jhEFwWPIjuExZMfwb+/J6+uf7TyG7DgeQ3aNHn0M2eWjCvqoN998U7ZarfLLL78s79q1S54/f74cHx8vHzp0KNK7FnG//OUv5ZUrV8oHDx6U169fL1922WVyYmKi97154okn5OTkZPndd9+Vd+zYIV933XVybm6uXF9fH+E97z4NDQ3yli1b5C1btsgA5KeeekresmWLfPjwYVmWw3uP5s6dK/fv31/+/PPP5c2bN8vnnXeePGHCBNnlckXqZZ0y7b1fDQ0N8i9/+Ut57dq1cnFxsfzVV1/JU6dOlfv169dn36877rhDTk5OlleuXCmXlZV5f5qbm73r8HfMKNR7xt8zfw888IC8evVqubi4WN6+fbv84IMPyiaTSf7ss89kWebvGFEwPIYMjseQofEYsmN4DNkxPIbsOB5Ddly0HUMyiNaFnnvuOXngwIGyzWaTzzjjDMMY277smmuukXNzc2Wr1Srn5eXJV155pbxz507v/R6PR/7d734n5+TkyHa7XT777LPlHTt2RHCPu99XX30lA/D7ufnmm2VZDu89amlpke+66y45LS1Njo2NlS+77DK5pKQkAq/m1Gvv/WpubpZnzZolZ2ZmylarVR4wYIB88803+70Xfen9CvReAZBfeeUV7zr8HTMK9Z7x98zfrbfe6v0MzMzMlM8//3zvwY8s83eMqD08hgyMx5Ch8RiyY3gM2TE8huw4HkN2XLQdQ0qyLMtdn99GRERERERERETUe7AnGhERERERERERUQgMohEREREREREREYXAIBoREREREREREVEIDKIRERERERERERGFwCAaERERERERERFRCAyiERERERERERERhcAgGhERERERERERUQgMohERBSBJEt5///1I7wYRERERRREeQxL1bgyiEVGPc8stt0CSJL+fiy++ONK7RkREREQ9FI8hiehUs0R6B4iIArn44ovxyiuvGJbZ7fYI7Q0RERERRQMeQxLRqcRMNCLqkex2O3Jycgw/qampAESa/OLFizF79mzExsaioKAAb7/9tuHxO3bswHnnnYfY2Fikp6fj5z//ORobGw3rLFmyBGPGjIHdbkdubi7uuusuw/2VlZX40Y9+hLi4OAwbNgwffPDBqX3RRERERHRSeAxJRKcSg2hEFJV++9vfYs6cOdi2bRtuuOEGXHfddSgqKgIANDc34+KLL0Zqaio2bNiAt99+G59//rnhAGfx4sW488478fOf/xw7duzABx98gKFDhxqe45FHHsGPf/xjbN++HZdccgmuv/56VFdXd+vrJCIiIqKuw2NIIjopMhFRD3PzzTfLZrNZjo+PN/w8+uijsizLMgB57ty5hsdMnjxZvuOOO2RZluW///3vcmpqqtzY2Oi9/+OPP5ZNJpNcXl4uy7Is5+XlyQ899FDQfQAg/+Y3v/HebmxslCVJkv/73/922eskIiIioq7DY0giOtXYE42IeqRzzz0XixcvNixLS0vzXp86darhvqlTp2Lr1q0AgKKiIkyYMAHx8fHe+6dPnw6Px4M9e/ZAkiQcO3YM559/frv7MH78eO/1+Ph4JCYmoqKiorMviYiIiIhOMR5DEtGpxCAaEfVI8fHxfqnxoUiSBACQZdl7PdA6sbGxYW3ParX6Pdbj8XRon4iIiIio+/AYkohOJfZEI6KotH79er/bI0eOBACMHj0aW7duRVNTk/f+b775BiaTCcOHD0diYiIGDRqEL774olv3mYiIiIgii8eQRHQymIlGRD1SW1sbysvLDcssFgsyMjIAAG+//TYmTpyIs846C6+//jq+++47vPzyywCA66+/Hr/73e9w88034+GHH8aJEydw991348Ybb0R2djYA4OGHH8bcuXORlZWF2bNno6GhAd988w3uvvvu7n2hRERERNRleAxJRKcSg2hE1CN98sknyM3NNSwbMWIEdu/eDUBMPXrzzTcxb9485OTk4PXXX8fo0aMBAHFxcfj0009x7733YtKkSYiLi8OcOXPw1FNPebd18803o7W1FU8//TTuu+8+ZGRk4Kqrruq+F0hEREREXY7HkER0KkmyLMuR3gkioo6QJAnvvfcerrjiikjvChERERFFCR5DEtHJYk80IiIiIiIiIiKiEBhEIyIiIiIiIiIiCoHlnERERERERERERCEwE42IiIiIiIiIiCgEBtGIiIiIiIiIiIhCYBCNiIiIiIiIiIgoBAbRiIiIiIiIiIiIQmAQjYiIiIiIiIiIKAQG0YiIiIiIiIiIiEJgEI2IiIiIiIiIiCgEBtGIiIiIiIiIiIhCYBCNiIiIiIiIiIgohP8PHHGaeNnOUgwAAAAASUVORK5CYII="},"index_0_class_0_preds.png":{"image/png":"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"},"index_0_class_0_targets.png":{"image/png":"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"},"index_0_class_12_preds.png":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAOAAAADgCAIAAACVT/22AAABZklEQVR4nO3ZQY6DMAxA0bqa+185XSBVDAUm000S+71VK7FwxMeo6uMBAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAn2L0AJxore2/RtS9TXVPPq1DnVeKVFvikAvprPMgIlprKZN9jh6AXyLiX52ljHJPoDP6OrvvFvDMkj9/q7sJLv3u3NigUytSIRnke30DAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAQBkxegD+1lrbPkSUu1/lDrycd52n0if7HD0Ad+7r7LlgdT+jB+Bc+vI62aBMTaDLy71rBTqv9D+Aegh0UurcCHRqnZkmfssLdHb7Rq96TbxuBbqSxJvyStonL41DlO9lWeT/z8xnS6NIiwAAAAAAAAAAAAAAAAAAAAAAAAAAAABJvAB2BjYsZkPFCAAAAABJRU5ErkJggg=="},"index_0_class_12_targets.png":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAOAAAADgCAIAAACVT/22AAAFb0lEQVR4nO3d2ZLjNgwFUDs1///LzoMnXR3LliWKC0Ce85apVJcIXIGSF/l2AwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAPng8Ho/HY/RRNHcffQCUeInm/T5tH6dd2Kx2puaUMf0z+gCq2XZuvoatsKe/mKGFR9qWOqyncpl6pVu5F1MwUbL07+KwzLLMrxIv4/p+F6eL1ffuOEu7KOUy6razRS8jXCzOkdF8a+jT+9/djZC2MhNkNNkC8mZloNQx/Wf0AZwgnWVS1y3NuZW6ykFkHKU5jlg6K8oV0wTHKp0tZIlppmtQKsryYSgBXVr8jCYIaJbNKKngGU0QUFqLnNE0wylyEacRcLNKM0ED1o4OcnfdWG0h1CxIM0G3pLORUIXNGtBQRZxPnPKmDGic8tFavoBKZx9B6pwsoEGqtogI1c4U0Aj1orNMAaW/4UMhTUCHV2pZYyufI6DSOdbA+ucIKMONymiCgBqfK4seUOmMY0gvogeUOIZ8iCR0QI1PQgeUOEZ9Bk9ACU1ACS1uQF2AcgsbUOnkKWJApTMg7yTBG4G+v3czO8Pr/2JToAkqnfH171GUgEonb0UJKLw1/qcQzU52DJ6g0sm+kQGVTr5yDUpoY65BzU4OGjBBpZPjegdUOrPr3EHXoJzWM6NdA2p8TqNbK/sFVDon06ehtnjKdchop4Aan5TpEVDppFjzgErn3Fr31zUoobUNqPHJRQ0DKp2LaNpoWzyhtQqo8UkVJihXNf0ucpOAGp/UYoJySetHOQgoodUPqP2dikxQQqscUONzKR2eJWaCEpqAEpqAElrNgLoAXUqfh9mOf7od6fR8zrItnnM6PwXcBOUoPyYLrwSU0ASU0ASU0ASUQ/xePHGNSuet+k8hejMpo2f+3vZuYDT/HkDFvyWdGW0j+Ozj8Gg+VTsI6UwqSBA/qXBwoplX8HTert8kSSdNlb8XL5p0UDhBpXMO8ftYEtCvq4p/ZUMWp7f4/XSKJnUdzdORveAnnfE3Dn6LPFY+HtmpkL2sUEDTCZvR/23xBcEKuzCOi9zEvwEtm3lvF2Z8UlH5C/WRTzumURhQ6aSPkoBK50yCd/N0QIOvh8mceKFeNOcTv6dHJ2j8lXBWip7+uXn3cg1J+/hxi0+6nvg6vyGcvY9vinVxSV6of+tTVRuVK3sufzRZRreiBz8Zjqek4kKmieZTjqfbfSr6zvdlR+mQj8kiuC9HQPcF+ZhfcW5CnWDRtDoXw+5ZOwc2ZDJV/JjOlBqus0pGp++EgO4L/WymddpwylJliRvQpdrAJw0DKmEtrFbViHfxi/TAF2yOaDhBvXrCdeGuQRccEuyIFVDp5EWsgC7F2XhEq5uksxegusVbTSZowe2RO6ojFqxS/YAuWMQyCnVE5YAqemurVbhmQK/UbrVr0NVyVqxaQKXzuIvpXCrc419mks5RfySFOgFdp14XVSzUIjUf+QXO1WbnCx/oPqJ8gi5yBgc3fRcKf+XDhVQcc5fxdEDrlmPu4nYzcRnPBXTiQmQ3a2uOBrTWts5Ti2JO2aBDAf1Zed17xvv9Pv1N6Ce1Fj79DwCdeD7osxbXK3v/z8W/M5+ymsxdyUMBrViCuat5xc/5X1Ciias6+C6e4/ZTWGt/i+ZEQK/fJ81Xvs6287Xig11jOnqT9CzElcvHKctXy0txTtVq7j3tS0B/T83fVZO26yqe6hNntPBXPl4qsvOAWVHe97U+b/+HKxM3l9N38dsr0be3n9v/vHqk8zqY0YnH5I5zudnW6CW723+kzLaYj8djwcKeu4t/+Zcjuw9ltldHCnvJmhsQAAAAAAAAAAAAAAAAAAAAAAAAAAAAMMS/W7sRD1ven6wAAAAASUVORK5CYII="}},"cell_type":"markdown","metadata":{},"source":["![Training.png](attachment:Training.png)\n","\n","Figure: Cached plots of the training run loss and IoU values\n","\n","Loss values obtained during training of the model decrease over 300 epochs from values around 0.9 to values around 0.004. This is mainly due to the model being progressively better at \"not responding\" (since the majority of the target's pixels are 0) and differenciating the background from other foreground elements, which is the class that most positive (value of 1) pixels contains (see figures below for an example). \n","\n","![index_0_class_0_preds.png](attachment:index_0_class_0_preds.png) \n","![index_0_class_0_targets.png](attachment:index_0_class_0_targets.png)\n","\n","Figure: Prediction (top), ground truth (bottom) after 170 epochs. Class: background.\n","\n","![index_0_class_12_preds.png](attachment:index_0_class_12_preds.png) \n","![index_0_class_12_targets.png](attachment:index_0_class_12_targets.png)\n","\n","Figure: Prediction (top), ground truth (bottom) after 170 epochs. Class: Frog.\n","\n","As for IoU, around 80 epochs in the training we can observe an increase of training IoU while Validation IoU starts fluctuating at 110 epochs. However, validation IoU imrpoves more slowly after 170 epochs, denoting the point where training the model further would not yield better results. Therefore we conclude that for this run, the model reached its peak performance after 170-175 epochs.\n","\n","Performance-wise, the model showed poor results (as depicted by the low IoU value). This could be due to a number of reasons that we enumerate in the following bullet point:\n","- Large Learning rate: training stagnation could be solved by tranining in phases with progressively smaller tranining rate to avoid overshooting better results.\n","- Insufficient weight to positive values.\n","- Small image database for training. \n","- Need for splitting the Encoder and Decoder. Training them separately can improve performance."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-24T19:46:16.274769Z","iopub.status.busy":"2023-05-24T19:46:16.274186Z","iopub.status.idle":"2023-05-24T19:46:16.940304Z","shell.execute_reply":"2023-05-24T19:46:16.938756Z","shell.execute_reply.started":"2023-05-24T19:46:16.274706Z"},"trusted":true},"outputs":[],"source":["# Code to use the pre-trained model\n","\n","# Create instances of ImageDataset for test dataset\n","test_dataset = ImageDataset(test_df['img'].to_list(), test_df['seg'].to_numpy(), transform=None, target_transform=None)\n","Nimages = test_dataset.__len__()\n","print(\"Number of images is \", Nimages)\n","\n","# Initialize model with the best available weights\n","weights = FCN_ResNet50_Weights.DEFAULT\n","model = fcn_resnet50(weights=weights)\n","model.eval() # set the model to evaluation to avoid saving gradients\n","model.to(DEVICE)\n","\n","# Initialize the inference transforms (resize + transforms such as normalization)\n","preprocess = weights.transforms().to(DEVICE)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["The following code uses Resnet50 to classify the images of the test dataframe and adds the results to their corresponding column (warning: takes long)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-24T20:02:13.30661Z","iopub.status.busy":"2023-05-24T20:02:13.305357Z","iopub.status.idle":"2023-05-24T20:02:13.313894Z","shell.execute_reply":"2023-05-24T20:02:13.312485Z","shell.execute_reply.started":"2023-05-24T20:02:13.306559Z"},"trusted":true},"outputs":[],"source":["# for pn in range(Nimages): #Nimages\n","\n","#     im = test_dataset.__getitem__(pn)[0]\n","#     im = np.expand_dims(im,axis=0)\n","#     im = np.swapaxes(im,1,3)\n","#     im = np.swapaxes(im,2,3)\n","#     im = torch.tensor(im).to(DEVICE)\n","    \n","#     with torch.no_grad():\n","#         # Apply inference preprocessing transforms\n","#         im = preprocess(im)\n","#         # Use the model\n","#         prediction = model(im)[\"out\"]\n","#         normalized_masks = prediction.softmax(dim=1)\n","#         for c in range(20):\n","#             test_df[\"seg\"][c] = normalized_masks[:,c,:,:].cpu().numpy()\n","\n","#     print(\"Image #\", pn)\n","\n","#     # See the result only every 100 images\n","#     if pn % 100 == 0:\n","#         class_to_idx = {cls: idx for (idx, cls) in enumerate(weights.meta[\"categories\"])}\n","#         fig, axs = plt.subplots(1, 21, figsize=(5 * 20, 5 * 2))\n","#         for cla in class_to_idx:\n","#             mask = normalized_masks[0, class_to_idx[cla]]\n","#             axs[class_to_idx[cla]].imshow(mask.to('cpu').detach().numpy())\n","#             axs[class_to_idx[cla]].set_title(cla,fontsize=14)\n","        "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-24T20:02:23.847018Z","iopub.status.busy":"2023-05-24T20:02:23.846434Z","iopub.status.idle":"2023-05-24T20:03:04.368093Z","shell.execute_reply":"2023-05-24T20:03:04.366861Z","shell.execute_reply.started":"2023-05-24T20:02:23.846966Z"},"trusted":true},"outputs":[],"source":["# check the head of test_df to see if it correctly set the values\n","\n","# test_df.head(10)\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"papermill":{"duration":0.196901,"end_time":"2022-04-12T14:49:19.951018","exception":false,"start_time":"2022-04-12T14:49:19.754117","status":"completed"},"tags":[]},"source":["## Submit to competition\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-24T20:06:38.793913Z","iopub.status.busy":"2023-05-24T20:06:38.793326Z","iopub.status.idle":"2023-05-24T20:06:38.88698Z","shell.execute_reply":"2023-05-24T20:06:38.883903Z","shell.execute_reply.started":"2023-05-24T20:06:38.793865Z"},"papermill":{"duration":81.176133,"end_time":"2022-04-12T14:50:41.324425","exception":false,"start_time":"2022-04-12T14:49:20.148292","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["# generate_submission(test_df)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# 4 Adversarial attack NNs\n","\n","## 4.1. Loading and preprocessing the data"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-24T19:53:37.14141Z","iopub.status.busy":"2023-05-24T19:53:37.14097Z","iopub.status.idle":"2023-05-24T19:53:37.154682Z","shell.execute_reply":"2023-05-24T19:53:37.153348Z","shell.execute_reply.started":"2023-05-24T19:53:37.141373Z"},"trusted":true},"outputs":[],"source":["import torch\n","from torchvision import models, transforms\n","from torch.utils.data import DataLoader, Dataset, random_split\n","import pandas as pd\n","import numpy as np\n","from PIL import Image\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import matplotlib.pyplot as plt\n","import math\n","from sklearn.metrics import f1_score, average_precision_score\n","from sklearn.utils.class_weight import compute_class_weight\n","from sklearn.preprocessing import label_binarize\n","import numpy as np\n","from torch.utils.data import WeightedRandomSampler\n","from sklearn.model_selection import train_test_split\n","from imblearn.over_sampling import SMOTE\n","from imblearn.under_sampling import RandomUnderSampler\n","from imblearn.pipeline import Pipeline\n","import torchvision.transforms as transforms\n","from torchvision.transforms import ToTensor\n","from torchvision import transforms as tt\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import f1_score, accuracy_score\n","from torchvision import models\n","from tqdm.notebook import tqdm\n","\n","\n","# Check if CUDA is available and use GPU if possible\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(\"Using device:\", device)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["- Read the train dataset CSV file into a pandas DataFrame.\n","- Extract the labels and image paths from the DataFrame.\n","- Define the `ImageDataset` class for custom dataset handling.\n","- Define the data transformations using torchvision transforms.\n","- Create an instance of the `ImageDataset` for the train dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-24T19:53:37.158628Z","iopub.status.busy":"2023-05-24T19:53:37.157375Z","iopub.status.idle":"2023-05-24T19:53:39.042322Z","shell.execute_reply":"2023-05-24T19:53:39.040887Z","shell.execute_reply.started":"2023-05-24T19:53:37.158584Z"},"trusted":true},"outputs":[],"source":["# Loading the training data\n","train_df = pd.read_csv('/kaggle/input/kul-h02a5a-computer-vision-ga2-2023/train/train_set.csv', index_col=\"Id\")\n","labels = train_df.columns\n","train_df[\"img\"] = [np.load('/kaggle/input/kul-h02a5a-computer-vision-ga2-2023/train/img/train_{}.npy'.format(idx)) for idx, _ in train_df.iterrows()]\n","train_df[\"seg\"] = [np.load('/kaggle/input/kul-h02a5a-computer-vision-ga2-2023/train/seg/train_{}.npy'.format(idx)) for idx, _ in train_df.iterrows()]\n","print(\"The training set contains {} examples.\".format(len(train_df)))\n","\n","\n","# ImageDataset class\n","class ImageDataset(Dataset):\n","    def __init__(self, images, labels=None, transform=None):\n","        self.images = images\n","        self.labels = labels\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.images)\n","\n","    def __getitem__(self, idx):\n","        image = self.images[idx]\n","        if self.transform:\n","            image = Image.fromarray(image)\n","            image = self.transform(image)\n","        else:\n","            image = torch.from_numpy(image).cpu()  # Ensure image is a Tensor and is on the CPU\n","        \n","        if self.labels is not None:\n","            label = self.labels[idx]\n","            # Convert label to binary format\n","            label_binary = torch.tensor(label, dtype=torch.float32)\n","            return image, label_binary\n","        else:\n","            return image"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-24T19:53:39.04601Z","iopub.status.busy":"2023-05-24T19:53:39.044806Z","iopub.status.idle":"2023-05-24T19:53:42.100754Z","shell.execute_reply":"2023-05-24T19:53:42.099397Z","shell.execute_reply.started":"2023-05-24T19:53:39.045942Z"},"trusted":true},"outputs":[],"source":["# Calculate mean and std\n","train_dataset_for_stats = ImageDataset(train_df['img'].to_list(), train_df[labels].to_numpy(), transform=ToTensor())\n","meanRGB = [np.mean(x[0].numpy(), axis=(1, 2)) for x in train_dataset_for_stats]\n","stdRGB = [np.std(x[0].numpy(), axis=(1, 2)) for x in train_dataset_for_stats]\n","\n","\n","meanR=np.mean([m[0] for m in meanRGB])\n","meanG=np.mean([m[1] for m in meanRGB])\n","meanB=np.mean([m[2] for m in meanRGB])\n","\n","stdR=np.mean([s[0] for s in stdRGB])\n","stdG=np.mean([s[1] for s in stdRGB])\n","stdB=np.mean([s[2] for s in stdRGB])\n","\n","print(meanR,meanG,meanB)\n","print(stdR,stdG,stdB)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-24T19:53:42.105143Z","iopub.status.busy":"2023-05-24T19:53:42.102619Z","iopub.status.idle":"2023-05-24T19:53:42.123218Z","shell.execute_reply":"2023-05-24T19:53:42.121991Z","shell.execute_reply.started":"2023-05-24T19:53:42.105094Z"},"trusted":true},"outputs":[],"source":["# Data transforms (normalization & data augmentation)\n","stats = ((meanR, meanG, meanB), (stdR, stdG, stdB))\n","\n","# Data transforms\n","train_tfms = tt.Compose([\n","    tt.Resize((300, 300)),\n","    tt.RandomChoice([\n","        tt.ColorJitter(brightness=(0.80, 1.20)),\n","        tt.RandomGrayscale(p=0.25)\n","    ]),\n","    tt.RandomHorizontalFlip(p=0.25),\n","    tt.RandomRotation(25),\n","    tt.ToTensor(),\n","    tt.Normalize(*stats, inplace=True)\n","])\n","\n","valid_tfms = tt.Compose([\n","    tt.Resize(330),\n","    tt.CenterCrop(300),\n","    tt.ToTensor(),\n","    tt.Normalize(*stats)\n","])\n","\n","\n","# Split the data into train and validation sets\n","train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42)\n","\n","# Create instances of ImageDataset for the train and validation datasets\n","train_dataset = ImageDataset(train_df['img'].to_list(), train_df[labels].to_numpy(), transform=train_tfms)\n","val_dataset = ImageDataset(val_df['img'].to_list(), val_df[labels].to_numpy(), transform=valid_tfms)\n","\n","# Create DataLoaders for train_dataset and val_dataset\n","train_dl = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=2, pin_memory=True)\n","val_dl = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=2, pin_memory=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-24T19:53:42.128564Z","iopub.status.busy":"2023-05-24T19:53:42.127772Z","iopub.status.idle":"2023-05-24T19:53:42.149805Z","shell.execute_reply":"2023-05-24T19:53:42.148497Z","shell.execute_reply.started":"2023-05-24T19:53:42.128502Z"},"trusted":true},"outputs":[],"source":["import torch\n","\n","# Calculate the number of labels for each class by summing the values in each column\n","class_counts = train_df[labels].sum(axis=0)\n","\n","def calculate_weights(class_counts):\n","    total_samples = class_counts.sum()\n","    neg_counts = total_samples - class_counts\n","    weights = neg_counts / (class_counts + 1e-5)\n","\n","    return torch.as_tensor(weights, dtype=torch.float)\n","\n","weights_tensor = calculate_weights(class_counts)\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","weights_tensor = weights_tensor.to(device)\n","\n","print(weights_tensor)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-24T19:53:42.152148Z","iopub.status.busy":"2023-05-24T19:53:42.151452Z","iopub.status.idle":"2023-05-24T19:53:42.163287Z","shell.execute_reply":"2023-05-24T19:53:42.161935Z","shell.execute_reply.started":"2023-05-24T19:53:42.152103Z"},"trusted":true},"outputs":[],"source":["def get_available_device():\n","    \"\"\"Returns the default device (GPU if available, else CPU)\"\"\"\n","    if torch.cuda.is_available():\n","        return torch.device('cuda')\n","    else:\n","        return torch.device('cpu')\n","    \n","def to_device(data, device):\n","    \"\"\"Move tensor(s) to the specified device\"\"\"\n","    if isinstance(data, (list, tuple)):\n","        return [to_device(x, device) for x in data]\n","    return data.to(device, non_blocking=True)\n","\n","class DataLoaderDevice():\n","    \"\"\"Wrapper class for a dataloader that moves data to a specified device\"\"\"\n","    def __init__(self, dl, device):\n","        self.dl = dl\n","        self.device = device\n","        \n","    def __iter__(self):\n","        \"\"\"Yield a batch of data after moving it to the device\"\"\"\n","        for b in self.dl: \n","            yield to_device(b, self.device)\n","\n","    def __len__(self):\n","        \"\"\"Returns the number of batches\"\"\"\n","        return len(self.dl)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-24T19:53:42.166287Z","iopub.status.busy":"2023-05-24T19:53:42.165806Z","iopub.status.idle":"2023-05-24T19:53:42.183175Z","shell.execute_reply":"2023-05-24T19:53:42.181858Z","shell.execute_reply.started":"2023-05-24T19:53:42.166244Z"},"trusted":true},"outputs":[],"source":["device = get_available_device()\n","device"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-24T19:53:42.186527Z","iopub.status.busy":"2023-05-24T19:53:42.184967Z","iopub.status.idle":"2023-05-24T19:53:42.19198Z","shell.execute_reply":"2023-05-24T19:53:42.190746Z","shell.execute_reply.started":"2023-05-24T19:53:42.186474Z"},"trusted":true},"outputs":[],"source":["train_dl = DataLoaderDevice(train_dl, device)\n","val_dl = DataLoaderDevice(val_dl, device)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-24T19:53:42.195122Z","iopub.status.busy":"2023-05-24T19:53:42.194351Z","iopub.status.idle":"2023-05-24T19:53:42.214102Z","shell.execute_reply":"2023-05-24T19:53:42.213102Z","shell.execute_reply.started":"2023-05-24T19:53:42.195081Z"},"trusted":true},"outputs":[],"source":["class ClassificationModelBase(nn.Module):\n","    def forward_training(self, batch):\n","        images, labels = batch \n","        out = self(images)  # Generate predictions\n","        loss = F.binary_cross_entropy_with_logits(out, labels, pos_weight=weights_tensor, reduction='sum')                 \n","        return loss\n","\n","    def forward_validation(self, batch):\n","        images, labels = batch\n","        out = self(images)\n","        loss = F.binary_cross_entropy_with_logits(out, labels, pos_weight=weights_tensor, reduction='sum')\n","        \n","        # Convert the output to binary format\n","        predicted_labels = torch.sigmoid(out) > 0.5\n","\n","        # Calculate accuracy and F1 score\n","        accuracy = accuracy_score(labels.cpu().detach().numpy().reshape(-1), predicted_labels.cpu().detach().numpy().reshape(-1))\n","        f1 = f1_score(labels.cpu().detach().numpy().reshape(-1), predicted_labels.cpu().detach().numpy().reshape(-1))\n","        \n","        # average precision\n","        ap = average_precision_score(labels.cpu().detach().numpy().reshape(-1), out.cpu().detach().numpy().reshape(-1))\n","        return {'val_loss': loss.detach(), 'val_accuracy': accuracy, 'val_f1': f1, 'val_ap': ap}\n","\n","\n","    def epoch_validation_summary(self, outputs):\n","        batch_losses = [x['val_loss'] for x in outputs]\n","        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n","        batch_aps = [x['val_ap'] for x in outputs]\n","        epoch_ap = (np.array(batch_aps)).mean()      # Combine average precision scores\n","        batch_accs = [x['val_accuracy'] for x in outputs]\n","        epoch_acc = np.array(batch_accs).mean()  # Calculate mean accuracy\n","        batch_f1s = [x['val_f1'] for x in outputs]\n","        epoch_f1 = np.array(batch_f1s).mean()  # Calculate mean F1 score\n","\n","        return {'val_loss': epoch_loss.item(), 'val_ap': epoch_ap, 'val_accuracy': epoch_acc, 'val_f1': epoch_f1}\n","\n","\n","\n","    def print_epoch_summary(self, epoch, result):\n","        print(\"Epoch [{}],{} train_loss: {:.4f}, val_loss: {:.4f}, val_ap: {:.4f}, val_accuracy: {:.4f}, val_f1: {:.4f}\".format(\n","            epoch, \"last_lr: {:.5f},\".format(result['lrs'][-1]) if 'lrs' in result else '', \n","            result['train_loss'], result['val_loss'], result['val_ap'], result['val_accuracy'], result['val_f1']))\n","            \n","  \n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-24T19:53:42.216996Z","iopub.status.busy":"2023-05-24T19:53:42.215623Z","iopub.status.idle":"2023-05-24T19:53:42.22871Z","shell.execute_reply":"2023-05-24T19:53:42.22778Z","shell.execute_reply.started":"2023-05-24T19:53:42.216952Z"},"trusted":true},"outputs":[],"source":["epochs=[]\n","\n","class CustomScheduler:\n","    def __init__(self, optimizer, base_lr=0.00015, max_lr=0.00017, epochs=epochs, steps_per_epoch=len(train_dl)):\n","        self.optimizer = optimizer\n","        self.base_lr = base_lr\n","        self.max_lr = max_lr\n","        self.total_steps = epochs * steps_per_epoch\n","        self.step_count = 0  # renamed from 'step' to 'step_count'\n","\n","    def step(self):\n","        self.step_count += 1  # updated here as well\n","        lr = self.calculate_lr()\n","        for param_group in self.optimizer.param_groups:\n","            param_group['lr'] = lr\n","\n","    def calculate_lr(self):\n","        return self.base_lr + ((self.max_lr - self.base_lr) / self.total_steps) * self.step_count  # updated here as well\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-24T19:53:42.23212Z","iopub.status.busy":"2023-05-24T19:53:42.231513Z","iopub.status.idle":"2023-05-24T19:53:42.257195Z","shell.execute_reply":"2023-05-24T19:53:42.255952Z","shell.execute_reply.started":"2023-05-24T19:53:42.232078Z"},"trusted":true},"outputs":[],"source":["@torch.no_grad()\n","def evaluate(model, val_loader):\n","    model.eval()\n","    outputs = [model.forward_validation(batch) for batch in val_loader]\n","    return model.epoch_validation_summary(outputs)\n","\n","\n","def fit(epochs, lr, model, train_loader, val_loader, opt_func=torch.optim.SGD):\n","    training_results = []\n","    optimizer = opt_func(model.parameters(), lr)\n","    for epoch in range(epochs):\n","        # Training Phase\n","        model.train()\n","        train_losses = []\n","        for batch in tqdm(train_loader):\n","            loss = model.forward_training(batch)\n","            train_losses.append(loss)\n","            loss.backward()\n","            optimizer.step()\n","            optimizer.zero_grad()\n","        # Validation phase\n","        result = evaluate(model, val_loader)\n","        result['train_loss'] = torch.stack(train_losses).mean().item()\n","        model.print_epoch_summary(epoch, result)\n","        training_results.append(result)\n","    return training_results\n","\n","def get_lr(optimizer):\n","    for param_group in optimizer.param_groups:\n","        return param_group['lr']\n","\n","def fit_one_cycle(epochs, model, train_loader, val_loader, weight_decay=0, grad_clip=None, opt_func=torch.optim.SGD):\n","    torch.cuda.empty_cache()\n","    training_results = []\n","    best_loss = float('inf')  # initialize with high value\n","\n","    # Set up custom optimizer with weight decay\n","    optimizer = opt_func(model.parameters(), max_lr)\n","    \n","    # Set up custom learning rate scheduler\n","    sched = CustomScheduler(optimizer, epochs=epochs, steps_per_epoch=len(train_loader))\n","    \n","    best_ap = 0  # Initialize best_ap with a low value\n","\n","    for epoch in range(epochs):\n","        # Training Phase\n","        model.train()\n","        train_losses = []\n","        train_accs = []\n","        train_f1s = []\n","        lrs = []\n","\n","        for batch in tqdm(train_loader):\n","            loss = model.forward_training(batch)\n","            train_losses.append(loss)\n","            loss.backward()\n","\n","            # Gradient clipping\n","            if grad_clip:\n","                nn.utils.clip_grad_value_(model.parameters(), grad_clip)\n","\n","            optimizer.step()\n","            optimizer.zero_grad()\n","\n","            # Record & update learning rate\n","            lrs.append(get_lr(optimizer))\n","            sched.step()\n","\n","            # Calculate accuracy and F1 score on training data\n","            predicted_labels = torch.sigmoid(model(batch[0].to(device))) > 0.5\n","            train_accuracy = accuracy_score(batch[1].cpu().detach().numpy().reshape(-1), predicted_labels.cpu().detach().numpy().reshape(-1))\n","            train_f1 = f1_score(batch[1].cpu().detach().numpy().reshape(-1), predicted_labels.cpu().detach().numpy().reshape(-1))\n","            train_accs.append(train_accuracy)\n","            train_f1s.append(train_f1)\n","\n","        # Validation phase\n","        result = evaluate(model, val_loader)\n","        result['train_loss'] = torch.stack(train_losses).mean().item()\n","        result['lrs'] = lrs\n","        result['train_accuracy'] = np.mean(train_accs)\n","        result['train_f1'] = np.mean(train_f1s)\n","\n","        # Save the best model\n","        if result['val_ap'] > best_ap:\n","            best_ap = result['val_ap']\n","            torch.save(model.state_dict(), 'best_model.pth')  # save the best model to a file\n","\n","        model.print_epoch_summary(epoch, result)\n","        training_results.append(result)\n","\n","    \n","    return training_results\n","\n","\n","\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-24T19:53:42.262041Z","iopub.status.busy":"2023-05-24T19:53:42.260924Z","iopub.status.idle":"2023-05-24T19:53:42.271479Z","shell.execute_reply":"2023-05-24T19:53:42.270579Z","shell.execute_reply.started":"2023-05-24T19:53:42.261996Z"},"trusted":true},"outputs":[],"source":["class PascalVocModelKUL(ClassificationModelBase):\n","    def __init__(self, num_classes, pretrained=True):\n","        super().__init__()\n","        # Use a pretrained model\n","        self.network = models.resnet34(pretrained=pretrained)\n","        # Replace last layer\n","        num_classes=20\n","        self.network.fc = nn.Linear(self.network.fc.in_features, num_classes)\n","\n","    def forward(self, xb):\n","        return self.network(xb)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-24T19:53:42.274217Z","iopub.status.busy":"2023-05-24T19:53:42.273137Z","iopub.status.idle":"2023-05-24T19:53:42.280983Z","shell.execute_reply":"2023-05-24T19:53:42.279903Z","shell.execute_reply.started":"2023-05-24T19:53:42.274172Z"},"trusted":true},"outputs":[],"source":["# Get the object categories from the label columns\n","def get_categories(df):\n","    return list(df.columns)\n","\n","object_categories = get_categories(train_df)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-24T19:53:42.289554Z","iopub.status.busy":"2023-05-24T19:53:42.28854Z","iopub.status.idle":"2023-05-24T19:53:43.404115Z","shell.execute_reply":"2023-05-24T19:53:43.402739Z","shell.execute_reply.started":"2023-05-24T19:53:42.289512Z"},"trusted":true},"outputs":[],"source":["model = PascalVocModelKUL(len(object_categories))\n","to_device(model, device);"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-24T19:53:43.407123Z","iopub.status.busy":"2023-05-24T19:53:43.40589Z","iopub.status.idle":"2023-05-24T19:53:43.413568Z","shell.execute_reply":"2023-05-24T19:53:43.412284Z","shell.execute_reply.started":"2023-05-24T19:53:43.407075Z"},"trusted":true},"outputs":[],"source":["epochs = 7\n","max_lr = 0.001\n","grad_clip = 0.1\n","weight_decay = 1e-4\n","opt_func = torch.optim.Adam"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-24T19:53:43.416028Z","iopub.status.busy":"2023-05-24T19:53:43.415605Z","iopub.status.idle":"2023-05-24T19:53:43.429222Z","shell.execute_reply":"2023-05-24T19:53:43.427978Z","shell.execute_reply.started":"2023-05-24T19:53:43.415984Z"},"trusted":true},"outputs":[],"source":["def plot_losses(training_results, ax):\n","    losses = [x['val_loss'] for x in training_results]\n","    ax.plot(losses, '-x')\n","    ax.set_xlabel('epoch')\n","    ax.set_ylabel('loss')\n","    ax.set_title('Loss vs. No. of epochs')\n","\n","def plot_avg_precision_scores(training_results, ax):\n","    apscores = [x['val_ap'] for x in training_results]\n","    ax.plot(apscores, '-x')\n","    ax.set_xlabel('epoch')\n","    ax.set_ylabel('avg_precision_score')\n","    ax.set_title('avg_precision_score vs. No. of epochs')\n","    \n","def plot_accuracy(training_results, ax):\n","    accuracies = [x.get('val_accuracy') for x in training_results]\n","    ax.plot(accuracies, '-x')\n","    ax.set_xlabel('epoch')\n","    ax.set_ylabel('accuracy')\n","    ax.set_title('Accuracy vs. No. of epochs')\n","\n","def plot_f1(training_results, ax):\n","    f1_scores = [x.get('val_f1') for x in training_results]\n","    ax.plot(f1_scores, '-x')\n","    ax.set_xlabel('epoch')\n","    ax.set_ylabel('F1 score')\n","    ax.set_title('F1 Score vs. No. of epochs')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-24T19:53:43.433644Z","iopub.status.busy":"2023-05-24T19:53:43.433165Z","iopub.status.idle":"2023-05-24T19:53:43.443217Z","shell.execute_reply":"2023-05-24T19:53:43.442117Z","shell.execute_reply.started":"2023-05-24T19:53:43.433612Z"},"trusted":true},"outputs":[],"source":["training_results = []"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-24T19:53:43.445722Z","iopub.status.busy":"2023-05-24T19:53:43.444917Z","iopub.status.idle":"2023-05-24T19:54:40.722262Z","shell.execute_reply":"2023-05-24T19:54:40.720896Z","shell.execute_reply.started":"2023-05-24T19:53:43.445595Z"},"trusted":true},"outputs":[],"source":["# Training the model\n","training_results = [evaluate(model, val_dl)]\n","\n","training_results += fit_one_cycle(epochs, model, train_dl, val_dl, \n","                         grad_clip=grad_clip, \n","                         weight_decay=weight_decay, \n","                         opt_func=opt_func)\n","\n","fig, axs = plt.subplots(2, 2, figsize=(10, 10))\n","\n","# now call the modified functions and pass the corresponding axes\n","plot_losses(training_results, ax=axs[0, 0])\n","plot_avg_precision_scores(training_results, ax=axs[0, 1])\n","plot_accuracy(training_results, ax=axs[1, 0])\n","plot_f1(training_results, ax=axs[1, 1])\n","\n","# Automatically adjust the subplot parameters to give specified padding\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-24T19:54:40.728111Z","iopub.status.busy":"2023-05-24T19:54:40.724321Z","iopub.status.idle":"2023-05-24T19:54:40.734504Z","shell.execute_reply":"2023-05-24T19:54:40.733196Z","shell.execute_reply.started":"2023-05-24T19:54:40.728071Z"},"trusted":true},"outputs":[],"source":["object_categories = get_categories(train_df)\n","print(object_categories)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-24T19:54:40.737079Z","iopub.status.busy":"2023-05-24T19:54:40.736638Z","iopub.status.idle":"2023-05-24T19:54:42.226407Z","shell.execute_reply":"2023-05-24T19:54:42.224877Z","shell.execute_reply.started":"2023-05-24T19:54:40.737035Z"},"trusted":true},"outputs":[],"source":["def predict_and_plot_sample(sample_index, dataset, model):\n","    plt.figure()  # Create a new figure\n","    img, label = dataset[sample_index]\n","    plt.imshow(img.permute(1, 2, 0).clamp(0, 1))\n","    label_text = get_label_txt(label)\n","    xb = to_device(img.unsqueeze(0), device)\n","    preds = model(xb)\n","    output = (torch.sigmoid(preds.cpu().detach()))>0.5\n","    result = []\n","    idx_lst = output.tolist()[0]\n","    for i in range(20):\n","        if idx_lst[i]==1:\n","            result.append(object_categories[i])\n","    print('Label:', label_text, ', Predicted:', result)\n","\n","# Gather training_results\n","training_results = [evaluate(model, val_dl)]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-24T19:54:42.229442Z","iopub.status.busy":"2023-05-24T19:54:42.229001Z","iopub.status.idle":"2023-05-24T19:54:42.348452Z","shell.execute_reply":"2023-05-24T19:54:42.347352Z","shell.execute_reply.started":"2023-05-24T19:54:42.229394Z"},"trusted":true},"outputs":[],"source":["# Define the best_model_path variable and assign it the path of the best model file\n","best_model_path = '/kaggle/working/best_model.pth'\n","\n","# Load best model\n","model.load_state_dict(torch.load(best_model_path))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-24T19:54:42.35028Z","iopub.status.busy":"2023-05-24T19:54:42.349865Z","iopub.status.idle":"2023-05-24T19:54:42.358099Z","shell.execute_reply":"2023-05-24T19:54:42.35687Z","shell.execute_reply.started":"2023-05-24T19:54:42.350238Z"},"trusted":true},"outputs":[],"source":["val_loss = training_results[0]['val_loss']\n","val_ap = training_results[0]['val_ap']\n","val_acc = training_results[0]['val_accuracy']\n","val_f1 = training_results[0]['val_f1']\n","print(f\"Loss: {val_loss:.4f}, Avg Precision Score: {val_ap:.4f}, Accuracy: {val_acc:.4f}, F1 Score: {val_f1:.4f}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-24T19:54:42.361614Z","iopub.status.busy":"2023-05-24T19:54:42.360126Z","iopub.status.idle":"2023-05-24T19:54:44.484884Z","shell.execute_reply":"2023-05-24T19:54:44.483759Z","shell.execute_reply.started":"2023-05-24T19:54:42.361569Z"},"trusted":true},"outputs":[],"source":["def denormalize(images, means, stds):\n","    means = torch.tensor(means).reshape(1, 3, 1, 1)\n","    stds = torch.tensor(stds).reshape(1, 3, 1, 1)\n","    return images * stds + means\n","\n","def get_label_txt(label):\n","    # Assuming object_categories is defined globally\n","    return [object_categories[i] for i, val in enumerate(label) if val == 1]\n","\n","def plot_sample_predictions(sample_indices, dataset, df, model):\n","    num_samples = len(sample_indices)\n","    num_cols = 3  # Number of columns in the grid\n","    num_rows = math.ceil(num_samples / num_cols)  # Number of rows in the grid\n","\n","    fig, axs = plt.subplots(num_rows, num_cols, figsize=(12, 4 * num_rows))\n","    fig.tight_layout()\n","\n","    for i, sample_index in enumerate(sample_indices):\n","        row = i // num_cols\n","        col = i % num_cols\n","\n","        img, label = dataset[sample_index]\n","        label_text = get_label_txt(label)\n","        xb = to_device(img.unsqueeze(0), device)\n","        preds = model(xb)\n","        output = (torch.sigmoid(preds.cpu().detach())) > 0.5\n","        result = [object_categories[j] for j in range(20) if output[0, j] == 1]\n","\n","        # Denormalize image\n","        denorm_img = denormalize(img.unsqueeze(0), *stats).squeeze().permute(1, 2, 0).clamp(0, 1)\n","\n","        # Show original image from dataframe\n","        orig_img = df['img'].iloc[sample_index]\n","        \n","        axs[row, col].imshow(orig_img)\n","        axs[row, col].set_title(f\"Label: {label_text}\\nPredicted: {result}\")\n","        axs[row, col].axis(\"off\")\n","\n","    plt.show()\n","\n","# Sample predictions\n","sample_indices = [100, 50, 85, 30, 10, 35, 44, 56, 99, 120, 34, 89]\n","plot_sample_predictions(sample_indices, val_dataset, val_df, model)  # added `val_df` as the third argument\n","\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## 4.2. Adversarial Attack\n","\n","We consider a white-box adversarial attack where the objective is to manipulate a convolutional autoencoder to generate noise from an image in a manner that confuses the targeted model. We choose to target the pretrained model for the classification part of the assignment (Resnet34 pretrained on ImageNet), because it seems to be the best performing model across all assignment tasks. \n","\n","In order to regulate the magnitude of perturbations, two approaches can be employed. The first approach involves applying a squashing function, such as a sigmoid or hyperbolic tangent, scaled by a constant. The second approach involves incorporating regularization techniques, such as l2-regularization norm, to constrain the weights of the model. It is worth noting that stronger perturbations increase the likelihood of inducing misclassification, but they also become more noticeable to human observers.\n","\n","A challenge arises due to the limited availability of labeled data for training an adversarial attack, as the model under attack is trained using only 80% of the original training set. A straightforward solution to this problem would be to utilize the complete Voc2009 dataset, which can be accessed at http://host.robots.ox.ac.uk/pascal/VOC/voc2009/index.html#data, instead of relying solely on the subset from the Kaggle competition. However, this approach does not seem to be in the spirit of the assignment. Consequently, we opt to employ the unlabeled test set provided on Kaggle for training the adversarial attack. We justify this decision since the training data for adversarial attacks consists of fake labels, and having groundtruth isn't necessary. However, this approach introduces limitations in terms of the quality and informativeness of the fake labels.\n","\n","To explore different variations of fake labels, we experimented with the following approaches:\n","1. Assigning a label of 0 to every class. Ideally, this would result in the targeted model not recognizing any objects on the image\n","2. Assigning a label of 0.5 to every class, with an idea that model outputs will be no better than a random guess.\n","3. Assigning a label of 1 to every class, making the model think every class of object is on each image.\n","\n","Various other hyperparameters have been experimented with:\n","- different autoencoder architectures (with changes in number of layers, convolution kernel sizes, activation functions (LeakyRelu, Relu, Sigmoid), autoencoders with and without a fully connected layer in the middle.\n","- optimizers: Adam, Adagrad, SGD\n","- restricting the scale of the perturbations with l2 regularization and/or hyperbolic tangent at the autoencoder output\n","\n","Most successful attacks (measured by a decrease in f1-score) were observed while using labels zero for each class, although even these are not particularly successful. \n","\n","It seems that proposed autoencoder architectures do not have the capacity to fool the targeted model when the scale of perturbations is very small (not noticable by human). A likely reason for this is the Resnet34's size and pretraining procedure; having many more parameters and being pretrained on large amounts of data, make it's latent representations robust against targeted attacks from small capacity models. \n","\n","Nevertheless, even if the target models weren't completely fooled, a clear decrease in confidence is observed. This makes our attack successful for use cases where low confidence predictions require human intervention.\n","\n","Using a more sophisticated autoencoder model, training on more data and data with informed fake labels (e.g. inverting groundtruth) should result in a more successful attack. Additionally, our attack focused on all classes, a less ambitious approach, such as focusing on one label only (e.g. 'person') would probably yield better results considering small capacity models were used.\n","\n","Regarding other, future improvements and potential directions for the assignment, during our exploration of perturbation strength manipulation, an intriguing idea for an adversarial attack emerged: generating strong perturbations that completely mask the original image while ensuring that the perturbed image is classified by the model in the same manner as the unperturbed image. This technique could be beneficial in scenarios where it is desirable to utilize a service, such as a classifier, while safeguarding the privacy of the classified data."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-24T19:54:44.487435Z","iopub.status.busy":"2023-05-24T19:54:44.486719Z","iopub.status.idle":"2023-05-24T19:54:44.511569Z","shell.execute_reply":"2023-05-24T19:54:44.510037Z","shell.execute_reply.started":"2023-05-24T19:54:44.487393Z"},"trusted":true},"outputs":[],"source":["from sklearn.metrics import classification_report\n","\n","class VocAdversarial3(nn.Module):\n","    def __init__(self, model, scaling_factor=0.33):\n","        super(VocAdversarial3, self).__init__()\n","        \n","        self.scaling_factor=scaling_factor\n","        self.mp = nn.MaxPool2d(2, 2)\n","        self.conv1 = nn.Conv2d(3, 64, 3, 1, bias=True)\n","        self.relu = nn.LeakyReLU(True)\n","        self.conv2 = nn.Conv2d(64, 32, 5, 1, bias=True)\n","        self.conv3 = nn.Conv2d(32, 16, 10, 1, bias=True)\n","\n","        self.fc1 = nn.Linear(16*26*26, 200)\n","        self.fc2 = nn.Linear(200, 16*26*26)\n","        self.transConv1 = nn.ConvTranspose2d(16, 32, 10, 1, bias=True)\n","        self.transConv2 = nn.ConvTranspose2d(32, 64, 5, 1, bias=True)\n","        self.transConv3 = nn.ConvTranspose2d(64, 3, 3, 1, bias=True)\n","        self.upsample = nn.Upsample(scale_factor=2)\n","        self.model = model              #model under attack (e.g. MNIST classifier)\n","        model.requires_grad_(False)     #freezes gradients of the model under attack\n","\n","    def forward(self, X):\n","        delta = self.mp(X) #150\n","        delta = self.conv1(delta) #148               \n","        delta = self.relu(delta)\n","        delta = self.mp(delta) #74\n","        delta = self.conv2(delta) #70           \n","        delta = self.relu(delta)\n","        delta = self.mp(delta) #35\n","        delta = self.conv3(delta) #31          \n","        delta = self.relu(delta)\n","        delta = self.fc1(torch.flatten(delta, 1, -1))\n","        delta = self.relu(delta)\n","        delta = self.fc2(delta)\n","        delta = self.relu(delta)\n","        delta = torch.reshape(delta, (1, 16, 26, 26))\n","\n","        delta = self.transConv1(delta) #35\n","        delta = self.relu(delta)\n","        delta = self.upsample(delta)   #70\n","        delta = self.transConv2(delta) #74\n","        delta = self.relu(delta)\n","        delta = self.upsample(delta)   #148\n","        delta = self.transConv3(delta) #150\n","        delta = self.upsample(delta)   #300\n","        X_delta = X + torch.tanh(delta) * self.scaling_factor       #combines noise and original image\n","        self.delta = torch.tanh(delta) * self.scaling_factor\n","        self.X = X\n","\n","        prediction = self.model.forward(X_delta)    #model under attack classifies perturbed image\n","        return prediction"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-24T19:54:44.514858Z","iopub.status.busy":"2023-05-24T19:54:44.513803Z","iopub.status.idle":"2023-05-24T19:54:44.526563Z","shell.execute_reply":"2023-05-24T19:54:44.525483Z","shell.execute_reply.started":"2023-05-24T19:54:44.514781Z"},"trusted":true},"outputs":[],"source":["def plot_images(image, delta):\n","    plt.subplot(1, 3, 1)\n","    plt.gca().set_title('original image')\n","\n","    im = torch.permute(image.detach().squeeze(), (2, 1, 0)).to(device='cpu')\n","\n","    plt.imshow(im * std_tensor + mean_tensor)\n","    plt.subplot(1, 3, 2)\n","    plt.gca().set_title('delta')\n","    delta = torch.permute(delta.detach().squeeze(), (2, 1, 0)).to(device='cpu')\n","    plt.imshow(delta * std_tensor + mean_tensor)\n","    plt.subplot(1, 3, 3)\n","    plt.gca().set_title('original+delta')\n","    X_delta = (im + delta) * std_tensor + mean_tensor\n","    plt.imshow(X_delta)\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-24T19:54:44.528941Z","iopub.status.busy":"2023-05-24T19:54:44.528242Z","iopub.status.idle":"2023-05-24T19:54:44.54008Z","shell.execute_reply":"2023-05-24T19:54:44.538807Z","shell.execute_reply.started":"2023-05-24T19:54:44.528902Z"},"trusted":true},"outputs":[],"source":["torch.set_printoptions(precision=2)\n","np.set_printoptions(precision=2)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-24T19:54:44.542505Z","iopub.status.busy":"2023-05-24T19:54:44.541739Z","iopub.status.idle":"2023-05-24T19:54:45.672253Z","shell.execute_reply":"2023-05-24T19:54:45.670997Z","shell.execute_reply.started":"2023-05-24T19:54:44.542468Z"},"trusted":true},"outputs":[],"source":["mean_tensor = torch.tensor((meanR, meanG, meanB))\n","std_tensor = torch.tensor((stdR, stdG, stdB))\n","\n","train_df = pd.read_csv('/kaggle/input/kul-h02a5a-computer-vision-ga2-2023/test/test_set.csv', index_col=\"Id\")\n","labels = train_df.columns\n","train_df[\"img\"] = [np.load('/kaggle/input/kul-h02a5a-computer-vision-ga2-2023/test/img/test_{}.npy'.format(idx)) for idx, _ in train_df.iterrows()]\n","train_dataset = ImageDataset(train_df['img'].to_list(), train_df[labels].to_numpy(), transform=valid_tfms)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-24T19:54:45.674565Z","iopub.status.busy":"2023-05-24T19:54:45.674142Z","iopub.status.idle":"2023-05-24T19:54:45.693455Z","shell.execute_reply":"2023-05-24T19:54:45.692035Z","shell.execute_reply.started":"2023-05-24T19:54:45.674519Z"},"trusted":true},"outputs":[],"source":["def train_adversarial(loss_fn, optimizer, trained_model, adversarial, train_dataloader, validation_dataloader, epochs=2, fake_labels=0.0):\n","    \n","    fake_labels = torch.full([1,20], fake_labels).to(device=\"cuda\")\n","    \n","    for epoch in range(epochs):\n","        running_loss = 0.\n","        last_loss = 0.\n","        \n","        adversarial.train()\n","        trained_model.eval()\n","        for i, data in enumerate(train_dataloader):\n","                \n","            inputs, labels = data\n","            labels = fake_labels\n","            \n","            optimizer.zero_grad()\n","            outputs = adversarial(inputs.to(device='cuda'))\n","\n","            loss = loss_fn(torch.sigmoid(outputs), labels)\n","            loss.backward()\n","\n","            optimizer.step()\n","            running_loss += loss.item()\n","\n","        groundtruth = []\n","        adversary_predicted = []\n","        predicted = []\n","        adversarial.eval()\n","\n","        for i, data in enumerate(validation_dataloader):\n","            inputs, labels = data\n","            groundtruth.append((labels.to(device='cpu')))\n","\n","            outputs = adversarial(inputs.to(device=\"cuda\"))\n","            adversary_predicted.append((torch.sigmoid(outputs) > 0.5).long().to(device='cpu'))\n","\n","            outputs = trained_model(inputs.to(device=\"cuda\"))\n","            predicted.append((torch.sigmoid(outputs) > 0.5).long().to(device='cpu'))\n","\n","        groundtruth = torch.cat(groundtruth, dim=1).squeeze()\n","        adversary_predicted = torch.cat(adversary_predicted, dim=1).squeeze()\n","        predicted = torch.cat(predicted, dim=1).squeeze()\n","        \n","        if epoch == 0:\n","            print('Classificiation report for original model without perturbations')\n","            print(f1_score(groundtruth, predicted))\n","\n","        print('classification reports for adversarial model after epoch ', epoch)\n","        print('adversary----------------------')\n","        print(f1_score(groundtruth, adversary_predicted))\n","\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### 4.2.1 Fake labels 1"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-24T19:54:45.697305Z","iopub.status.busy":"2023-05-24T19:54:45.696382Z","iopub.status.idle":"2023-05-24T19:56:13.855748Z","shell.execute_reply":"2023-05-24T19:56:13.853108Z","shell.execute_reply.started":"2023-05-24T19:54:45.697257Z"},"trusted":true},"outputs":[],"source":["tr_dl = DataLoader(train_dataset, batch_size=1, shuffle=True, num_workers=2, pin_memory=True)\n","val_dl = DataLoader(val_dataset, batch_size=1, shuffle=True, num_workers=2, pin_memory=True)\n","\n","#model under attack\n","trained_model = model\n","model.eval()\n","\n","#loading adversarial model\n","adversarial = VocAdversarial3(trained_model, scaling_factor=0.66)\n","adversarial.cuda()\n","\n","optimizer = torch.optim.Adagrad(adversarial.parameters())\n","loss_fn = torch.nn.BCELoss()\n","\n","train_adversarial(loss_fn=loss_fn, \n","                  optimizer=optimizer,\n","                 trained_model=model,\n","                 adversarial=adversarial,\n","                 train_dataloader=tr_dl,\n","                 validation_dataloader=val_dl,\n","                 fake_labels=1.0)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-24T19:56:13.859066Z","iopub.status.busy":"2023-05-24T19:56:13.858537Z","iopub.status.idle":"2023-05-24T19:56:14.971942Z","shell.execute_reply":"2023-05-24T19:56:14.970586Z","shell.execute_reply.started":"2023-05-24T19:56:13.858998Z"},"trusted":true},"outputs":[],"source":["plot_images(adversarial.X, adversarial.delta)\n","print('logits without perturbations')\n","print(trained_model(adversarial.X))\n","print('logits with perturbations')\n","print(adversarial(adversarial.X))"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### 4.2.2 Fake labels 0.5"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-24T19:56:14.97522Z","iopub.status.busy":"2023-05-24T19:56:14.974347Z","iopub.status.idle":"2023-05-24T19:57:43.739647Z","shell.execute_reply":"2023-05-24T19:57:43.736631Z","shell.execute_reply.started":"2023-05-24T19:56:14.975174Z"},"trusted":true},"outputs":[],"source":["tr_dl = DataLoader(train_dataset, batch_size=1, shuffle=True, num_workers=2, pin_memory=True)\n","val_dl = DataLoader(val_dataset, batch_size=1, shuffle=True, num_workers=2, pin_memory=True)\n","\n","#model under attack\n","trained_model = model\n","model.eval()\n","\n","#loading adversarial model\n","adversarial = VocAdversarial3(trained_model, scaling_factor=0.66)\n","adversarial.cuda()\n","\n","optimizer = torch.optim.Adagrad(adversarial.parameters())\n","loss_fn = torch.nn.BCELoss()\n","\n","train_adversarial(loss_fn=loss_fn, \n","                  optimizer=optimizer,\n","                 trained_model=model,\n","                 adversarial=adversarial,\n","                 train_dataloader=tr_dl,\n","                 validation_dataloader=val_dl,\n","                 fake_labels=0.5)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-24T19:57:43.743211Z","iopub.status.busy":"2023-05-24T19:57:43.742266Z","iopub.status.idle":"2023-05-24T19:57:44.237818Z","shell.execute_reply":"2023-05-24T19:57:44.236337Z","shell.execute_reply.started":"2023-05-24T19:57:43.743156Z"},"trusted":true},"outputs":[],"source":["plot_images(adversarial.X, adversarial.delta)\n","print('logits without perturbations')\n","print(trained_model(adversarial.X))\n","print('logits with perturbations')\n","print(adversarial(adversarial.X))"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### 4.2.3 Fake labels 0"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-24T19:57:44.240386Z","iopub.status.busy":"2023-05-24T19:57:44.239841Z","iopub.status.idle":"2023-05-24T19:58:39.254314Z","shell.execute_reply":"2023-05-24T19:58:39.247942Z","shell.execute_reply.started":"2023-05-24T19:57:44.24034Z"},"trusted":true},"outputs":[],"source":["#trainset_1 = torch.utils.data.Subset(train_dataset, [1, 2, 3, 4, 5])\n","#trainset_1 = val_dataset\n","\n","tr_dl = DataLoader(train_dataset, batch_size=1, shuffle=True, num_workers=2, pin_memory=True)\n","val_dl = DataLoader(val_dataset, batch_size=1, shuffle=True, num_workers=2, pin_memory=True)\n","\n","#model under attack\n","trained_model = model\n","model.eval()\n","\n","#loading adversarial model\n","adversarial = VocAdversarial3(trained_model, scaling_factor=0.66)\n","adversarial.cuda()\n","\n","optimizer = torch.optim.Adagrad(adversarial.parameters())\n","loss_fn = torch.nn.BCELoss()\n","\n","train_adversarial(loss_fn=loss_fn, \n","                  optimizer=optimizer,\n","                 trained_model=model,\n","                 adversarial=adversarial,\n","                 train_dataloader=tr_dl,\n","                 validation_dataloader=val_dl,\n","                 fake_labels=0.0)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-05-24T19:58:39.259122Z","iopub.status.idle":"2023-05-24T19:58:39.260183Z","shell.execute_reply":"2023-05-24T19:58:39.25985Z","shell.execute_reply.started":"2023-05-24T19:58:39.2598Z"},"trusted":true},"outputs":[],"source":["plot_images(adversarial.X, adversarial.delta)\n","print('logits without perturbations')\n","print(trained_model(adversarial.X))\n","print('logits with perturbations')\n","print(adversarial(adversarial.X))"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### 4.2.4 Experiment with perturbations small in scale"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-05-24T19:58:39.261985Z","iopub.status.idle":"2023-05-24T19:58:39.262936Z","shell.execute_reply":"2023-05-24T19:58:39.262628Z","shell.execute_reply.started":"2023-05-24T19:58:39.262597Z"},"trusted":true},"outputs":[],"source":["tr_dl = DataLoader(train_dataset, batch_size=1, shuffle=True, num_workers=2, pin_memory=True)\n","val_dl = DataLoader(val_dataset, batch_size=1, shuffle=True, num_workers=2, pin_memory=True)\n","\n","#model under attack\n","trained_model = model\n","model.eval()\n","\n","#loading adversarial model\n","adversarial = VocAdversarial3(trained_model, scaling_factor=0.1)\n","adversarial.cuda()\n","\n","optimizer = torch.optim.Adagrad(adversarial.parameters())\n","loss_fn = torch.nn.BCELoss()\n","\n","train_adversarial(loss_fn=loss_fn, \n","                  optimizer=optimizer,\n","                 trained_model=model,\n","                 adversarial=adversarial,\n","                 train_dataloader=tr_dl,\n","                 validation_dataloader=val_dl,\n","                 fake_labels=0.0)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-05-24T19:58:39.264638Z","iopub.status.idle":"2023-05-24T19:58:39.265563Z","shell.execute_reply":"2023-05-24T19:58:39.26529Z","shell.execute_reply.started":"2023-05-24T19:58:39.26526Z"},"trusted":true},"outputs":[],"source":["plot_images(adversarial.X, adversarial.delta)\n","print('logits without perturbations')\n","print(trained_model(adversarial.X))\n","print('logits with perturbations')\n","print(adversarial(adversarial.X))"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### 4.2.5 Misc. results\n","\n","Before a model was trained on the VOC dataset, a simple testing environment was made to experiment with models trained on the MNIST dataset. The target model was sucessfully attacked despite having, 99.4% accuracy on MNIST\n","\n","An example of an image originally classified as 3, after added perturbations classified as 9. The perturbation visualized in the middle is scaled by the plotting function, otherwise it would not be visible (as shown on the perturbed image on the right)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-05-24T19:58:39.267202Z","iopub.status.idle":"2023-05-24T19:58:39.268114Z","shell.execute_reply":"2023-05-24T19:58:39.267805Z","shell.execute_reply.started":"2023-05-24T19:58:39.267774Z"},"trusted":true},"outputs":[],"source":["%matplotlib inline\n","import matplotlib.pyplot as plt\n","import matplotlib.image as mpimg\n","\n","# img = mpimg.imread('/kaggle/input/mnist-result/Figure_3.png')\n","# imgplot = plt.imshow(img)\n","# plt.show()"]},{"attachments":{"a5d739d9-9977-42df-b3d9-2298da69f295.png":{"image/png":"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"}},"cell_type":"markdown","metadata":{},"source":["![Figure_3.png](attachment:a5d739d9-9977-42df-b3d9-2298da69f295.png)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-05-24T19:58:39.269818Z","iopub.status.idle":"2023-05-24T19:58:39.270731Z","shell.execute_reply":"2023-05-24T19:58:39.270451Z","shell.execute_reply.started":"2023-05-24T19:58:39.270421Z"},"trusted":true},"outputs":[],"source":["from torch import nn\n","class MnistAdversarial(nn.Module):\n","    def __init__(self, model):\n","        super(MnistAdversarial, self).__init__()\n","        # Downsizing\n","        # input Bx28x28x3, output Bx25x25x16\n","        self.conv1 = nn.Conv2d(1, 16, 4, 1, 0, bias=False)\n","        print(self.conv1)\n","        self.relu1 = nn.ReLU(True)\n","        # Bx18x18x32\n","        self.conv2 = nn.Conv2d(16, 32, 8, 1, bias=False)\n","        self.relu2 = nn.ReLU(True)\n","        # Bx11x11x32\n","        self.conv3 = nn.Conv2d(32, 32, 8, 1, bias=False)\n","        self.relu3 = nn.ReLU(True)\n","        # resizing\n","        # Bx20x20x16\n","        self.transConv1 = nn.ConvTranspose2d(32, 16, 10, 1, bias=False)\n","        self.relu4 = nn.ReLU(True)\n","        # Bx28x28x3\n","        self.transConv2 = nn.ConvTranspose2d(16, 1, 9, 1, bias=False)\n","\n","        self.model = model              #model under attack (e.g. MNIST classifier)\n","        model.requires_grad_(False)     #freezes gradients of the model under attack\n","\n","    def forward(self, X):\n","        delta = self.conv1(X)               #Bx25x25x16\n","        delta = self.relu1(delta)\n","        delta = self.conv2(delta)           #Bx18x18x32\n","        delta = self.relu2(delta)\n","        delta = self.conv3(delta)           #Bx11x11x32\n","        delta = self.relu3(delta)\n","        delta = self.transConv1(delta)      #Bx20x20x16\n","        delta = self.relu4(delta)\n","        delta = self.transConv2(delta)      #Bx28x28x3\n","\n","        X_delta = X + delta                 #combines noise and original image\n","\n","        self.delta = delta\n","        self.X = X\n","\n","        prediction = self.model.forward(X_delta)    #model under attack classifies perturbed image\n","        return prediction\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-05-24T19:58:39.272403Z","iopub.status.idle":"2023-05-24T19:58:39.273312Z","shell.execute_reply":"2023-05-24T19:58:39.273021Z","shell.execute_reply.started":"2023-05-24T19:58:39.272991Z"},"trusted":true},"outputs":[],"source":["from torch import nn\n","import torch.nn.functional as F\n","\n","\n","class MnistModel(nn.Module):\n","    def __init__(self):\n","        super(MnistModel, self).__init__()\n","        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n","        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n","        self.conv2_drop = nn.Dropout2d()\n","        self.fc1 = nn.Linear(320, 50)\n","        self.fc2 = nn.Linear(50, 10)\n","\n","    def forward(self, x):\n","        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n","        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n","        x = x.view(-1, 320)\n","        x = F.relu(self.fc1(x))\n","        x = F.dropout(x, training=self.training)\n","        x = self.fc2(x)\n","        return F.log_softmax(x)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-05-24T19:58:39.275034Z","iopub.status.idle":"2023-05-24T19:58:39.275945Z","shell.execute_reply":"2023-05-24T19:58:39.275642Z","shell.execute_reply.started":"2023-05-24T19:58:39.275613Z"},"trusted":true},"outputs":[],"source":["# import matplotlib.pyplot as plt\n","# import numpy\n","# import torch\n","# import torchvision.datasets\n","\n","# from models.mnist_attacker import MnistAdversarial\n","# from models.mnist_model import MnistModel\n","\n","# torch.set_printoptions(precision=2)\n","# numpy.set_printoptions(precision=2)\n","\n","# #loading model under attack\n","# trained_model = MnistModel()\n","# trained_model.load_state_dict(torch.load('/kaggle/input/mnist-model/model.pth'))\n","\n","# #loading adversarial model\n","# adversarial = MnistAdversarial(trained_model)\n","\n","# '''\n","# load dataset (this dataset still has original, correct labels (labels are changed in the train loop for now, \n","# this should be changed (e.g. by creating a custom dataset)\n","# '''\n","# dataset = torchvision.datasets.MNIST('dataset/', train=False, download=True,\n","#                                transform=torchvision.transforms.Compose([\n","#                                    torchvision.transforms.ToTensor(),\n","#                                    torchvision.transforms.Normalize(\n","#                                        (0.1307,), (0.3081,))\n","#                                    ]))\n","\n","# data_loader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=True)\n","\n","# '''\n","# SGD optimizer\n","# weight_decay is the L2 regularization factor \n","# (the stronger the regularization factor, the less the images will be perturbed (i think))\n","# '''\n","\n","# optimizer = torch.optim.SGD(adversarial.parameters(), lr=0.001, momentum=0.9, weight_decay=1)\n","# loss_fn = torch.nn.BCEWithLogitsLoss()\n","\n","# epochs = 5\n","# for epoch in range(epochs):\n","#     running_loss = 0.\n","#     last_loss = 0.\n","#     # Here, we use enumerate(training_loader) instead of\n","#     # iter(training_loader) so that we can track the batch\n","#     # index and do some intra-epoch reporting\n","#     for i, data in enumerate(data_loader):\n","#         # Every data instance is an input + label pair\n","#         inputs, labels = data\n","#         labels = torch.full([1, 10], 1/10)\n","#         # Zero your gradients for every batch!\n","#         optimizer.zero_grad()\n","\n","#         # Make predictions for this batch\n","#         outputs = adversarial(inputs)\n","\n","#         # Compute the loss and its gradients\n","#         loss = loss_fn(outputs, labels)\n","#         loss.backward()\n","\n","#         # Adjust learning weights\n","#         optimizer.step()\n","\n","#         # Gather data and report\n","#         running_loss += loss.item()\n","#         if i % 1000 == 0:\n","#             with torch.no_grad():\n","#                 original_outputs = trained_model(inputs)\n","#                 original_probabilities = torch.exp(original_outputs)\n","#                 original_classification = torch.argmax(original_probabilities)\n","\n","#                 print('original')\n","#                 print(original_outputs.numpy())\n","#                 print(original_probabilities.numpy())\n","#                 print(original_classification.numpy())\n","\n","#                 probabilities = torch.exp(outputs)\n","#                 classification = torch.argmax(probabilities)\n","#                 print('adversary')\n","#                 print(outputs.numpy())\n","#                 print(probabilities.numpy())\n","#                 print(classification.numpy())\n","#                 '''\n","#                 print('Original image probabilities')\n","#                 print(probabilities.numpy())\n","#                 print('label predicted = ', classification.item())\n","#                 print('Perturbed image probabilities')\n","#                 print(torch.exp(outputs).detach().numpy())\n","#                 print('label predicted =', torch.argmax(torch.exp(outputs)).item())\n","#                 '''\n","#                 plt.subplot(1, 3, 1)\n","#                 plt.gca().set_title('original image')\n","#                 plt.imshow(adversarial.X.detach().squeeze(), cmap='gray')\n","#                 plt.subplot(1, 3, 2)\n","#                 plt.gca().set_title('delta')\n","#                 plt.imshow(adversarial.delta.detach().squeeze(), cmap='gray')\n","#                 plt.subplot(1, 3, 3)\n","#                 plt.gca().set_title('original+delta')\n","#                 X_delta = adversarial.X.detach().squeeze() + adversarial.delta.detach().squeeze()\n","#                 plt.imshow(X_delta, cmap='gray')\n","#                 plt.show()\n","#                 last_loss = running_loss / 1000  # loss per batch\n","#                 print('  batch {} loss: {}'.format(i + 1, last_loss))\n","#                 tb_x = epochs * len(data_loader) + i + 1"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Below are two convolutional autoencoders used for adversarial attacks on the VOC dataset. They performed poorly, giving the same output regardless of the input. Primary suspected cause are convolution kernels being too large."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-24T21:21:43.749703Z","iopub.status.busy":"2023-05-24T21:21:43.749093Z","iopub.status.idle":"2023-05-24T21:21:43.791944Z","shell.execute_reply":"2023-05-24T21:21:43.790892Z","shell.execute_reply.started":"2023-05-24T21:21:43.749661Z"},"trusted":true},"outputs":[],"source":["import torch\n","from torch import nn\n","class VocAdversarial(nn.Module):\n","    def __init__(self, model):\n","        super(VocAdversarial, self).__init__()\n","        # Downsizing\n","        # input Bx3x300x300, output Bx16x281x281\n","        self.conv1 = nn.Conv2d(3, 16, 20, 1, 0, bias=False)\n","        self.relu = nn.ReLU(True)\n","        # Bx32x262x262\n","        self.conv2 = nn.Conv2d(16, 32, 20, 1, bias=False)\n","        # Bx32x243x243\n","        self.conv3 = nn.Conv2d(32, 32, 40, 1, bias=False)\n","        # Bx32x224x224\n","        self.conv4 = nn.Conv2d(32, 32, 60, 1, bias=False)\n","        # Bx32x205x205\n","        self.conv5 = nn.Conv2d(32, 32, 80, 1, bias=False)\n","        # Bx32x186x186\n","        self.conv6 = nn.Conv2d(32, 32, 20, 1, bias=False)\n","        # Bx32x167x167\n","        self.conv7 = nn.Conv2d(32, 32, 20, 1, bias=False)\n","        # Bx32x148x148\n","        self.conv8 = nn.Conv2d(32, 32, 20, 1, bias=False)\n","\n","        self.fc1 = nn.Linear(32*28*28, 50)\n","        # resizing\n","        self.fc2 = nn.Linear(50, 32*28*28)\n","        # Bx32x205x205\n","        self.transConv1 = nn.ConvTranspose2d(32, 32, 20, 1, bias=False)\n","        # Bx32x224x224\n","        self.transConv2 = nn.ConvTranspose2d(32, 32, 20, 1, bias=False)\n","        # Bx16x243x243\n","        self.transConv3 = nn.ConvTranspose2d(32, 32, 80, 1, bias=False)\n","        # Bx3x262x262\n","        self.transConv4 = nn.ConvTranspose2d(32, 32, 60, 1, bias=False)\n","        # Bx16x281x281\n","        self.transConv5 = nn.ConvTranspose2d(32, 32, 40, 1, bias=False)\n","        # Bx16x281x281\n","        self.transConv6 = nn.ConvTranspose2d(32, 32, 20, 1, bias=False)\n","        # Bx16x281x281\n","        self.transConv7 = nn.ConvTranspose2d(32, 16, 20, 1, bias=False)\n","        # Bx3x300x300\n","        self.transConv8 = nn.ConvTranspose2d(16, 3, 20, 1, bias=False)\n","\n","        self.model = model              #model under attack (e.g. MNIST classifier)\n","        model.requires_grad_(False)     #freezes gradients of the model under attack\n","\n","    def forward(self, X):\n","        delta = self.conv1(X)               #Bx25x25x16\n","        delta = self.relu(delta)\n","        delta = self.conv2(delta)           #Bx18x18x32\n","        delta = self.relu(delta)\n","        delta = self.conv3(delta)           #Bx11x11x32\n","        delta = self.relu(delta)\n","        delta = self.conv4(delta)           #Bx11x11x32\n","        delta = self.relu(delta)\n","        delta = self.conv5(delta)           #Bx11x11x32\n","        delta = self.relu(delta)\n","        delta = self.conv6(delta)           #Bx11x11x32\n","        delta = self.relu(delta)\n","        delta = self.conv7(delta)           #Bx11x11x32\n","        delta = self.relu(delta)\n","        delta = self.conv8(delta)           #Bx11x11x32\n","        delta = self.relu(delta)\n","\n","#        delta = self.fc1(torch.flatten(delta, 1, -1))\n"," #       delta = self.relu(delta)\n","  #      delta = self.fc2(delta)\n","   #     delta = self.relu(delta)\n","    #    delta = torch.reshape(delta, (1, 32, 28, 28))\n","\n","        delta = self.transConv1(delta)      #Bx20x20x16\n","        delta = self.relu(delta)\n","        delta = self.transConv2(delta)      #Bx28x28x3\n","        delta = self.relu(delta)\n","        delta = self.transConv3(delta)       #Bx28x28x3\n","        delta = self.relu(delta)\n","        delta = self.transConv4(delta)         #Bx28x28x3\n","        delta = self.relu(delta)\n","        delta = self.transConv5(delta)         #Bx28x28x3\n","        delta = self.relu(delta)\n","        delta = self.transConv6(delta)     #Bx28x28x3\n","        delta = self.relu(delta)\n","        delta = self.transConv7(delta)     #Bx28x28x3\n","        delta = self.relu(delta)\n","        delta = self.transConv8(delta)     #Bx28x28x3\n","        X_delta = X + torch.tanh(delta)          #combines noise and original image\n","        self.delta = torch.tanh(delta)\n","        self.X = X\n","\n","        prediction = self.model.forward(X_delta)    #model under attack classifies perturbed image\n","        return prediction\n","\n","\n","\n","class VocAdversarial2(nn.Module):\n","    def __init__(self, model):\n","        super(VocAdversarial2, self).__init__()\n","\n","        self.mp = nn.MaxPool2d(3, 3)\n","        self.conv1 = nn.Conv2d(3, 16, 20, 1, 0, bias=False)\n","        self.relu = nn.LeakyReLU(True)\n","        self.conv2 = nn.Conv2d(16, 32, 20, 1, bias=False)\n","        self.conv3 = nn.Conv2d(32, 64, 40, 1, bias=False)\n","\n","        self.fc1 = nn.Linear(64*23*23, 50)\n","        # resizing\n","        self.fc2 = nn.Linear(50, 64*23*23)\n","        # Bx32x205x205\n","        self.transConv1 = nn.ConvTranspose2d(64, 32, 40, 1, bias=False)\n","        # Bx32x224x224\n","        self.transConv2 = nn.ConvTranspose2d(32, 16, 20, 1, bias=False)\n","        # Bx16x243x243\n","        self.transConv3 = nn.ConvTranspose2d(16, 3, 20, 1, bias=False)\n","        self.upsample = nn.Upsample(scale_factor=3)\n","        self.model = model              #model under attack (e.g. MNIST classifier)\n","        model.requires_grad_(False)     #freezes gradients of the model under attack\n","\n","    def forward(self, X):\n","        delta = self.mp(X)\n","        delta = self.conv1(delta)               #Bx25x25x16\n","        delta = self.relu(delta)\n","        delta = self.conv2(delta)           #Bx18x18x32\n","        delta = self.relu(delta)\n","        delta = self.conv3(delta)           #Bx11x11x32\n","        delta = self.relu(delta)\n","        \n","     #   delta = self.fc1(torch.flatten(delta, 1, -1))\n","     #   delta = self.relu(delta)\n","     #   delta = self.fc2(delta)\n","     #   delta = self.relu(delta)\n","     #   delta = torch.reshape(delta, (1, 64, 23, 23))\n","\n","        delta = self.transConv1(delta)      #Bx20x20x16\n","        delta = self.relu(delta)\n","        delta = self.transConv2(delta)      #Bx28x28x3\n","        delta = self.relu(delta)\n","        delta = self.transConv3(delta)       #Bx28x28x3\n","        delta = self.upsample(delta)\n","        X_delta = X + torch.tanh(delta)*20         #combines noise and original image\n","        self.delta = torch.tanh(delta)*20\n","        self.X = X\n","\n","        prediction = self.model.forward(X_delta)    #model under attack classifies perturbed image\n","        return prediction"]},{"attachments":{},"cell_type":"markdown","metadata":{"papermill":{"duration":0.195695,"end_time":"2022-04-12T14:50:42.117581","exception":false,"start_time":"2022-04-12T14:50:41.921886","status":"completed"},"tags":[]},"source":["# 5. Discussion\n","\n"," \n","\n","## 5.1 Classification\n","\n"," \n","\n","In the classification part of the assignment, we chose to approach the issue at hand both from by training custom made neural networks to be trained from scratch, as well as implementing one based upon a pretrained version of ResNet50.\n","\n"," \n","\n","As stated in the corresponding section above, we implemented three different CNN-based neural network models to try and tackle the issue: SimpleNetwork (consisting of two stacks of a convolutional layer followed by a max pooling one), ComplexNetwork (four stacks of Convolutional + Max pooling layers followed by a fully connected one) and TinyVGG, which follows the architecture of VGG[5] using smaller dimension layers.\n","\n"," \n","\n","As expected, the model based on the pretrained ResNet50 outperformed every one of the trained-from-scratch models by a significant margin in every metric available. The main reason behind the difference in performance is the volume of training data provided: having available approximately 1/10th of the original dataset, while having the same amount of classes proved to be exceptionally challenging from the trained from scratch model in comparison to the pretrained one. Furthermore, there is the issue of resources: a restricted amount of time to complete the assignment, along with limited amount of hardware resources render the shallow model architectures incapable to outperform a model trained for amounts of time and on resources unavailable at a scale of a semester assignment.\n","\n"," \n","\n","\n","## 5.2 Segmentation\n","\n"," \n","\n","Image segmentation involves identifying objects in an image and assigning each pixel to a recognized object or background. In recent years, fully convolutional neural networks utilizing the encoder-decoder architecture, such as Ronneberger et al.'s U-Net[4], have gained popularity for performing this task due to the advancements [1,2] in deep learning.\n","\n"," \n","\n","In order to tackle this problem, as seen in the corresponding previous section, we again decided to approach it with both an implementation from scratch and pretrained. For our trained-from-scratch model, we chose to implement a U-Net[4] architecture based one, whereas for the pretrained one we once again chose to base it off ResNet.\n","\n"," \n","\n","Yet again, in this case, the issues we faced on the classification task are still present: small, imbalanced training set and insufficient hardware resources to train deep architectures render the custom made U-Net model unable to converge and yield adequate results during inference. On the contrary, the pretrained ResNet provides very high quality results in terms of Intersection-over-Union, without much effort on fine-tuning and adjusting it to our data.\n","\n"," \n","\n","\n","## 5.3 Adversary Attack\n","\n"," \n","\n","For the last part of the assignment, we investigated a white-box adversarial attack strategy in which the objective was to manipulate a convolutional autoencoder to generate noise from an image, thereby confusing the targeted model. The pretrained model chosen for this purpose was a variation of the classification component of the assignment, namely ResNet34 pretrained on ImageNet.\n","\n"," \n","\n","Two approaches were employed to regulate the magnitude of perturbations. The first approach involved the application of a squashing function, such as a sigmoid or hyperbolic tangent, scaled by a constant factor. The second approach incorporated regularization techniques, specifically the l2-regularization norm, to constrain the model's weights. It is noteworthy that stronger perturbations were found to increase the likelihood of inducing misclassification, albeit at the cost of becoming more noticeable to human observers.\n","\n"," \n","\n","In summary, the proposed autoencoder architectures proved inadequate in deceiving the targeted model with subtle perturbations that went unnoticed by humans. This was likely due to the robustness of ResNet34, which possesses a larger parameter count and has undergone extensive prior training. Consequently, its latent representations remained resilient against attacks from less powerful models. However, although the target models were not completely fooled, our attack did succeed in significantly reducing their confidence levels. As a result, our approach proves effective in scenarios where predictions with low confidence necessitate human intervention.\n","\n"," \n","\n","To enhance the attack's efficacy, future endeavors should involve employing more sophisticated autoencoder models trained on larger and more diverse datasets, including data with deliberately mislabeled information. By focusing on specific labels rather than targeting all classes, especially when utilizing smaller capacity models, better results can be achieved. Additionally, exploring the manipulation of perturbation strength has sparked an intriguing idea for an adversarial attack: generating strong perturbations that completely mask the original image while ensuring the perturbed image is classified in the same manner as the unaltered image. This technique holds potential for situations where leveraging services like classifiers is desired while safeguarding the privacy of the underlying data.\n","\n"," \n","\n","## 5.4 Theoretical vs. Real-World Scenarios\n","\n"," \n","\n","The first major outake one can have from this assignment is the importance of data when trying to implement a solution to tackle the problem of either image classification or segmentation. While theory (in means of recent academic research, publications, lectures, and even industry standards) imply that deep learning based solutions outperform every other method in terms of results, in cases such as ours that data are scarce, such techniques lead to failure. It is in these cases also that one could try to implement approaches based on more \"classic\" computer vision methods, such as e.g. feature extraction combined with a Machine Learning classifier, such as e.g. SVMs, to approach the tasks of classification.\n","\n"," \n","\n","Of course, there is no doubt that, provided enough data, deep learning based methods achieve exceptional results, not only in terms of accuracy in their estimations, but in terms of robustness and generalization on inconsistent image data.\n","\n"," \n","\n","\n","## 5.5 References\n","\n"," \n","\n","\n","- 1. https://doi.org/10.1016/j.asoc.2018.05.018\n","- 2. https://doi.org/10.1109/TPAMI.2021.3059968\n","- 3. https://arxiv.org/abs/1512.03385\n","- 4. https://arxiv.org/abs/1505.04597\n","- 5. https://doi.org/10.48550/arXiv.1409.1556"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.16"}},"nbformat":4,"nbformat_minor":4}
